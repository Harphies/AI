## Series of important points

- The difference between a successful model and a failed model heavily lies in the choice of features used for the model. (As we can't use a wrong factor to make good decisions) Good decisions depends heavily on right factors.
- Basic Recurrent Neural Networks are affected by Vashishing and Exploding Gradients
  - Vanishing gradients are addressed by having long term dependences where LSTM and Gated Recurrent Units can solve that.
  - Exploding gradients is addressed by clipping the gradient vector
- Mean squared error and Mean Absolute Error are not suitable for deep learning models metric evaluation
- Adam optimzer is always a good choice of optimizer
- ReLU is always a good choice of activation function for hidden layers.
