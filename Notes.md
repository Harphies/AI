## Series of important points

- Basic Recurrent Neural Networks are affected by Vashishing and Exploding Gradients
  - Vanishing gradients are addressed by having long term dependences where LSTM and Gated Recurrent Units can solve that.
  - Exploding gradients is addressed by clipping the gradient vector
- Mean squared error and Mean Absolute Error are not suitable for deep learning models metric evaluation
- Adam optimzer is always a good choice ofoptimizer
- ReLU is always a good choice of activation function for hidden layers.
