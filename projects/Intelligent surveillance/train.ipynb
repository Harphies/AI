{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Surveillance - Anomaly event detection\n",
    "<i> Other applications deep surveillance included theft identification, violence detection and detection of chance of explosion</i> \n",
    "Using spatial temporal Autoencoders to identify abnormal activities\n",
    "The encoder part extracts the spatial and temporal Information, and then the decoder reconstruct the frames\n",
    "The abnormal events are Identified by computing the reconstruction loss using Euclidean distance between original and reconstructed batch.\n",
    "- Training a model for anomaly detection tasks\n",
    "- The training video contains videos with normal situation (16 training video clips)\n",
    "- The testing video contains videos with both standard and abnormal events (21 testing video clips)\n",
    "- The videp contains 30652 frames in toatal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from keras.layers import Conv3D, ConvLSTM2D, Conv3DTranspose\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import imutils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steps\n",
    "- create an array holder\n",
    "- create a path to store the converted video frames\n",
    "- create a path to store the videos after converting them to images\n",
    "- convert the images into array\n",
    "- perform some image transformation/processing on the images that has been converted into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_image = []\n",
    "train_path = './train'\n",
    "fps = 5\n",
    "train_videos = os.listdir(train_path)\n",
    "train_images_path = train_path+'/frames'\n",
    "os.makedirs(train_images_path)\n",
    "\n",
    "def store_inarray(image_path):\n",
    "    image = load_img(image_path)\n",
    "    image = img_to_array(image)\n",
    "    image = cv2.resize(image, (227, 227), interpolation = cv2.INTER_AREA)\n",
    "    gray = 0.2989*image[:,:,0]+0.5870*image[:,:,1]+0.1140*image[:,:,2]\n",
    "    store_image.append(gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Extraxt frames/images from video and save </h5>\n",
    "- for each of the video in the training video\n",
    "- convert to images each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in train_videos:\n",
    "    eachVideo = cv2.VideoCapture(video)\n",
    "    success, frame = eachVideo.read()\n",
    "    if success:\n",
    "        name = train_images_path + '.jpg'\n",
    "        cv2.imwrite(name, frame)\n",
    "    eachVideo.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    images = os.listdir(train_images_path)\n",
    "    for image in images:\n",
    "        image_path = train_images_path + '/' + image\n",
    "        store_inarray(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Store the converted images into numpy array of a separate file </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_image = np.array(store_image)\n",
    "# a,b,c=store_image.shape\n",
    "# store_image.resize(b,c,a)\n",
    "# store_image = (store_image - store_image.mean()) / (store_image.std())\n",
    "# store_image = np.clip(store_image, 0, 1)\n",
    "# np.save('training.npy', store_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial temporal autoencoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stae_model = Sequential()\n",
    "\n",
    "stae_model.add(Conv3D(filters=128, kernel_size=(11, 11, 1), strides=(4, 4, 1), padding='valid', input_shape=(227, 227, 10, 1), activation='tanh'))\n",
    "stae_model.add(Conv3D(filters=64, kernel_size=(5, 5, 1), strides=(2, 2, 1), padding='valid', activation='tanh'))\n",
    "stae_model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', dropout=0.4, recurrent_dropout=0.3, return_sequences=True))\n",
    "stae_model.add(ConvLSTM2D(filters=32, kernel_size=(3, 3), strides=1, padding='same', dropout=0.3, return_sequences=True))\n",
    "stae_model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), strides=1, return_sequences=True, padding='same', dropout=0.5))\n",
    "stae_model.add(Conv3DTranspose(filters=128, kernel_size=(5, 5, 1), strides=(2, 2, 1), padding='valid', activation='tanh'))\n",
    "stae_model.add(Conv3DTranspose(filters=1, kernel_size=(11, 11, 1), strides=(4, 4, 1), padding='valid', activation='tanh'))\n",
    "\n",
    "stae_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the autoencoder with the converted numpy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b847ff90641b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training.npy'"
     ]
    }
   ],
   "source": [
    "training_data = np.load('training.npy')\n",
    "frames = training_data.shape[2]\n",
    "frames = frames-frames%10\n",
    "\n",
    "training_data = training_data[:,:,:frames]\n",
    "training_data = training_data.reshape(-1, 227, 227, 10)\n",
    "training_data = np.expand_dims(training_data, axis=4)\n",
    "target_data = training_data.copy()\n",
    "\n",
    "epochs =5 \n",
    "batch_size = 1\n",
    "\n",
    "callback_save = ModelCheckpoint(\"saved_model.h5\", montor='mean_squared-error', save_best_only=True)\n",
    "\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "stae_model.fit(training_data, target_data, batch_size=batch_size, epochs=epochs, callbacks = [callbacksave, callback_early_stopping])\n",
    "stae_model.save('saved_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
