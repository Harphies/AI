{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project: A multilayer perceptron for multiclass classification with Batch Normalization\n",
    "\n",
    "Batch Normalization: This is a technique of normalizing the input input data to every layer before applying an activation function to speed up the learning. we normalize by subrating the mean from the data and devide by standard deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages selection\n",
    "- The first things is to import all the neccesary packages needed for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# select GPU when cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings\n",
    "- Configure the device\n",
    "- define all the hyperparameters to be used and needed to be tuned to achive a better accuracy\n",
    "- Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimension torch.Size([64, 1, 28, 28])\n",
      "Inage label dimension torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 0.001\n",
    "random_seed = 1\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# model architecture parameters\n",
    "num_features = 784\n",
    "num_hidden_1 = 128\n",
    "num_hidden_2 = 256\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "# dataset => MNIST\n",
    "# Note: transform.ToTensor() scale image from 0-1 range\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data',\n",
    "                              train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "# check the dataset\n",
    "for images, labels in train_loader:\n",
    "    print(\"Image batch dimension\", images.shape)\n",
    "    print(\"Inage label dimension\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the architecture of the model such as\n",
    "- The number of input layers; which is determined by the features of the data\n",
    "- Number of total hidden layers in the model (iterative) of hidden units in each layers (iterative)\n",
    "- The output layer node units is determined by the intended outcome to achieve\n",
    "- Here: we build a 3 layers multilayer perceptron i.e 2 hidden layers and 1 output layer\n",
    "- Note: We don't count the input layer as part of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Architecture\n",
    "X -> Linear -> BatchNorm -> ReLU -> Linear -> BatchNorm -> ReLU -> Linear -> Softmax layer -> y\n",
    "\"\"\"\n",
    "\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        \n",
    "        # 1st hidden layer\n",
    "        self.linear_1 = nn.Linear(num_features, num_hidden_1)\n",
    "        # Nomalize this layer before applying some activation function\n",
    "        self.linear_1_bn = nn.BatchNorm1d(num_hidden_1)\n",
    "        \n",
    "        # 2nd hidden layer\n",
    "        self.linear_2 = nn.Linear(num_hidden_1, num_hidden_2)\n",
    "        # Nomalize this layer before applying some activation function\n",
    "        self.linear_2_bn = nn.BatchNorm1d(num_hidden_2)\n",
    "        \n",
    "        # output layer\n",
    "        self.linear_out = nn.Linear(num_hidden_2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear_1(x)\n",
    "        out = self.linear_1_bn(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.linear_2(out)\n",
    "        out = self.linear_2_bn(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        outputs = self.linear_out(out)\n",
    "        probas = F.softmax(outputs, dim=1)\n",
    "        return outputs, probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and optimizer¶\n",
    "- Instantiate the model\n",
    "- define the specific Loss function to be used either cross entropy, MSELoss, etc\n",
    "- define the optimization algorithm to be used either SGD, Adam, RMSprop, Momentum etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "model = MultilayerPerceptron(num_features, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute accuracy\n",
    "- A function to compute train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features = features.view(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs, probas = model(features)\n",
    "            _, predicted_labels = torch.max(probas, 1)\n",
    "            num_examples += labels.size(0)\n",
    "            correct_predictions += (predicted_labels == labels).sum()\n",
    "        return correct_predictions.float() / num_examples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a model requires the following steps¶\n",
    "- Reset all the gradients to zero (0)\n",
    "- Make a forward pass (make a prediction)\n",
    "- Calculate the loss\n",
    "- Perform back propagation\n",
    "- Update all the parameters (weight and biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/010 | Batch: 000/938 | loss: 0.0657\n",
      "Epoch 001/010 | Batch: 050/938 | loss: 0.0457\n",
      "Epoch 001/010 | Batch: 100/938 | loss: 0.0015\n",
      "Epoch 001/010 | Batch: 150/938 | loss: 0.0003\n",
      "Epoch 001/010 | Batch: 200/938 | loss: 0.0002\n",
      "Epoch 001/010 | Batch: 250/938 | loss: 0.0003\n",
      "Epoch 001/010 | Batch: 300/938 | loss: 0.0071\n",
      "Epoch 001/010 | Batch: 350/938 | loss: 0.0136\n",
      "Epoch 001/010 | Batch: 400/938 | loss: 0.0047\n",
      "Epoch 001/010 | Batch: 450/938 | loss: 0.0001\n",
      "Epoch 001/010 | Batch: 500/938 | loss: 0.0037\n",
      "Epoch 001/010 | Batch: 550/938 | loss: 0.0001\n",
      "Epoch 001/010 | Batch: 600/938 | loss: 0.0243\n",
      "Epoch 001/010 | Batch: 650/938 | loss: 0.0019\n",
      "Epoch 001/010 | Batch: 700/938 | loss: 0.0034\n",
      "Epoch 001/010 | Batch: 750/938 | loss: 0.0109\n",
      "Epoch 001/010 | Batch: 800/938 | loss: 0.0596\n",
      "Epoch 001/010 | Batch: 850/938 | loss: 0.1468\n",
      "Epoch 001/010 | Batch: 900/938 | loss: 0.0168\n",
      "Epoch: 001/010 training accuracy: 99.58%\n",
      "Time elapsed: 0.38 min \n",
      "Epoch 002/010 | Batch: 000/938 | loss: 0.0309\n",
      "Epoch 002/010 | Batch: 050/938 | loss: 0.0016\n",
      "Epoch 002/010 | Batch: 100/938 | loss: 0.0033\n",
      "Epoch 002/010 | Batch: 150/938 | loss: 0.0013\n",
      "Epoch 002/010 | Batch: 200/938 | loss: 0.0025\n",
      "Epoch 002/010 | Batch: 250/938 | loss: 0.0015\n",
      "Epoch 002/010 | Batch: 300/938 | loss: 0.0982\n",
      "Epoch 002/010 | Batch: 350/938 | loss: 0.0153\n",
      "Epoch 002/010 | Batch: 400/938 | loss: 0.0033\n",
      "Epoch 002/010 | Batch: 450/938 | loss: 0.0079\n",
      "Epoch 002/010 | Batch: 500/938 | loss: 0.0326\n",
      "Epoch 002/010 | Batch: 550/938 | loss: 0.0019\n",
      "Epoch 002/010 | Batch: 600/938 | loss: 0.0122\n",
      "Epoch 002/010 | Batch: 650/938 | loss: 0.0246\n",
      "Epoch 002/010 | Batch: 700/938 | loss: 0.0316\n",
      "Epoch 002/010 | Batch: 750/938 | loss: 0.0032\n",
      "Epoch 002/010 | Batch: 800/938 | loss: 0.0935\n",
      "Epoch 002/010 | Batch: 850/938 | loss: 0.0023\n",
      "Epoch 002/010 | Batch: 900/938 | loss: 0.0176\n",
      "Epoch: 002/010 training accuracy: 99.41%\n",
      "Time elapsed: 0.81 min \n",
      "Epoch 003/010 | Batch: 000/938 | loss: 0.0003\n",
      "Epoch 003/010 | Batch: 050/938 | loss: 0.0001\n",
      "Epoch 003/010 | Batch: 100/938 | loss: 0.0044\n",
      "Epoch 003/010 | Batch: 150/938 | loss: 0.0011\n",
      "Epoch 003/010 | Batch: 200/938 | loss: 0.0014\n",
      "Epoch 003/010 | Batch: 250/938 | loss: 0.0006\n",
      "Epoch 003/010 | Batch: 300/938 | loss: 0.0019\n",
      "Epoch 003/010 | Batch: 350/938 | loss: 0.0048\n",
      "Epoch 003/010 | Batch: 400/938 | loss: 0.0079\n",
      "Epoch 003/010 | Batch: 450/938 | loss: 0.0169\n",
      "Epoch 003/010 | Batch: 500/938 | loss: 0.0012\n",
      "Epoch 003/010 | Batch: 550/938 | loss: 0.0436\n",
      "Epoch 003/010 | Batch: 600/938 | loss: 0.0004\n",
      "Epoch 003/010 | Batch: 650/938 | loss: 0.0179\n",
      "Epoch 003/010 | Batch: 700/938 | loss: 0.0016\n",
      "Epoch 003/010 | Batch: 750/938 | loss: 0.0013\n",
      "Epoch 003/010 | Batch: 800/938 | loss: 0.0026\n",
      "Epoch 003/010 | Batch: 850/938 | loss: 0.0101\n",
      "Epoch 003/010 | Batch: 900/938 | loss: 0.0001\n",
      "Epoch: 003/010 training accuracy: 99.79%\n",
      "Time elapsed: 1.23 min \n",
      "Epoch 004/010 | Batch: 000/938 | loss: 0.0171\n",
      "Epoch 004/010 | Batch: 050/938 | loss: 0.0077\n",
      "Epoch 004/010 | Batch: 100/938 | loss: 0.0024\n",
      "Epoch 004/010 | Batch: 150/938 | loss: 0.0004\n",
      "Epoch 004/010 | Batch: 200/938 | loss: 0.0513\n",
      "Epoch 004/010 | Batch: 250/938 | loss: 0.0008\n",
      "Epoch 004/010 | Batch: 300/938 | loss: 0.0002\n",
      "Epoch 004/010 | Batch: 350/938 | loss: 0.0303\n",
      "Epoch 004/010 | Batch: 400/938 | loss: 0.0013\n",
      "Epoch 004/010 | Batch: 450/938 | loss: 0.0139\n",
      "Epoch 004/010 | Batch: 500/938 | loss: 0.0005\n",
      "Epoch 004/010 | Batch: 550/938 | loss: 0.0024\n",
      "Epoch 004/010 | Batch: 600/938 | loss: 0.0011\n",
      "Epoch 004/010 | Batch: 650/938 | loss: 0.0014\n",
      "Epoch 004/010 | Batch: 700/938 | loss: 0.0006\n",
      "Epoch 004/010 | Batch: 750/938 | loss: 0.0004\n",
      "Epoch 004/010 | Batch: 800/938 | loss: 0.0000\n",
      "Epoch 004/010 | Batch: 850/938 | loss: 0.0017\n",
      "Epoch 004/010 | Batch: 900/938 | loss: 0.0000\n",
      "Epoch: 004/010 training accuracy: 99.58%\n",
      "Time elapsed: 1.65 min \n",
      "Epoch 005/010 | Batch: 000/938 | loss: 0.0002\n",
      "Epoch 005/010 | Batch: 050/938 | loss: 0.0047\n",
      "Epoch 005/010 | Batch: 100/938 | loss: 0.0007\n",
      "Epoch 005/010 | Batch: 150/938 | loss: 0.0002\n",
      "Epoch 005/010 | Batch: 200/938 | loss: 0.0001\n",
      "Epoch 005/010 | Batch: 250/938 | loss: 0.0006\n",
      "Epoch 005/010 | Batch: 300/938 | loss: 0.0010\n",
      "Epoch 005/010 | Batch: 350/938 | loss: 0.0079\n",
      "Epoch 005/010 | Batch: 400/938 | loss: 0.0001\n",
      "Epoch 005/010 | Batch: 450/938 | loss: 0.0765\n",
      "Epoch 005/010 | Batch: 500/938 | loss: 0.0176\n",
      "Epoch 005/010 | Batch: 550/938 | loss: 0.0451\n",
      "Epoch 005/010 | Batch: 600/938 | loss: 0.0000\n",
      "Epoch 005/010 | Batch: 650/938 | loss: 0.0296\n",
      "Epoch 005/010 | Batch: 700/938 | loss: 0.0113\n",
      "Epoch 005/010 | Batch: 750/938 | loss: 0.0000\n",
      "Epoch 005/010 | Batch: 800/938 | loss: 0.0012\n",
      "Epoch 005/010 | Batch: 850/938 | loss: 0.0400\n",
      "Epoch 005/010 | Batch: 900/938 | loss: 0.0330\n",
      "Epoch: 005/010 training accuracy: 99.68%\n",
      "Time elapsed: 2.08 min \n",
      "Epoch 006/010 | Batch: 000/938 | loss: 0.0004\n",
      "Epoch 006/010 | Batch: 050/938 | loss: 0.0007\n",
      "Epoch 006/010 | Batch: 100/938 | loss: 0.0194\n",
      "Epoch 006/010 | Batch: 150/938 | loss: 0.0484\n",
      "Epoch 006/010 | Batch: 200/938 | loss: 0.0002\n",
      "Epoch 006/010 | Batch: 250/938 | loss: 0.0511\n",
      "Epoch 006/010 | Batch: 300/938 | loss: 0.0001\n",
      "Epoch 006/010 | Batch: 350/938 | loss: 0.0012\n",
      "Epoch 006/010 | Batch: 400/938 | loss: 0.0439\n",
      "Epoch 006/010 | Batch: 450/938 | loss: 0.0605\n",
      "Epoch 006/010 | Batch: 500/938 | loss: 0.0003\n",
      "Epoch 006/010 | Batch: 550/938 | loss: 0.0000\n",
      "Epoch 006/010 | Batch: 600/938 | loss: 0.0021\n",
      "Epoch 006/010 | Batch: 650/938 | loss: 0.0029\n",
      "Epoch 006/010 | Batch: 700/938 | loss: 0.0055\n",
      "Epoch 006/010 | Batch: 750/938 | loss: 0.0000\n",
      "Epoch 006/010 | Batch: 800/938 | loss: 0.0000\n",
      "Epoch 006/010 | Batch: 850/938 | loss: 0.0019\n",
      "Epoch 006/010 | Batch: 900/938 | loss: 0.0045\n",
      "Epoch: 006/010 training accuracy: 99.58%\n",
      "Time elapsed: 2.54 min \n",
      "Epoch 007/010 | Batch: 000/938 | loss: 0.0028\n",
      "Epoch 007/010 | Batch: 050/938 | loss: 0.0384\n",
      "Epoch 007/010 | Batch: 100/938 | loss: 0.0055\n",
      "Epoch 007/010 | Batch: 150/938 | loss: 0.0028\n",
      "Epoch 007/010 | Batch: 200/938 | loss: 0.0023\n",
      "Epoch 007/010 | Batch: 250/938 | loss: 0.0005\n",
      "Epoch 007/010 | Batch: 300/938 | loss: 0.0005\n",
      "Epoch 007/010 | Batch: 350/938 | loss: 0.0014\n",
      "Epoch 007/010 | Batch: 400/938 | loss: 0.0284\n",
      "Epoch 007/010 | Batch: 450/938 | loss: 0.0001\n",
      "Epoch 007/010 | Batch: 500/938 | loss: 0.0001\n",
      "Epoch 007/010 | Batch: 550/938 | loss: 0.0123\n",
      "Epoch 007/010 | Batch: 600/938 | loss: 0.0007\n",
      "Epoch 007/010 | Batch: 650/938 | loss: 0.0013\n",
      "Epoch 007/010 | Batch: 700/938 | loss: 0.0002\n",
      "Epoch 007/010 | Batch: 750/938 | loss: 0.0000\n",
      "Epoch 007/010 | Batch: 800/938 | loss: 0.0527\n",
      "Epoch 007/010 | Batch: 850/938 | loss: 0.0212\n",
      "Epoch 007/010 | Batch: 900/938 | loss: 0.0001\n",
      "Epoch: 007/010 training accuracy: 99.56%\n",
      "Time elapsed: 2.97 min \n",
      "Epoch 008/010 | Batch: 000/938 | loss: 0.1188\n",
      "Epoch 008/010 | Batch: 050/938 | loss: 0.0001\n",
      "Epoch 008/010 | Batch: 100/938 | loss: 0.0008\n",
      "Epoch 008/010 | Batch: 150/938 | loss: 0.0004\n",
      "Epoch 008/010 | Batch: 200/938 | loss: 0.0006\n",
      "Epoch 008/010 | Batch: 250/938 | loss: 0.0001\n",
      "Epoch 008/010 | Batch: 300/938 | loss: 0.0789\n",
      "Epoch 008/010 | Batch: 350/938 | loss: 0.0001\n",
      "Epoch 008/010 | Batch: 400/938 | loss: 0.0003\n",
      "Epoch 008/010 | Batch: 450/938 | loss: 0.0910\n",
      "Epoch 008/010 | Batch: 500/938 | loss: 0.0010\n",
      "Epoch 008/010 | Batch: 550/938 | loss: 0.0564\n",
      "Epoch 008/010 | Batch: 600/938 | loss: 0.0001\n",
      "Epoch 008/010 | Batch: 650/938 | loss: 0.0000\n",
      "Epoch 008/010 | Batch: 700/938 | loss: 0.0745\n",
      "Epoch 008/010 | Batch: 750/938 | loss: 0.0157\n",
      "Epoch 008/010 | Batch: 800/938 | loss: 0.0001\n",
      "Epoch 008/010 | Batch: 850/938 | loss: 0.0015\n",
      "Epoch 008/010 | Batch: 900/938 | loss: 0.0328\n",
      "Epoch: 008/010 training accuracy: 99.63%\n",
      "Time elapsed: 3.39 min \n",
      "Epoch 009/010 | Batch: 000/938 | loss: 0.1286\n",
      "Epoch 009/010 | Batch: 050/938 | loss: 0.0012\n",
      "Epoch 009/010 | Batch: 100/938 | loss: 0.0018\n",
      "Epoch 009/010 | Batch: 150/938 | loss: 0.0925\n",
      "Epoch 009/010 | Batch: 200/938 | loss: 0.0055\n",
      "Epoch 009/010 | Batch: 250/938 | loss: 0.0255\n",
      "Epoch 009/010 | Batch: 300/938 | loss: 0.1063\n",
      "Epoch 009/010 | Batch: 350/938 | loss: 0.0012\n",
      "Epoch 009/010 | Batch: 400/938 | loss: 0.0004\n",
      "Epoch 009/010 | Batch: 450/938 | loss: 0.0001\n",
      "Epoch 009/010 | Batch: 500/938 | loss: 0.0001\n",
      "Epoch 009/010 | Batch: 550/938 | loss: 0.0238\n",
      "Epoch 009/010 | Batch: 600/938 | loss: 0.0000\n",
      "Epoch 009/010 | Batch: 650/938 | loss: 0.0000\n",
      "Epoch 009/010 | Batch: 700/938 | loss: 0.0037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009/010 | Batch: 750/938 | loss: 0.0152\n",
      "Epoch 009/010 | Batch: 800/938 | loss: 0.0002\n",
      "Epoch 009/010 | Batch: 850/938 | loss: 0.0019\n",
      "Epoch 009/010 | Batch: 900/938 | loss: 0.0001\n",
      "Epoch: 009/010 training accuracy: 99.69%\n",
      "Time elapsed: 3.82 min \n",
      "Epoch 010/010 | Batch: 000/938 | loss: 0.0000\n",
      "Epoch 010/010 | Batch: 050/938 | loss: 0.0027\n",
      "Epoch 010/010 | Batch: 100/938 | loss: 0.0000\n",
      "Epoch 010/010 | Batch: 150/938 | loss: 0.0503\n",
      "Epoch 010/010 | Batch: 200/938 | loss: 0.0000\n",
      "Epoch 010/010 | Batch: 250/938 | loss: 0.0031\n",
      "Epoch 010/010 | Batch: 300/938 | loss: 0.0058\n",
      "Epoch 010/010 | Batch: 350/938 | loss: 0.0000\n",
      "Epoch 010/010 | Batch: 400/938 | loss: 0.0016\n",
      "Epoch 010/010 | Batch: 450/938 | loss: 0.0000\n",
      "Epoch 010/010 | Batch: 500/938 | loss: 0.0065\n",
      "Epoch 010/010 | Batch: 550/938 | loss: 0.0000\n",
      "Epoch 010/010 | Batch: 600/938 | loss: 0.0005\n",
      "Epoch 010/010 | Batch: 650/938 | loss: 0.0001\n",
      "Epoch 010/010 | Batch: 700/938 | loss: 0.0139\n",
      "Epoch 010/010 | Batch: 750/938 | loss: 0.0082\n",
      "Epoch 010/010 | Batch: 800/938 | loss: 0.0116\n",
      "Epoch 010/010 | Batch: 850/938 | loss: 0.0574\n",
      "Epoch 010/010 | Batch: 900/938 | loss: 0.0000\n",
      "Epoch: 010/010 training accuracy: 99.86%\n",
      "Time elapsed: 4.24 min \n",
      "Total training time: 1558724693.66 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward and Back pass\n",
    "        outputs, probas = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        if not i % 50:\n",
    "            print('Epoch %03d/%03d | Batch: %03d/%03d | loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i, total_step, loss))\n",
    "    print('Epoch: %03d/%03d training accuracy: %.2f%%' %(\n",
    "    epoch+1, num_epochs, compute_accuracy(model, train_loader)))\n",
    "    print('Time elapsed: %.2f min ' % ((time.time() - start_time) / 60))\n",
    "print('Total training time: %.2f min' %(time.time() - start_time / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  98.02%\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy\n",
    "print(\"Test Accuracy:  %.2f%%\" %(compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
