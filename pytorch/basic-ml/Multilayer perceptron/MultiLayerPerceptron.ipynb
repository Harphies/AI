{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer percepron: A class of fully connected layers also known as FeedForward Neural Networks\n",
    "\n",
    "#####  Feedforward Network:  \n",
    "is a function approximation machine that are designed to achieve statistical generalization, occasionally drawn some insights from what we known about the brain, rather than as models of brain function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages selection\n",
    "- The first things is to import all the neccesary packages needed for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# select GPU when cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings\n",
    "- Configure the device\n",
    "- define all the hyperparameters to be used and needs to be tuned to achive a better accuracy\n",
    "- Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimension torch.Size([64, 1, 28, 28])\n",
      "image lable dimension torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1 # for generating random number\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64 #power of 2 is prefferable\n",
    "\n",
    "# Model Architecture parameters\n",
    "num_features = 784 \n",
    "num_hidden_1 = 128 \n",
    "num_hidden_2 = 256\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "# dataset -> MNIST\n",
    "# Note: tranforms.ToTensor() scale images to 0-1 range\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data',\n",
    "                              train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "# check the dataset\n",
    "for images, labels in train_loader:\n",
    "    print('Image batch dimension', images.shape)\n",
    "    print('image lable dimension', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the architecture of the model such as\n",
    "- The number of input layers; which is determined by the features of the data\n",
    "- Number of total hidden layers in the model (iterative)\n",
    "- Number of hidden units in each layers (iterative)\n",
    "- The output layer node units is determined by the intended outcome to achieve\n",
    "\n",
    "### Here: we build a 3 layers multilayer perceptron i.e 2 hidden layers and 1 output layer\n",
    "#####  Note: We don't count the input layer as part of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Architecture\n",
    "X -> Linear -> Relu -> Linear -> Relu -> Linear -> softmax -> y\n",
    "\"\"\"\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        \n",
    "        \"\"\"\n",
    "        Basic defintion of each layers parameters\n",
    "        \"\"\"\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        \n",
    "        # 1st Hidden Layer\n",
    "        self.linear_1 = nn.Linear(num_features, num_hidden_1)\n",
    "        \n",
    "        # 2nd Hidden layer\n",
    "        self.linear_2 = nn.Linear(num_hidden_1, num_hidden_2)\n",
    "        \n",
    "        # output layer\n",
    "        self.linear_out = nn.Linear(num_hidden_2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Link all the layers together\n",
    "        \"\"\"\n",
    "        out = self.linear_1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear_2(out)\n",
    "        out = F.relu(out)\n",
    "        outputs = self.linear_out(out)\n",
    "        probas = F.log_softmax(outputs, dim=1)\n",
    "        return outputs, probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and optimizer\n",
    "- Instantiate the model\n",
    "- define the specific Loss function to be used either cross entropy, MSELoss, etc\n",
    "- define the optimization algorithm to be used either SGD, Adam, RMSprop, Momentum et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed) # generate a random number\n",
    "model = MultiLayerPerceptron(num_features=num_features,\n",
    "                            num_classes=num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute accuracy\n",
    "- A function to compute train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(net, data_loader):\n",
    "    net.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features = features.view(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs, probas = net(features)\n",
    "            _, predicted_labels = torch.max(probas, 1)\n",
    "            num_examples += labels.size(0)\n",
    "            correct_pred += (predicted_labels == labels).sum()\n",
    "        return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a model requires the following steps\n",
    "- Reset all the gradients to zero (0)\n",
    "- Make a forward pass (make a prediction)\n",
    "- Calculate the loss\n",
    "- Perform back propagation\n",
    "- Update all the parameters (weight and biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 000/938 | Loss: 2.3075\n",
      "Epoch: 001/010 | Batch 050/938 | Loss: 0.4381\n",
      "Epoch: 001/010 | Batch 100/938 | Loss: 0.3703\n",
      "Epoch: 001/010 | Batch 150/938 | Loss: 0.3457\n",
      "Epoch: 001/010 | Batch 200/938 | Loss: 0.2026\n",
      "Epoch: 001/010 | Batch 250/938 | Loss: 0.2958\n",
      "Epoch: 001/010 | Batch 300/938 | Loss: 0.2289\n",
      "Epoch: 001/010 | Batch 350/938 | Loss: 0.0571\n",
      "Epoch: 001/010 | Batch 400/938 | Loss: 0.1543\n",
      "Epoch: 001/010 | Batch 450/938 | Loss: 0.2024\n",
      "Epoch: 001/010 | Batch 500/938 | Loss: 0.2336\n",
      "Epoch: 001/010 | Batch 550/938 | Loss: 0.1635\n",
      "Epoch: 001/010 | Batch 600/938 | Loss: 0.1554\n",
      "Epoch: 001/010 | Batch 650/938 | Loss: 0.2012\n",
      "Epoch: 001/010 | Batch 700/938 | Loss: 0.1284\n",
      "Epoch: 001/010 | Batch 750/938 | Loss: 0.2299\n",
      "Epoch: 001/010 | Batch 800/938 | Loss: 0.1116\n",
      "Epoch: 001/010 | Batch 850/938 | Loss: 0.0329\n",
      "Epoch: 001/010 | Batch 900/938 | Loss: 0.2301\n",
      "Epoch: 001/010 training accuracy: 96.18%\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 002/010 | Batch 000/938 | Loss: 0.1135\n",
      "Epoch: 002/010 | Batch 050/938 | Loss: 0.1005\n",
      "Epoch: 002/010 | Batch 100/938 | Loss: 0.1593\n",
      "Epoch: 002/010 | Batch 150/938 | Loss: 0.2462\n",
      "Epoch: 002/010 | Batch 200/938 | Loss: 0.1464\n",
      "Epoch: 002/010 | Batch 250/938 | Loss: 0.2016\n",
      "Epoch: 002/010 | Batch 300/938 | Loss: 0.0441\n",
      "Epoch: 002/010 | Batch 350/938 | Loss: 0.0644\n",
      "Epoch: 002/010 | Batch 400/938 | Loss: 0.0831\n",
      "Epoch: 002/010 | Batch 450/938 | Loss: 0.1284\n",
      "Epoch: 002/010 | Batch 500/938 | Loss: 0.0465\n",
      "Epoch: 002/010 | Batch 550/938 | Loss: 0.1464\n",
      "Epoch: 002/010 | Batch 600/938 | Loss: 0.1777\n",
      "Epoch: 002/010 | Batch 650/938 | Loss: 0.1096\n",
      "Epoch: 002/010 | Batch 700/938 | Loss: 0.0607\n",
      "Epoch: 002/010 | Batch 750/938 | Loss: 0.2990\n",
      "Epoch: 002/010 | Batch 800/938 | Loss: 0.2667\n",
      "Epoch: 002/010 | Batch 850/938 | Loss: 0.0546\n",
      "Epoch: 002/010 | Batch 900/938 | Loss: 0.1349\n",
      "Epoch: 002/010 training accuracy: 97.43%\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 003/010 | Batch 000/938 | Loss: 0.0386\n",
      "Epoch: 003/010 | Batch 050/938 | Loss: 0.0187\n",
      "Epoch: 003/010 | Batch 100/938 | Loss: 0.0340\n",
      "Epoch: 003/010 | Batch 150/938 | Loss: 0.1001\n",
      "Epoch: 003/010 | Batch 200/938 | Loss: 0.0971\n",
      "Epoch: 003/010 | Batch 250/938 | Loss: 0.0287\n",
      "Epoch: 003/010 | Batch 300/938 | Loss: 0.0608\n",
      "Epoch: 003/010 | Batch 350/938 | Loss: 0.2194\n",
      "Epoch: 003/010 | Batch 400/938 | Loss: 0.1168\n",
      "Epoch: 003/010 | Batch 450/938 | Loss: 0.1597\n",
      "Epoch: 003/010 | Batch 500/938 | Loss: 0.0990\n",
      "Epoch: 003/010 | Batch 550/938 | Loss: 0.0505\n",
      "Epoch: 003/010 | Batch 600/938 | Loss: 0.0740\n",
      "Epoch: 003/010 | Batch 650/938 | Loss: 0.0375\n",
      "Epoch: 003/010 | Batch 700/938 | Loss: 0.0543\n",
      "Epoch: 003/010 | Batch 750/938 | Loss: 0.0701\n",
      "Epoch: 003/010 | Batch 800/938 | Loss: 0.0430\n",
      "Epoch: 003/010 | Batch 850/938 | Loss: 0.0545\n",
      "Epoch: 003/010 | Batch 900/938 | Loss: 0.0665\n",
      "Epoch: 003/010 training accuracy: 98.36%\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 004/010 | Batch 000/938 | Loss: 0.0094\n",
      "Epoch: 004/010 | Batch 050/938 | Loss: 0.0347\n",
      "Epoch: 004/010 | Batch 100/938 | Loss: 0.0604\n",
      "Epoch: 004/010 | Batch 150/938 | Loss: 0.1752\n",
      "Epoch: 004/010 | Batch 200/938 | Loss: 0.0325\n",
      "Epoch: 004/010 | Batch 250/938 | Loss: 0.0065\n",
      "Epoch: 004/010 | Batch 300/938 | Loss: 0.0459\n",
      "Epoch: 004/010 | Batch 350/938 | Loss: 0.0448\n",
      "Epoch: 004/010 | Batch 400/938 | Loss: 0.1558\n",
      "Epoch: 004/010 | Batch 450/938 | Loss: 0.0123\n",
      "Epoch: 004/010 | Batch 500/938 | Loss: 0.0410\n",
      "Epoch: 004/010 | Batch 550/938 | Loss: 0.0204\n",
      "Epoch: 004/010 | Batch 600/938 | Loss: 0.0068\n",
      "Epoch: 004/010 | Batch 650/938 | Loss: 0.0112\n",
      "Epoch: 004/010 | Batch 700/938 | Loss: 0.1368\n",
      "Epoch: 004/010 | Batch 750/938 | Loss: 0.0988\n",
      "Epoch: 004/010 | Batch 800/938 | Loss: 0.0495\n",
      "Epoch: 004/010 | Batch 850/938 | Loss: 0.0202\n",
      "Epoch: 004/010 | Batch 900/938 | Loss: 0.1027\n",
      "Epoch: 004/010 training accuracy: 98.65%\n",
      "Time elapsed: 1.47 min\n",
      "Epoch: 005/010 | Batch 000/938 | Loss: 0.0515\n",
      "Epoch: 005/010 | Batch 050/938 | Loss: 0.0712\n",
      "Epoch: 005/010 | Batch 100/938 | Loss: 0.0053\n",
      "Epoch: 005/010 | Batch 150/938 | Loss: 0.0155\n",
      "Epoch: 005/010 | Batch 200/938 | Loss: 0.1059\n",
      "Epoch: 005/010 | Batch 250/938 | Loss: 0.0774\n",
      "Epoch: 005/010 | Batch 300/938 | Loss: 0.0299\n",
      "Epoch: 005/010 | Batch 350/938 | Loss: 0.1275\n",
      "Epoch: 005/010 | Batch 400/938 | Loss: 0.0053\n",
      "Epoch: 005/010 | Batch 450/938 | Loss: 0.0615\n",
      "Epoch: 005/010 | Batch 500/938 | Loss: 0.0501\n",
      "Epoch: 005/010 | Batch 550/938 | Loss: 0.0467\n",
      "Epoch: 005/010 | Batch 600/938 | Loss: 0.0456\n",
      "Epoch: 005/010 | Batch 650/938 | Loss: 0.0113\n",
      "Epoch: 005/010 | Batch 700/938 | Loss: 0.0181\n",
      "Epoch: 005/010 | Batch 750/938 | Loss: 0.0325\n",
      "Epoch: 005/010 | Batch 800/938 | Loss: 0.0268\n",
      "Epoch: 005/010 | Batch 850/938 | Loss: 0.0030\n",
      "Epoch: 005/010 | Batch 900/938 | Loss: 0.0530\n",
      "Epoch: 005/010 training accuracy: 98.63%\n",
      "Time elapsed: 1.85 min\n",
      "Epoch: 006/010 | Batch 000/938 | Loss: 0.0056\n",
      "Epoch: 006/010 | Batch 050/938 | Loss: 0.0359\n",
      "Epoch: 006/010 | Batch 100/938 | Loss: 0.0057\n",
      "Epoch: 006/010 | Batch 150/938 | Loss: 0.0094\n",
      "Epoch: 006/010 | Batch 200/938 | Loss: 0.0093\n",
      "Epoch: 006/010 | Batch 250/938 | Loss: 0.0659\n",
      "Epoch: 006/010 | Batch 300/938 | Loss: 0.0088\n",
      "Epoch: 006/010 | Batch 350/938 | Loss: 0.0355\n",
      "Epoch: 006/010 | Batch 400/938 | Loss: 0.0064\n",
      "Epoch: 006/010 | Batch 450/938 | Loss: 0.0025\n",
      "Epoch: 006/010 | Batch 500/938 | Loss: 0.0963\n",
      "Epoch: 006/010 | Batch 550/938 | Loss: 0.0243\n",
      "Epoch: 006/010 | Batch 600/938 | Loss: 0.0948\n",
      "Epoch: 006/010 | Batch 650/938 | Loss: 0.0036\n",
      "Epoch: 006/010 | Batch 700/938 | Loss: 0.0011\n",
      "Epoch: 006/010 | Batch 750/938 | Loss: 0.0638\n",
      "Epoch: 006/010 | Batch 800/938 | Loss: 0.0650\n",
      "Epoch: 006/010 | Batch 850/938 | Loss: 0.1021\n",
      "Epoch: 006/010 | Batch 900/938 | Loss: 0.0545\n",
      "Epoch: 006/010 training accuracy: 99.14%\n",
      "Time elapsed: 2.22 min\n",
      "Epoch: 007/010 | Batch 000/938 | Loss: 0.0150\n",
      "Epoch: 007/010 | Batch 050/938 | Loss: 0.0573\n",
      "Epoch: 007/010 | Batch 100/938 | Loss: 0.0378\n",
      "Epoch: 007/010 | Batch 150/938 | Loss: 0.0490\n",
      "Epoch: 007/010 | Batch 200/938 | Loss: 0.0305\n",
      "Epoch: 007/010 | Batch 250/938 | Loss: 0.0038\n",
      "Epoch: 007/010 | Batch 300/938 | Loss: 0.0030\n",
      "Epoch: 007/010 | Batch 350/938 | Loss: 0.0865\n",
      "Epoch: 007/010 | Batch 400/938 | Loss: 0.0141\n",
      "Epoch: 007/010 | Batch 450/938 | Loss: 0.0301\n",
      "Epoch: 007/010 | Batch 500/938 | Loss: 0.0483\n",
      "Epoch: 007/010 | Batch 550/938 | Loss: 0.0035\n",
      "Epoch: 007/010 | Batch 600/938 | Loss: 0.0044\n",
      "Epoch: 007/010 | Batch 650/938 | Loss: 0.0237\n",
      "Epoch: 007/010 | Batch 700/938 | Loss: 0.0276\n",
      "Epoch: 007/010 | Batch 750/938 | Loss: 0.0136\n",
      "Epoch: 007/010 | Batch 800/938 | Loss: 0.0023\n",
      "Epoch: 007/010 | Batch 850/938 | Loss: 0.0161\n",
      "Epoch: 007/010 | Batch 900/938 | Loss: 0.0959\n",
      "Epoch: 007/010 training accuracy: 98.22%\n",
      "Time elapsed: 2.60 min\n",
      "Epoch: 008/010 | Batch 000/938 | Loss: 0.0017\n",
      "Epoch: 008/010 | Batch 050/938 | Loss: 0.0568\n",
      "Epoch: 008/010 | Batch 100/938 | Loss: 0.0254\n",
      "Epoch: 008/010 | Batch 150/938 | Loss: 0.0155\n",
      "Epoch: 008/010 | Batch 200/938 | Loss: 0.0115\n",
      "Epoch: 008/010 | Batch 250/938 | Loss: 0.0056\n",
      "Epoch: 008/010 | Batch 300/938 | Loss: 0.0060\n",
      "Epoch: 008/010 | Batch 350/938 | Loss: 0.0180\n",
      "Epoch: 008/010 | Batch 400/938 | Loss: 0.0710\n",
      "Epoch: 008/010 | Batch 450/938 | Loss: 0.0123\n",
      "Epoch: 008/010 | Batch 500/938 | Loss: 0.0539\n",
      "Epoch: 008/010 | Batch 550/938 | Loss: 0.0330\n",
      "Epoch: 008/010 | Batch 600/938 | Loss: 0.0462\n",
      "Epoch: 008/010 | Batch 650/938 | Loss: 0.0288\n",
      "Epoch: 008/010 | Batch 700/938 | Loss: 0.0195\n",
      "Epoch: 008/010 | Batch 750/938 | Loss: 0.0013\n",
      "Epoch: 008/010 | Batch 800/938 | Loss: 0.1235\n",
      "Epoch: 008/010 | Batch 850/938 | Loss: 0.0138\n",
      "Epoch: 008/010 | Batch 900/938 | Loss: 0.0012\n",
      "Epoch: 008/010 training accuracy: 99.16%\n",
      "Time elapsed: 2.98 min\n",
      "Epoch: 009/010 | Batch 000/938 | Loss: 0.0350\n",
      "Epoch: 009/010 | Batch 050/938 | Loss: 0.0028\n",
      "Epoch: 009/010 | Batch 100/938 | Loss: 0.0090\n",
      "Epoch: 009/010 | Batch 150/938 | Loss: 0.0400\n",
      "Epoch: 009/010 | Batch 200/938 | Loss: 0.0125\n",
      "Epoch: 009/010 | Batch 250/938 | Loss: 0.0010\n",
      "Epoch: 009/010 | Batch 300/938 | Loss: 0.0619\n",
      "Epoch: 009/010 | Batch 350/938 | Loss: 0.0889\n",
      "Epoch: 009/010 | Batch 400/938 | Loss: 0.0016\n",
      "Epoch: 009/010 | Batch 450/938 | Loss: 0.0219\n",
      "Epoch: 009/010 | Batch 500/938 | Loss: 0.0020\n",
      "Epoch: 009/010 | Batch 550/938 | Loss: 0.0084\n",
      "Epoch: 009/010 | Batch 600/938 | Loss: 0.0494\n",
      "Epoch: 009/010 | Batch 650/938 | Loss: 0.0016\n",
      "Epoch: 009/010 | Batch 700/938 | Loss: 0.0148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009/010 | Batch 750/938 | Loss: 0.0034\n",
      "Epoch: 009/010 | Batch 800/938 | Loss: 0.0218\n",
      "Epoch: 009/010 | Batch 850/938 | Loss: 0.0013\n",
      "Epoch: 009/010 | Batch 900/938 | Loss: 0.0197\n",
      "Epoch: 009/010 training accuracy: 99.25%\n",
      "Time elapsed: 3.37 min\n",
      "Epoch: 010/010 | Batch 000/938 | Loss: 0.0447\n",
      "Epoch: 010/010 | Batch 050/938 | Loss: 0.0014\n",
      "Epoch: 010/010 | Batch 100/938 | Loss: 0.0030\n",
      "Epoch: 010/010 | Batch 150/938 | Loss: 0.0081\n",
      "Epoch: 010/010 | Batch 200/938 | Loss: 0.0262\n",
      "Epoch: 010/010 | Batch 250/938 | Loss: 0.0040\n",
      "Epoch: 010/010 | Batch 300/938 | Loss: 0.0163\n",
      "Epoch: 010/010 | Batch 350/938 | Loss: 0.0130\n",
      "Epoch: 010/010 | Batch 400/938 | Loss: 0.0291\n",
      "Epoch: 010/010 | Batch 450/938 | Loss: 0.0013\n",
      "Epoch: 010/010 | Batch 500/938 | Loss: 0.0002\n",
      "Epoch: 010/010 | Batch 550/938 | Loss: 0.0742\n",
      "Epoch: 010/010 | Batch 600/938 | Loss: 0.0077\n",
      "Epoch: 010/010 | Batch 650/938 | Loss: 0.0164\n",
      "Epoch: 010/010 | Batch 700/938 | Loss: 0.0056\n",
      "Epoch: 010/010 | Batch 750/938 | Loss: 0.0247\n",
      "Epoch: 010/010 | Batch 800/938 | Loss: 0.1012\n",
      "Epoch: 010/010 | Batch 850/938 | Loss: 0.0275\n",
      "Epoch: 010/010 | Batch 900/938 | Loss: 0.0008\n",
      "Epoch: 010/010 training accuracy: 99.50%\n",
      "Time elapsed: 3.75 min\n",
      "Total Training Time: 3.75 min \n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "total_step = len(train_loader)\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs, probas = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        if not i % 50:\n",
    "            print('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i, total_step, loss))\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print('Epoch: %03d/%03d training accuracy: %.2f%%' %(\n",
    "        epoch+1, num_epochs, compute_accuracy(model, train_loader)))\n",
    "        \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time) / 60))\n",
    "print('Total Training Time: %.2f min ' % ((time.time() - start_time) / 60))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.80%\n"
     ]
    }
   ],
   "source": [
    "# Print the test accuracy\n",
    "print('Test Accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
