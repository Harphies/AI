{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An implementation of logistics regression for binary class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ada97417d179>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#DATASET\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mByteIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimeter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dev\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    622\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data not found."
     ]
    }
   ],
   "source": [
    "# preparing a toy dataset\n",
    "#DATASET\n",
    "ds = np.lib.DataSource()\n",
    "fp = ds.open('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data')\n",
    "\n",
    "x = np.genfromtxt(ByteIO(fp.read().encode()), delimeter=',', usecols=range(2), max_rows=100)\n",
    "y = np.zeros(100)\n",
    "y[50:] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "idx = np.arange(y.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "X_test, y_test = x[idx[:25]], y[idx[:25]]\n",
    "X_train , y_train = x[idx[25:]], y[idx[25:]]\n",
    "mu, std = np.mean(X_train, axis=0), np.std(X_train, axis=0)\n",
    "X_train, X_test = (X_train - mu)/ std, (X_test - mu)/ std\n",
    "\n",
    "# make some plots\n",
    "fig, ax = plt.subplots(1,2, figsize=(7, 2.5))\n",
    "ax[0].scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1])\n",
    "ax[0].scatter(X_train[y_train == 0,0], X_train[y_train == 0,1])\n",
    "ax[1].scatter(X_test[y_test == 1,0], X_test[y_test == 1,1])\n",
    "ax[1].scatter(X_test[y_test == 0,0], X_test[y_test == 0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low level implementation with manual gradients\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def custom_where(cond, x_1, x_2):\n",
    "    return (cond * x_1) + ((1-cond) * x_2)\n",
    "\n",
    "class LogisticRegression1():\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weights = torch.zeros(num_features, 1, \n",
    "                                  dtype=torch.float32, device=device)\n",
    "        self.bias = torch.zeros(1, dtype=torch.float32, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = torch.add(torch.mm(x, self.weights), self.bias)\n",
    "        probas = self._sigmoid(linear)\n",
    "        return probas\n",
    "    \n",
    "    def backward(self, probas, y):\n",
    "        errors = y - probas.view(-1)\n",
    "        return errors\n",
    "    \n",
    "    def predict_labels(self, x):\n",
    "        probas = self.forward(x)\n",
    "        labels = custom_where(probas >= .5, 1, 0)\n",
    "        return labels\n",
    "    \n",
    "    def evaluate(self, x,y):\n",
    "        labels = self.predict_labels(x).float()\n",
    "        accuracy = torch.sum(labels.view(-1) == y) / y.size()[0]\n",
    "        return accuracy\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return  1./(1 + torch.exp(-z))\n",
    "    \n",
    "    def logits_cost(self, y, proba):\n",
    "        tmp1 = torch.mm(-y.view(1, -1), torch.log(proba))\n",
    "        tmp2 = torch.mm(1 - y).view(1, -1), torch.log(1- proba)\n",
    "        return tmp1 - tmp2\n",
    "    \n",
    "    def train(self, x, y, num_epochs, learning_rate=0.01):\n",
    "        for e in range(num_epochs):\n",
    "            \n",
    "            ## compute your outputs ##\n",
    "            probas = self.forward(x)\n",
    "            \n",
    "            ## compute the gradient ##\n",
    "            errors = self.backward(probas, y)\n",
    "            neg_grad = torch.mm(x.transpose(0,1), errors.view(-1, 1))\n",
    "            \n",
    "            ## update weights ##\n",
    "            self.weights +=learning_rate * neg_gred\n",
    "            self.bias += learning_rate * torch.sum(errors)\n",
    "            \n",
    "            ## Logging ##\n",
    "            print('Epoch: '% (e+1), end=\"\")\n",
    "            print(' | Train ACC: %.3f' % self.evaluate(x,y), end=\"\")\n",
    "            print(' | Cost:%.3f' % self._logit_cost(y, self.forward(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-8ce57474ee08>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-8ce57474ee08>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    logr.train((X_train_tensor, y_train_tensor, num_epochs, learning_rate=0.1))\u001b[0m\n\u001b[1;37m                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# tensors implementation\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "y_train_tensor = torch.tensor(y_train,dtype=torch.float32, device=device)\n",
    "\n",
    "logr = LogisticRegression1(num_features=2)\n",
    "logr.train((X_train_tensor, y_train_tensor, num_epochs, learning_rate=0.1))\n",
    "\n",
    "print ('\\n Model Parameters:')\n",
    "print(' Weight: %s' % logr.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a436121fd37d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Evaluating the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_test_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_test_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "X_test_tensor = torch.tensor(X_test, dtypes=torch.float32, device=device)\n",
    "y_test_tensor = torch.tensor(y_test, dtypes=torch.float32, device=device)\n",
    "\n",
    "test_acc = logr.evaluate(X_test_tensor, y_test_tensor)\n",
    "print('Test set Accuracy: %.2f%%' % (test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-83cbad1783ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 2D decision boundary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mx_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m y_min = ( (-(w[0] * x_min) - b[0])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logr' is not defined"
     ]
    }
   ],
   "source": [
    "# 2D decision boundary\n",
    "\n",
    "w, b = logr.weights, logr.bias \n",
    "x_min = -2\n",
    "y_min = ( (-(w[0] * x_min) - b[0])\n",
    "         /w[1])\n",
    "\n",
    "x_max = 2\n",
    "y_max = ( (-(w[0] * x_max) - b[0])\n",
    "        / w[1] )\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharex=True, figsize=(7,3))\n",
    "\n",
    "ax[0].plot([x_min, x_max], [y_min, y_max])\n",
    "ax[1].plot([x_min, x_max], [y_min, y_max])\n",
    "\n",
    "ax[0].scatter(X_train[y_train==0,0], X_train[y_train==0,1], labels='class 0', marker='o')\n",
    "ax[0].scatter(X_train[y_train==1,0], X_train[y_train==1,1], labels='class 1', marker='s')\n",
    "\n",
    "ax[1].scatter(X_train[y_train==0,0], X_train[y_train==0,1], labels='class 0', marker='0')\n",
    "ax[1].scatter(X_train[y_train==1,0], X_train[y_train==1,1], labels='class 1', marker='s')\n",
    "\n",
    "ax[1].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low level Implementation Using AutoGrad\n",
    "\n",
    "def custom_where(cond, x_1, x_2):\n",
    "    return (cond * x_1) + ((1-cond)* x_2)\n",
    "\n",
    "class LogisticRegression2():\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        self.weights = torch.zeros(num_features, 1,\n",
    "                                  dtype=torch.float32,\n",
    "                                  device=device,\n",
    "                                  requires_grad=True) # autograd require.\n",
    "        self.bias = torch.zeros(1,\n",
    "                               dtype=torch.float32,\n",
    "                               device=device,\n",
    "                               requires_grad=True) # Grad required\n",
    "        # forward propagation\n",
    "        \n",
    "        def forward(self, x):\n",
    "            linear = torch.add(torch.mm(x, self.weights), self.bias)\n",
    "            probas = self._sigmoid(linear)\n",
    "            return probas\n",
    "        \n",
    "        def predict_labels(self, x):\n",
    "            probas = self.forward(x)\n",
    "            labels = custom_where((probas >= .5).float(), 1, 0)\n",
    "            return labels\n",
    "        \n",
    "        def evaluate(self, x, y):\n",
    "            labels = self.predict_labels(x)\n",
    "            accuracy = (torch.sum(labels.view(-1) == y.view(-1))).float() / y.size()[0]\n",
    "            return accuracy\n",
    "        \n",
    "        def _sigmoid(self, z):\n",
    "            return 1. /(1 + torch.exp(-z))\n",
    "        \n",
    "        def _logit_cost(self, y, probas):\n",
    "            tmp1 = torch.mm(-y.view(1, -1), torch.log(proba))\n",
    "            tmp2 = torch.mm((1 - y).view(1, -1), torch.log(1- proba))\n",
    "            return tmp1 - tmp2\n",
    "        \n",
    "        def train(self, x, y , num_epochs, learning_rate=0.01):\n",
    "            \n",
    "            for e in range(num_epochs):\n",
    "                \n",
    "                ## compute the outputs\n",
    "                probas = self.forward(x)\n",
    "                cost = self._logit_cost(y, proba)\n",
    "                \n",
    "                ## Compute gradients  ###\n",
    "                cost.backward()\n",
    "                \n",
    "                ## updates the weights\n",
    "                tmp = self.weights.detach()\n",
    "                tmp -= learning_rate * self.weights.grad\n",
    "                \n",
    "                tmp = self.bias.detach()\n",
    "                tmp -= learning_rate * self.bias.grad\n",
    "                \n",
    "                ## Reset gradients to zero for next iteration ###\n",
    "                self.weights.grad.zero_()\n",
    "                self.bias.grad.zero_()\n",
    "                \n",
    "                # Logging ###\n",
    "                print('Epoch: %03d' % (e+1), end=\"\")\n",
    "                print(' | Train ACC: %.3f' % self.evaluate(x,y), end=\"\")\n",
    "                print(' | Cost: %.3f' % self._logit_cost(y, self.forward(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-fae6bc07393a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_train_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlogr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlogr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype= torch.float32, device=device)\n",
    "\n",
    "logr = LogisticRegression2(num_features=2)\n",
    "logr.train(X_train_tensor, y_train_tensor, num_epochs=10, learning_rate=0.1)\n",
    "\n",
    "print('\\n Model parameters:')\n",
    "print('Weights %s' % logr.weights)\n",
    "print('Bias %s' %logr.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-06ac46ac68c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Model Evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_test_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_test_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoech\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "y_test_tensor = toech.tensor(y_test, dtype=torch.float32,device=device)\n",
    "\n",
    "test_acc = logr.evaluate(X_test_tensor, y_test_tensor)\n",
    "print('Test set accuracy: %.2f%%' % (test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2D decision boundary\n",
    "\n",
    "w, b = logr.weights, logr.bias\n",
    "\n",
    "x_min = -2\n",
    "y_min = ( (-(w[0] * x_min) -b[0])\n",
    "        /w[1])\n",
    "\n",
    "x_max = 2\n",
    "y_max = ( (-(w[0] * x_max) - b[0])\n",
    "        / w[1])\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharex=True, figsize=(7, 3))\n",
    "\n",
    "ax[0].plot([x_min, x_max], [y_min, y_max])\n",
    "ax[1].plot([x_min, x_max], [y_min, y_max])\n",
    "\n",
    "ax[0].scatter(X_train[y_tran==0, 0], X_train[y_train==0, 1], labels='class 0' marker='o')\n",
    "ax[0].scatter(X_train[y_train==1, 0], X_train[y_train==1,1], labels='class 1', marker='s')\n",
    "\n",
    "ax[0].scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], labels='class 0', marker='o')\n",
    "ax[0].scatter(X_test[y_test==1, 0], X_test[y_test==1,1], labels='class 1')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
