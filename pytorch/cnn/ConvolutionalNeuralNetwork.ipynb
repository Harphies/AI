{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project:Convilutional Neural Network\n",
    "Convolutional Neural Network: This is a special kind of Feedforward neural networks often used in solving image related taks and it has been successfuly used for object detection, image recognition and Neural Stlye transfer applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages selection\n",
    "- The first things is to import all the neccesary packages needed for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# select GPU when cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings\n",
    "- Configure the device\n",
    "- define all the hyperparameters to be used and needed to be tuned to achive a better accuracy\n",
    "- Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Batch dimension torch.Size([128, 1, 28, 28])\n",
      "Image labels dimension torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# model architecture parameters\n",
    "num_classes = 10\n",
    "\n",
    "# dataset -> MNIST\n",
    "# Note: Transform.Totensor() scale image from 0-1 range\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data',\n",
    "                              train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "# check the dataset\n",
    "for images, labels in train_loader:\n",
    "    print(\"Image Batch dimension\", images.shape)\n",
    "    print(\"Image labels dimension\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the architecture of the model such as\n",
    "- The number of input layers; which is determined by the features of the data\n",
    "- Number of total hidden layers in the model consist of (Convolution layer, Pooling Layer (Max Pool), Fully connected layer \n",
    "- The output layer node units is determined by the intended outcome to achieve\n",
    "- Here we have two layers convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Architecture\n",
    "X -> Conv layer -> ReLU -> MaxPool -> Conv layer -> Relu -> MaxPool -> FC -> Softmax -> y\n",
    "\n",
    "Note: we use conv2d and MaxPool2d because the input image is 2d image\n",
    "\"\"\"\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # 28x28x1 => 28x28x8\n",
    "        # Convolution layer 1\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1,\n",
    "                               out_channels=8,\n",
    "                               kernel_size=(3, 3),\n",
    "                               stride=(1, 1),\n",
    "                               padding=1)\n",
    "        \n",
    "        # 28x28x8 =>14x14x8\n",
    "        # MaxPool layer 1\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                  stride=(2, 2),\n",
    "                                  padding=0)\n",
    "        \n",
    "        # 14x14x8 => 14x14x16\n",
    "        # Convolution layer 2\n",
    "        self.conv_2 = nn.Conv2d(in_channels=8, \n",
    "                               out_channels=16,\n",
    "                               kernel_size=(3, 3),\n",
    "                               stride=(1, 1),\n",
    "                               padding=1)\n",
    "        \n",
    "        # 14x14x16 =>7x7x16\n",
    "        # MaxPool layer 2\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                  stride=(2, 2),\n",
    "                                  padding=0)\n",
    "        \n",
    "        \n",
    "        # flatten 7x7x16\n",
    "        # fully connected layer 1\n",
    "        self.fc_1 = nn.Linear(7*7*16, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool_1(out)\n",
    "        \n",
    "        out = self.conv_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        \n",
    "        # flatten the output from the final pool layer before connecting to fully connected layer\n",
    "        logits = self.fc_1(out.view(-1, 7*7*16)) \n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and optimizer\n",
    "- Instantiate the model\n",
    "- define the specific Loss function to be used either cross entropy, MSELoss, etc\n",
    "- define the optimization algorithm to be used either SGD, Adam, RMSprop, Momentum etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "model = ConvNet(num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute accuracy\n",
    "- A function to compute train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_prediction, num_examples = 0, 0\n",
    "    for features, labels in data_loader:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += labels.size(0)\n",
    "        correct_prediction += (predicted_labels == labels).sum()\n",
    "    return correct_prediction.float() / num_examples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a model requires the following stepsÂ¶\n",
    "- Reset all the gradients to zero (0)\n",
    "- Make a forward pass (make a prediction)\n",
    "- Calculate the loss\n",
    "- Perform back propagation\n",
    "- Update all the parameters (weight and biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 000/469 | Cost: 2.3044\n",
      "Epoch: 001/010 | Batch 050/469 | Cost: 1.1113\n",
      "Epoch: 001/010 | Batch 100/469 | Cost: 0.3436\n",
      "Epoch: 001/010 | Batch 150/469 | Cost: 0.4722\n",
      "Epoch: 001/010 | Batch 200/469 | Cost: 0.4307\n",
      "Epoch: 001/010 | Batch 250/469 | Cost: 0.2935\n",
      "Epoch: 001/010 | Batch 300/469 | Cost: 0.2337\n",
      "Epoch: 001/010 | Batch 350/469 | Cost: 0.1907\n",
      "Epoch: 001/010 | Batch 400/469 | Cost: 0.2570\n",
      "Epoch: 001/010 | Batch 450/469 | Cost: 0.1821\n",
      "Epoch: 001/010 training accuracy: 94.25%\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 002/010 | Batch 000/469 | Cost: 0.1796\n",
      "Epoch: 002/010 | Batch 050/469 | Cost: 0.2047\n",
      "Epoch: 002/010 | Batch 100/469 | Cost: 0.1672\n",
      "Epoch: 002/010 | Batch 150/469 | Cost: 0.1958\n",
      "Epoch: 002/010 | Batch 200/469 | Cost: 0.0825\n",
      "Epoch: 002/010 | Batch 250/469 | Cost: 0.1655\n",
      "Epoch: 002/010 | Batch 300/469 | Cost: 0.1354\n",
      "Epoch: 002/010 | Batch 350/469 | Cost: 0.1109\n",
      "Epoch: 002/010 | Batch 400/469 | Cost: 0.1238\n",
      "Epoch: 002/010 | Batch 450/469 | Cost: 0.0513\n",
      "Epoch: 002/010 training accuracy: 96.45%\n",
      "Time elapsed: 1.78 min\n",
      "Epoch: 003/010 | Batch 000/469 | Cost: 0.0935\n",
      "Epoch: 003/010 | Batch 050/469 | Cost: 0.1527\n",
      "Epoch: 003/010 | Batch 100/469 | Cost: 0.0785\n",
      "Epoch: 003/010 | Batch 150/469 | Cost: 0.1144\n",
      "Epoch: 003/010 | Batch 200/469 | Cost: 0.1342\n",
      "Epoch: 003/010 | Batch 250/469 | Cost: 0.0638\n",
      "Epoch: 003/010 | Batch 300/469 | Cost: 0.0415\n",
      "Epoch: 003/010 | Batch 350/469 | Cost: 0.0884\n",
      "Epoch: 003/010 | Batch 400/469 | Cost: 0.0562\n",
      "Epoch: 003/010 | Batch 450/469 | Cost: 0.0407\n",
      "Epoch: 003/010 training accuracy: 97.00%\n",
      "Time elapsed: 2.70 min\n",
      "Epoch: 004/010 | Batch 000/469 | Cost: 0.2406\n",
      "Epoch: 004/010 | Batch 050/469 | Cost: 0.0365\n",
      "Epoch: 004/010 | Batch 100/469 | Cost: 0.0396\n",
      "Epoch: 004/010 | Batch 150/469 | Cost: 0.0769\n",
      "Epoch: 004/010 | Batch 200/469 | Cost: 0.1295\n",
      "Epoch: 004/010 | Batch 250/469 | Cost: 0.0907\n",
      "Epoch: 004/010 | Batch 300/469 | Cost: 0.1319\n",
      "Epoch: 004/010 | Batch 350/469 | Cost: 0.0802\n",
      "Epoch: 004/010 | Batch 400/469 | Cost: 0.0863\n",
      "Epoch: 004/010 | Batch 450/469 | Cost: 0.0889\n",
      "Epoch: 004/010 training accuracy: 98.02%\n",
      "Time elapsed: 3.61 min\n",
      "Epoch: 005/010 | Batch 000/469 | Cost: 0.0586\n",
      "Epoch: 005/010 | Batch 050/469 | Cost: 0.0727\n",
      "Epoch: 005/010 | Batch 100/469 | Cost: 0.0765\n",
      "Epoch: 005/010 | Batch 150/469 | Cost: 0.1182\n",
      "Epoch: 005/010 | Batch 200/469 | Cost: 0.0237\n",
      "Epoch: 005/010 | Batch 250/469 | Cost: 0.0923\n",
      "Epoch: 005/010 | Batch 300/469 | Cost: 0.0583\n",
      "Epoch: 005/010 | Batch 350/469 | Cost: 0.0680\n",
      "Epoch: 005/010 | Batch 400/469 | Cost: 0.0186\n",
      "Epoch: 005/010 | Batch 450/469 | Cost: 0.0429\n",
      "Epoch: 005/010 training accuracy: 98.27%\n",
      "Time elapsed: 4.54 min\n",
      "Epoch: 006/010 | Batch 000/469 | Cost: 0.0524\n",
      "Epoch: 006/010 | Batch 050/469 | Cost: 0.0269\n",
      "Epoch: 006/010 | Batch 100/469 | Cost: 0.0785\n",
      "Epoch: 006/010 | Batch 150/469 | Cost: 0.0705\n",
      "Epoch: 006/010 | Batch 200/469 | Cost: 0.0552\n",
      "Epoch: 006/010 | Batch 250/469 | Cost: 0.1072\n",
      "Epoch: 006/010 | Batch 300/469 | Cost: 0.0274\n",
      "Epoch: 006/010 | Batch 350/469 | Cost: 0.0274\n",
      "Epoch: 006/010 | Batch 400/469 | Cost: 0.0457\n",
      "Epoch: 006/010 | Batch 450/469 | Cost: 0.0494\n",
      "Epoch: 006/010 training accuracy: 98.06%\n",
      "Time elapsed: 5.48 min\n",
      "Epoch: 007/010 | Batch 000/469 | Cost: 0.0582\n",
      "Epoch: 007/010 | Batch 050/469 | Cost: 0.0763\n",
      "Epoch: 007/010 | Batch 100/469 | Cost: 0.0156\n",
      "Epoch: 007/010 | Batch 150/469 | Cost: 0.0302\n",
      "Epoch: 007/010 | Batch 200/469 | Cost: 0.0396\n",
      "Epoch: 007/010 | Batch 250/469 | Cost: 0.0455\n",
      "Epoch: 007/010 | Batch 300/469 | Cost: 0.0258\n",
      "Epoch: 007/010 | Batch 350/469 | Cost: 0.0203\n",
      "Epoch: 007/010 | Batch 400/469 | Cost: 0.0314\n",
      "Epoch: 007/010 | Batch 450/469 | Cost: 0.0333\n",
      "Epoch: 007/010 training accuracy: 98.54%\n",
      "Time elapsed: 6.41 min\n",
      "Epoch: 008/010 | Batch 000/469 | Cost: 0.0133\n",
      "Epoch: 008/010 | Batch 050/469 | Cost: 0.0589\n",
      "Epoch: 008/010 | Batch 100/469 | Cost: 0.1131\n",
      "Epoch: 008/010 | Batch 150/469 | Cost: 0.0320\n",
      "Epoch: 008/010 | Batch 200/469 | Cost: 0.0759\n",
      "Epoch: 008/010 | Batch 250/469 | Cost: 0.0516\n",
      "Epoch: 008/010 | Batch 300/469 | Cost: 0.0646\n",
      "Epoch: 008/010 | Batch 350/469 | Cost: 0.0745\n",
      "Epoch: 008/010 | Batch 400/469 | Cost: 0.0345\n",
      "Epoch: 008/010 | Batch 450/469 | Cost: 0.0655\n",
      "Epoch: 008/010 training accuracy: 98.69%\n",
      "Time elapsed: 7.54 min\n",
      "Epoch: 009/010 | Batch 000/469 | Cost: 0.0395\n",
      "Epoch: 009/010 | Batch 050/469 | Cost: 0.0275\n",
      "Epoch: 009/010 | Batch 100/469 | Cost: 0.0682\n",
      "Epoch: 009/010 | Batch 150/469 | Cost: 0.0588\n",
      "Epoch: 009/010 | Batch 200/469 | Cost: 0.0453\n",
      "Epoch: 009/010 | Batch 250/469 | Cost: 0.0374\n",
      "Epoch: 009/010 | Batch 300/469 | Cost: 0.0411\n",
      "Epoch: 009/010 | Batch 350/469 | Cost: 0.0194\n",
      "Epoch: 009/010 | Batch 400/469 | Cost: 0.0493\n",
      "Epoch: 009/010 | Batch 450/469 | Cost: 0.0516\n",
      "Epoch: 009/010 training accuracy: 98.78%\n",
      "Time elapsed: 8.61 min\n",
      "Epoch: 010/010 | Batch 000/469 | Cost: 0.0479\n",
      "Epoch: 010/010 | Batch 050/469 | Cost: 0.0448\n",
      "Epoch: 010/010 | Batch 100/469 | Cost: 0.0956\n",
      "Epoch: 010/010 | Batch 150/469 | Cost: 0.0342\n",
      "Epoch: 010/010 | Batch 200/469 | Cost: 0.0255\n",
      "Epoch: 010/010 | Batch 250/469 | Cost: 0.1173\n",
      "Epoch: 010/010 | Batch 300/469 | Cost: 0.0093\n",
      "Epoch: 010/010 | Batch 350/469 | Cost: 0.0758\n",
      "Epoch: 010/010 | Batch 400/469 | Cost: 0.0118\n",
      "Epoch: 010/010 | Batch 450/469 | Cost: 0.0293\n",
      "Epoch: 010/010 training accuracy: 98.72%\n",
      "Time elapsed: 9.62 min\n",
      "Total Training Time: 9.62 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward and back pass\n",
    "        outputs, probas = model(features)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        if not i % 50:\n",
    "            print('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f'\n",
    "                 %(epoch+1, num_epochs, i, total_step, loss))\n",
    "            \n",
    "    model = model.eval()\n",
    "    print('Epoch: %03d/%03d training accuracy: %.2f%%' %(\n",
    "    epoch+1, num_epochs, compute_accuracy(model, train_loader)))\n",
    "    print('Time elapsed: %.2f min' %((time.time() - start_time) / 60))\n",
    "print('Total Training Time: %.2f min' %((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  98.67%\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy\n",
    "print(\"Test Accuracy:  %.2f%%\" %(compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
