{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project: LeNet-5 classic network digit classifiers with MNIST datasets\n",
    "Here I redesign the LeNet-5 architecture by replacing the pooling layers wiht Maxpooling and the hidden layers with relu instead of tanh and the output layers with softmax layers instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages selection\n",
    "- The first things is to import all the neccesary packages needed for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# select GPU if cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings\n",
    "- Configure the device\n",
    "- define all the hyperparameters to be used and needed to be tuned to achive a better accuracy\n",
    "- Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimension torch.Size([128, 1, 32, 32])\n",
      "Image Labels dimension torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# model architecture parameters\n",
    "NUM_FEATURES = 32*32\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "# others\n",
    "GRAYSCALE = True\n",
    "\n",
    "# rescale the data from 28*28 to 32*32\n",
    "resize_transform = transforms.Compose([transforms.Resize((32, 32)),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# dataset -> MNIST\n",
    "# Note: transforms.ToTesnsor() scale the image from range 0-1\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data',\n",
    "                              train=True,\n",
    "                              transform=resize_transform,\n",
    "                              download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data',\n",
    "                             train=False,\n",
    "                             transform=resize_transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False)\n",
    "\n",
    "# check the dataset\n",
    "for images,labels in train_loader:\n",
    "    print(\"Image batch dimension\", images.shape)\n",
    "    print(\"Image Labels dimension\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the architecture of the model such as\n",
    "- The number of input layers; which is determined by the features of the data\n",
    "- Number of total hidden layers in the model consist of only Convolution layers\n",
    "- The output layer node units is determined by the intended outcome to achieve\n",
    "- Here we have two layers convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Architecture: LeNet-5 == 5 Layers architecture and Conv layer + MaxPooling == 1 layer\n",
    "X -> Conv2d -> ReLU -> MaxPool -> Conv2d -> ReLU -> MaxPool -> Linear -> ReLU -> Linear -> ReLu -> Linear -> Softmax -> y\n",
    "\"\"\"\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, NUM_CLASSES, grayscale=False):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.grayscale = grayscale\n",
    "        \n",
    "        if self.grayscale:\n",
    "            in_channels = 1\n",
    "        else:\n",
    "            in_channels = 3\n",
    "            \n",
    "        self.features = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_channels, 6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, NUM_CLASSES),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        outputs = self.classifier(x)\n",
    "        probas = F.softmax(outputs, dim=1)\n",
    "        return outputs, probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and optimizer\n",
    "- Instantiate the model\n",
    "- define the specific Loss function to be used either cross entropy, MSELoss, etc\n",
    "- define the optimization algorithm to be used either SGD, Adam, RMSprop, Momentum etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = LeNet5(NUM_CLASSES, GRAYSCALE).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute accuracy\n",
    "- A function to compute train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    for features, labels in data_loader:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += labels.size(0)\n",
    "        correct_predictions += (predicted_labels == labels).sum()\n",
    "    return correct_predictions.float() / num_examples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a model requires the following steps\n",
    "- Reset all the gradients to zero (0)\n",
    "- Make a forward pass (make a prediction)\n",
    "- Calculate the loss\n",
    "- Perform back propagation\n",
    "- Update all the parameters (weight and biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch: 000/469 | Cost:2.3012\n",
      "Epoch: 001/010 | Batch: 050/469 | Cost:0.5588\n",
      "Epoch: 001/010 | Batch: 100/469 | Cost:0.4285\n",
      "Epoch: 001/010 | Batch: 150/469 | Cost:0.3741\n",
      "Epoch: 001/010 | Batch: 200/469 | Cost:0.2576\n",
      "Epoch: 001/010 | Batch: 250/469 | Cost:0.1507\n",
      "Epoch: 001/010 | Batch: 300/469 | Cost:0.1310\n",
      "Epoch: 001/010 | Batch: 350/469 | Cost:0.2786\n",
      "Epoch: 001/010 | Batch: 400/469 | Cost:0.0713\n",
      "Epoch: 001/010 | Batch: 450/469 | Cost:0.1760\n",
      "Epoch: 001/010 training accuracy: 96.35%\n",
      "Time Elapsed: 0.26 min\n",
      "Epoch: 002/010 | Batch: 000/469 | Cost:0.1220\n",
      "Epoch: 002/010 | Batch: 050/469 | Cost:0.0898\n",
      "Epoch: 002/010 | Batch: 100/469 | Cost:0.0559\n",
      "Epoch: 002/010 | Batch: 150/469 | Cost:0.0237\n",
      "Epoch: 002/010 | Batch: 200/469 | Cost:0.1449\n",
      "Epoch: 002/010 | Batch: 250/469 | Cost:0.0807\n",
      "Epoch: 002/010 | Batch: 300/469 | Cost:0.1181\n",
      "Epoch: 002/010 | Batch: 350/469 | Cost:0.1166\n",
      "Epoch: 002/010 | Batch: 400/469 | Cost:0.0537\n",
      "Epoch: 002/010 | Batch: 450/469 | Cost:0.1589\n",
      "Epoch: 002/010 training accuracy: 97.88%\n",
      "Time Elapsed: 0.51 min\n",
      "Epoch: 003/010 | Batch: 000/469 | Cost:0.0888\n",
      "Epoch: 003/010 | Batch: 050/469 | Cost:0.0477\n",
      "Epoch: 003/010 | Batch: 100/469 | Cost:0.0363\n",
      "Epoch: 003/010 | Batch: 150/469 | Cost:0.0239\n",
      "Epoch: 003/010 | Batch: 200/469 | Cost:0.0764\n",
      "Epoch: 003/010 | Batch: 250/469 | Cost:0.0497\n",
      "Epoch: 003/010 | Batch: 300/469 | Cost:0.0474\n",
      "Epoch: 003/010 | Batch: 350/469 | Cost:0.0977\n",
      "Epoch: 003/010 | Batch: 400/469 | Cost:0.0885\n",
      "Epoch: 003/010 | Batch: 450/469 | Cost:0.0831\n",
      "Epoch: 003/010 training accuracy: 98.46%\n",
      "Time Elapsed: 0.77 min\n",
      "Epoch: 004/010 | Batch: 000/469 | Cost:0.0228\n",
      "Epoch: 004/010 | Batch: 050/469 | Cost:0.0466\n",
      "Epoch: 004/010 | Batch: 100/469 | Cost:0.0106\n",
      "Epoch: 004/010 | Batch: 150/469 | Cost:0.0615\n",
      "Epoch: 004/010 | Batch: 200/469 | Cost:0.0282\n",
      "Epoch: 004/010 | Batch: 250/469 | Cost:0.0379\n",
      "Epoch: 004/010 | Batch: 300/469 | Cost:0.0835\n",
      "Epoch: 004/010 | Batch: 350/469 | Cost:0.0669\n",
      "Epoch: 004/010 | Batch: 400/469 | Cost:0.0277\n",
      "Epoch: 004/010 | Batch: 450/469 | Cost:0.0476\n",
      "Epoch: 004/010 training accuracy: 98.44%\n",
      "Time Elapsed: 1.02 min\n",
      "Epoch: 005/010 | Batch: 000/469 | Cost:0.0477\n",
      "Epoch: 005/010 | Batch: 050/469 | Cost:0.0800\n",
      "Epoch: 005/010 | Batch: 100/469 | Cost:0.0219\n",
      "Epoch: 005/010 | Batch: 150/469 | Cost:0.0552\n",
      "Epoch: 005/010 | Batch: 200/469 | Cost:0.0709\n",
      "Epoch: 005/010 | Batch: 250/469 | Cost:0.0423\n",
      "Epoch: 005/010 | Batch: 300/469 | Cost:0.0596\n",
      "Epoch: 005/010 | Batch: 350/469 | Cost:0.0711\n",
      "Epoch: 005/010 | Batch: 400/469 | Cost:0.0220\n",
      "Epoch: 005/010 | Batch: 450/469 | Cost:0.0178\n",
      "Epoch: 005/010 training accuracy: 98.87%\n",
      "Time Elapsed: 1.27 min\n",
      "Epoch: 006/010 | Batch: 000/469 | Cost:0.0382\n",
      "Epoch: 006/010 | Batch: 050/469 | Cost:0.0169\n",
      "Epoch: 006/010 | Batch: 100/469 | Cost:0.0094\n",
      "Epoch: 006/010 | Batch: 150/469 | Cost:0.0717\n",
      "Epoch: 006/010 | Batch: 200/469 | Cost:0.0519\n",
      "Epoch: 006/010 | Batch: 250/469 | Cost:0.0043\n",
      "Epoch: 006/010 | Batch: 300/469 | Cost:0.0134\n",
      "Epoch: 006/010 | Batch: 350/469 | Cost:0.0144\n",
      "Epoch: 006/010 | Batch: 400/469 | Cost:0.0187\n",
      "Epoch: 006/010 | Batch: 450/469 | Cost:0.0588\n",
      "Epoch: 006/010 training accuracy: 99.15%\n",
      "Time Elapsed: 1.53 min\n",
      "Epoch: 007/010 | Batch: 000/469 | Cost:0.0222\n",
      "Epoch: 007/010 | Batch: 050/469 | Cost:0.0318\n",
      "Epoch: 007/010 | Batch: 100/469 | Cost:0.0410\n",
      "Epoch: 007/010 | Batch: 150/469 | Cost:0.0198\n",
      "Epoch: 007/010 | Batch: 200/469 | Cost:0.0492\n",
      "Epoch: 007/010 | Batch: 250/469 | Cost:0.0434\n",
      "Epoch: 007/010 | Batch: 300/469 | Cost:0.0190\n",
      "Epoch: 007/010 | Batch: 350/469 | Cost:0.0142\n",
      "Epoch: 007/010 | Batch: 400/469 | Cost:0.0420\n",
      "Epoch: 007/010 | Batch: 450/469 | Cost:0.0150\n",
      "Epoch: 007/010 training accuracy: 98.96%\n",
      "Time Elapsed: 1.79 min\n",
      "Epoch: 008/010 | Batch: 000/469 | Cost:0.0345\n",
      "Epoch: 008/010 | Batch: 050/469 | Cost:0.0060\n",
      "Epoch: 008/010 | Batch: 100/469 | Cost:0.0154\n",
      "Epoch: 008/010 | Batch: 150/469 | Cost:0.0247\n",
      "Epoch: 008/010 | Batch: 200/469 | Cost:0.0274\n",
      "Epoch: 008/010 | Batch: 250/469 | Cost:0.0621\n",
      "Epoch: 008/010 | Batch: 300/469 | Cost:0.0121\n",
      "Epoch: 008/010 | Batch: 350/469 | Cost:0.0052\n",
      "Epoch: 008/010 | Batch: 400/469 | Cost:0.0236\n",
      "Epoch: 008/010 | Batch: 450/469 | Cost:0.0078\n",
      "Epoch: 008/010 training accuracy: 99.11%\n",
      "Time Elapsed: 2.05 min\n",
      "Epoch: 009/010 | Batch: 000/469 | Cost:0.0195\n",
      "Epoch: 009/010 | Batch: 050/469 | Cost:0.0218\n",
      "Epoch: 009/010 | Batch: 100/469 | Cost:0.0565\n",
      "Epoch: 009/010 | Batch: 150/469 | Cost:0.0090\n",
      "Epoch: 009/010 | Batch: 200/469 | Cost:0.0394\n",
      "Epoch: 009/010 | Batch: 250/469 | Cost:0.0215\n",
      "Epoch: 009/010 | Batch: 300/469 | Cost:0.0068\n",
      "Epoch: 009/010 | Batch: 350/469 | Cost:0.0494\n",
      "Epoch: 009/010 | Batch: 400/469 | Cost:0.0316\n",
      "Epoch: 009/010 | Batch: 450/469 | Cost:0.0172\n",
      "Epoch: 009/010 training accuracy: 99.40%\n",
      "Time Elapsed: 2.30 min\n",
      "Epoch: 010/010 | Batch: 000/469 | Cost:0.0313\n",
      "Epoch: 010/010 | Batch: 050/469 | Cost:0.0401\n",
      "Epoch: 010/010 | Batch: 100/469 | Cost:0.0241\n",
      "Epoch: 010/010 | Batch: 150/469 | Cost:0.0344\n",
      "Epoch: 010/010 | Batch: 200/469 | Cost:0.0071\n",
      "Epoch: 010/010 | Batch: 250/469 | Cost:0.0033\n",
      "Epoch: 010/010 | Batch: 300/469 | Cost:0.0233\n",
      "Epoch: 010/010 | Batch: 350/469 | Cost:0.0578\n",
      "Epoch: 010/010 | Batch: 400/469 | Cost:0.0469\n",
      "Epoch: 010/010 | Batch: 450/469 | Cost:0.0214\n",
      "Epoch: 010/010 training accuracy: 99.19%\n",
      "Time Elapsed: 2.55 min\n",
      "Total Training Time: 2.55 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward and back pass\n",
    "        output, probas = model(features)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        if not i % 50:\n",
    "            print('Epoch: %03d/%03d | Batch: %03d/%03d | Cost:%.4f'\n",
    "                 %(epoch+1, NUM_EPOCHS, i, total_step, loss))\n",
    "    model.eval()\n",
    "    print('Epoch: %03d/%03d training accuracy: %.2f%%' %(\n",
    "    epoch+1, NUM_EPOCHS, compute_accuracy(model, train_loader)))\n",
    "    print('Time Elapsed: %.2f min' %((time.time() - start_time) / 60))\n",
    "print('Total Training Time: %.2f min' %((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.81%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False): # save memory during Inference\n",
    "    print('Test Accuracy: %.2f%%' %(compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAQ5ElEQVR4nO3dW4xVdZbH8e8CubYgMBQ0KlCCPChmBFJcxEuYcewgaQMYMe1DhwdpOkTCYHpi1ElGxyc1ox1jRmOppGmhbVEx4gQcCfFGQhhK5To4SgNCKUIxXnAkisCah7OJJX3WrkOdW8H/90lInfNfZ7MXO/xqn7P32f9t7o6InPu61bsBEakNhV0kEQq7SCIUdpFEKOwiiVDYRRJxXjkLm9l04DGgO/CMuz+Y9/rBgwd7Y2NjOasUkRx79+7l8OHDVqzW6bCbWXfg34EbgFZgk5mtcvf/jpZpbGykpaWls6sUkQ40NTWFtXLexk8Cdrn7bnc/BvwZmFnG3yciVVRO2C8C9rd73pqNiUgXVE7Yi30u+Kvv3prZfDNrMbOWtra2MlYnIuUoJ+ytwPB2zy8GPjv9Re7e7O5N7t7U0NBQxupEpBzlhH0TMMbMLjGznsCvgFWVaUtEKq3TR+Pd/biZLQT+k8KptyXuvqNinYlIRZV1nt3dVwOrK9SLiFSRvkEnkgiFXSQRCrtIIhR2kUQo7CKJUNhFEqGwiyRCYRdJhMIukgiFXSQRCrtIIhR2kUQo7CKJUNhFEqGwiyRCYRdJhMIukgiFXSQRCrtIIhR2kUQo7CKJUNhFEqGwiyRCYRdJhMIukoiy7ghjZnuBb4ATwHF3j+8ELyJ1VVbYM3/n7ocr8PeISBXpbbxIIsoNuwNvmNl7Zja/Eg2JSHWU+zb+anf/zMyGAGvN7EN3f6f9C7JfAvMBRowYUebqRKSzytqzu/tn2c9DwCvApCKvaXb3JndvamhoKGd1IlKGTofdzH5mZv1OPQZ+AWyvVGMiUlnlvI0fCrxiZqf+nj+5++sV6UpyuXtY++GHH4qOf/fdd+EyJ06cCGsXXHBBWOvWTcd3zyadDru77waurGAvIlJF+tUskgiFXSQRCrtIIhR2kUQo7CKJqMSFMFIFJ0+eDGvHjh0La3v27Ck6/tRTT4XLHDx4MKwtWbIkrPXu3TusZadkpQvRnl0kEQq7SCIUdpFEKOwiiVDYRRKho/Fd1IEDB8LasmXLwlp09Hz//v3hMnkXu3z//fdhrVevXmFNR+O7Hu3ZRRKhsIskQmEXSYTCLpIIhV0kEQq7SCJ06q2O8uaFW7VqVVhrbm4Oa9Eptrx55o4cORLWHn/88bC2YMGCsDZw4MCi4927dw+XkerSnl0kEQq7SCIUdpFEKOwiiVDYRRKhsIskosNTb2a2BPglcMjdr8jGBgEvAI3AXuBWd/+yem2emz788MOw9vbbb4e1vCvYjh8/XnQ87yq0vDnt8uag+/jjj8PazTffXHT82muvDZcZNGhQWNNVdOUrZc/+B2D6aWN3A+vcfQywLnsuIl1Yh2HP7rf+xWnDM4Gl2eOlwKwK9yUiFdbZz+xD3f0AQPZzSOVaEpFqqPoBOjObb2YtZtbS1tZW7dWJSKCzYT9oZsMAsp+Hohe6e7O7N7l7U0NDQydXJyLl6mzYVwFzs8dzgVcr046IVEspp96eB6YBg82sFbgPeBBYYWa3A/uAOdVs8myWN2HjmjVrwtqGDRvCWt++fcPa4MGDi47nXW22a9eusLZv376w9uKLL4a1aMLMqD+AyZMnh7UePXqENSlNh2F399uC0vUV7kVEqkjfoBNJhMIukgiFXSQRCrtIIhR2kURowskq++qrr8Lali1bwlreZJQzZ84Ma9Onn37NUsHRo0fDZV5//fWw9u6774a1L744/ZKJH0WnDl966aVwmaFDh4a1UaNGhTVNYlka7dlFEqGwiyRCYRdJhMIukgiFXSQRCrtIInTqrcq+/DKehzPv1NWVV14Z1ubNmxfWrrnmmqLjeVffTZgwIazdc889YW39+vVh7dtvvy06vnr16nCZ8ePHh7WRI0eGNZ16K4327CKJUNhFEqGwiyRCYRdJhMIukggdja+yvAth8i52yTsyPWLEiLAW3Sapd+/e4TJjx44Na0OGxLcEOO+8+L9P1Ef//v3DZQYMGBDWunXTfqlc2oIiiVDYRRKhsIskQmEXSYTCLpIIhV0kEaXc/mkJ8EvgkLtfkY3dD/wGOHVb1nvdPb7CIWF5c7jt378/rOWdKvvkk0/C2vDhw4uOu3u4zLFjx8Lapk2bwlrexTWRvAtahg0bFtZ0sUv5Stmz/wEoNovh7919XPZHQRfp4joMu7u/A8TXYorIWaGcz+wLzWyrmS0xs4EV60hEqqKzYX8SGA2MAw4Aj0QvNLP5ZtZiZi1tbW3Ry0SkyjoVdnc/6O4n3P0k8DQwKee1ze7e5O5NDQ0Nne1TRMrUqbCbWfvDprOB7ZVpR0SqpZRTb88D04DBZtYK3AdMM7NxgAN7gd9Wscez2g033BDW1qxZE9Z27doV1jZv3hzWLrnkkjP++1577bWw1traGtbyTg+ePHmy6Pibb74ZLpM3795ll10W1vr16xfW5Ecdht3dbysy/GwVehGRKtI36EQSobCLJEJhF0mEwi6SCIVdJBGacLLKLr300rAWnSYDaGlpCWsPP/xwWHvmmWeKjh89ejRcJrpVE8BVV10V1u66666wtnz58qLjeaf5NmzYENamTZsW1q677rqwJj/Snl0kEQq7SCIUdpFEKOwiiVDYRRKhsIskQqfeqqxPnz5hbd68eWHt66+/Dmt5p68+/fTTouMDB8aTCc2ZMyesLVy4MKyNHj06rB0+fLjo+LZt28JlduzYEdbeeuutsDZx4sSwlrf9U6M9u0giFHaRRCjsIolQ2EUSobCLJEJH46ss77ZFeXOu3XHHHWFt6NChYW337t1Fx/Muulm0aFFYy7uQJ+/fFl248tFHH4XLLFu2LKzlXSTzwQcfhLWpU6eGtdRozy6SCIVdJBEKu0giFHaRRCjsIolQ2EUSUcrtn4YDfwR+DpwEmt39MTMbBLwANFK4BdSt7v5l9Vo99/Tt2zesTZ48OawNHz48rB06dKjo+KBBg8JlxowZE9a6dYv3B2YW1oYMGVJ0fMaMGeEy27fHtwxcv359WHv11VfD2qRJxe85et556Z11LmXPfhz4nbtfBkwB7jCzy4G7gXXuPgZYlz0XkS6qw7C7+wF3fz97/A2wE7gImAkszV62FJhVrSZFpHxn9JndzBqB8cBGYKi7H4DCLwSg+Ps2EekSSg67mZ0PvAwsdvcjZ7DcfDNrMbOWtra2zvQoIhVQUtjNrAeFoC9395XZ8EEzG5bVhwFFjwy5e7O7N7l7U0NDQyV6FpFO6DDsVjjk+iyw090fbVdaBczNHs8F4kOiIlJ3pZx/uBr4NbDNzDZnY/cCDwIrzOx2YB8QT2QmZyzvtFzelWh5tVqKTtmNGjUqXCbvCrWNGzeGtbyr3j7//POi4xdeeGG4TN7pxrNZh2F39/VAdEL1+sq2IyLVcm7+ChORv6KwiyRCYRdJhMIukgiFXSQR6V36I3WVdxuqG2+8Maxt2bIlrL3xxhthLZrEcvHixeEyvXr1Cmt5V/p1ddqziyRCYRdJhMIukgiFXSQRCrtIIhR2kUTo1JvUVN6pq7yJNGfPnh3W8iacfOihh4qOz5kTX6Q5YsSIsNajR4+w1tVpzy6SCIVdJBEKu0giFHaRRCjsIonQ0XjpMvr06RPWpkyZEtYWLVoU1qKj8Y888ki4zAMPPBDW8m6j1dXnruva3YlIxSjsIolQ2EUSobCLJEJhF0mEwi6SiA5PvZnZcOCPwM+Bk0Czuz9mZvcDvwFO3Zr1XndfXa1G5dyXd5HMgAEDwtqsWbPC2nPPPVd0fMWKFeEy8+bNC2v9+/cPaz179gxrXUEp59mPA79z9/fNrB/wnpmtzWq/d/d/q157IlIppdzr7QBwIHv8jZntBC6qdmMiUlln9JndzBqB8cCpW2ouNLOtZrbEzOI5gkWk7koOu5mdD7wMLHb3I8CTwGhgHIU9f9HvH5rZfDNrMbOWtra2Yi8RkRooKexm1oNC0Je7+0oAdz/o7ifc/STwNDCp2LLu3uzuTe7e1NDQUKm+ReQMdRh2KxwifRbY6e6Pthsf1u5ls4HtlW9PRCqllKPxVwO/BraZ2eZs7F7gNjMbBziwF/htVToUIX/utzFjxoS1J554ouj4LbfcEi6zcuXKsJY3T15Xf+daytH49UCxE6A6py5yFtE36EQSobCLJEJhF0mEwi6SCIVdJBGacFLOCnlXxOVNVDl16tSi4xMnTgyXWbt2bVi76aabwlrelXld4bZR2rOLJEJhF0mEwi6SCIVdJBEKu0giFHaRROjUm5z18u6xFp2Wu/POO8Nl8u71tmfPnrA2duzYsKZTbyJSMwq7SCIUdpFEKOwiiVDYRRKhsIskQqfe5JzWvXv3ouPTp08Pl2ltbQ1rI0eODGt5V+Z1BdqziyRCYRdJhMIukgiFXSQRCrtIIjo8Gm9mvYF3gF7Z619y9/vMbBDwAtBI4fZPt7r7l9VrVeTMRUfI8y5MWbBgQbXaqatS9uzfA3/v7ldSuD3zdDObAtwNrHP3McC67LmIdFEdht0L/i972iP748BMYGk2vhSYVZUORaQiSr0/e/fsDq6HgLXuvhEY6u4HALKfQ6rXpoiUq6Swu/sJdx8HXAxMMrMrSl2Bmc03sxYza2lra+tsnyJSpjM6Gu/uXwFvAdOBg2Y2DCD7eShYptndm9y9qavfv1rkXNZh2M2swcwGZI/7AP8AfAisAuZmL5sLvFqtJkWkfKVcCDMMWGpm3Sn8cljh7v9hZhuAFWZ2O7APmFPFPkWkTB2G3d23AuOLjP8vcH01mhKRytM36EQSobCLJEJhF0mEwi6SCIVdJBHm7rVbmVkb8En2dDBwuGYrj6mPn1IfP3W29THS3Yt+e62mYf/Jis1a3L2pLitXH+ojwT70Nl4kEQq7SCLqGfbmOq67PfXxU+rjp86ZPur2mV1Eaktv40USUZewm9l0M/sfM9tlZnWbu87M9prZNjPbbGYtNVzvEjM7ZGbb240NMrO1ZvZx9nNgnfq438w+zbbJZjObUYM+hpvZm2a208x2mNk/ZuM13SY5fdR0m5hZbzP7LzPbkvXxr9l4edvD3Wv6B+gO/AUYBfQEtgCX17qPrJe9wOA6rPc6YAKwvd3Yw8Dd2eO7gYfq1Mf9wD/VeHsMAyZkj/sBHwGX13qb5PRR020CGHB+9rgHsBGYUu72qMeefRKwy913u/sx4M8UJq9Mhru/A3xx2nDNJ/AM+qg5dz/g7u9nj78BdgIXUeNtktNHTXlBxSd5rUfYLwL2t3veSh02aMaBN8zsPTObX6ceTulKE3guNLOt2dv8qn+caM/MGinMn1DXSU1P6wNqvE2qMclrPcJebNb+ep0SuNrdJwA3AneY2XV16qMreRIYTeEeAQeAR2q1YjM7H3gZWOzuR2q13hL6qPk28TImeY3UI+ytwPB2zy8GPqtDH7j7Z9nPQ8ArFD5i1EtJE3hWm7sfzP6jnQSepkbbxMx6UAjYcndfmQ3XfJsU66Ne2yRb9xlP8hqpR9g3AWPM7BIz6wn8isLklTVlZj8zs36nHgO/ALbnL1VVXWICz1P/mTKzqcE2scI9mp4Fdrr7o+1KNd0mUR+13iZVm+S1VkcYTzvaOIPCkc6/AP9cpx5GUTgTsAXYUcs+gOcpvB38gcI7nduBv6FwG62Ps5+D6tTHc8A2YGv2n2tYDfq4hsJHua3A5uzPjFpvk5w+arpNgL8FPsjWtx34l2y8rO2hb9CJJELfoBNJhMIukgiFXSQRCrtIIhR2kUQo7CKJUNhFEqGwiyTi/wGrI2Kz4liL3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (features, labels) in enumerate(test_loader):\n",
    "    \n",
    "    features = features\n",
    "    labels = labels\n",
    "    break\n",
    "    \n",
    "nhwc_img = np.transpose(features[7], axes=(1, 2, 0))\n",
    "nhw_img = np.squeeze(nhwc_img.numpy(), axis=2)\n",
    "plt.imshow(nhw_img, cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
