{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the data\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RL         1151\n",
       "RM          218\n",
       "FV           65\n",
       "RH           16\n",
       "C (all)      10\n",
       "Name: MSZoning, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the value counts of the columns\n",
    "df['MSZoning'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26284127a20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False, cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 0\n",
       "MSSubClass         0\n",
       "MSZoning           0\n",
       "LotFrontage      259\n",
       "LotArea            0\n",
       "                ... \n",
       "MoSold             0\n",
       "YrSold             0\n",
       "SaleType           0\n",
       "SaleCondition      0\n",
       "SalePrice          0\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     1452 non-null   object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values\n",
    "df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with many null values\n",
    "df.drop(['Alley'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing data\n",
    "df['BsmtCond'] = df['BsmtCond'].fillna(df['BsmtCond'].mode()[0])\n",
    "df['BsmtQual'] = df['BsmtQual'].fillna(df['BsmtQual'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing data\n",
    "df['FireplaceQu'] = df['FireplaceQu'].fillna(df['FireplaceQu'].mode()[0])\n",
    "df['GarageType'] = df['GarageType'].fillna(df['GarageType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the garageYt\n",
    "df.drop(['GarageYrBlt'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 79)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing data\n",
    "df['GarageFinish'] = df['GarageFinish'].fillna(df['GarageFinish'].mode()[0])\n",
    "df['GarageQual'] = df['GarageQual'].fillna(df['GarageQual'].mode()[0])\n",
    "df['GarageCond'] = df['GarageCond'].fillna(df['GarageCond'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some cols\n",
    "df.drop(['PoolQC','Fence','MiscFeature'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 76)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the remaining cols\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 75)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass       0\n",
       "MSZoning         0\n",
       "LotFrontage      0\n",
       "LotArea          0\n",
       "Street           0\n",
       "                ..\n",
       "MoSold           0\n",
       "YrSold           0\n",
       "SaleType         0\n",
       "SaleCondition    0\n",
       "SalePrice        0\n",
       "Length: 75, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MasVnrType']=df['MasVnrType'].fillna(df['MasVnrType'].mode()[0])\n",
    "df['MasVnrArea']=df['MasVnrArea'].fillna(df['MasVnrArea'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAE5CAYAAAA3GCPGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA+VUlEQVR4nO2dd5hlVZW339U0GZsgSUWCZAYlCArCR9ZBBxSRIMGAKI4itKKiMCqIoyiiA4qgKCAiUQmCkrNkG+hu8oiAYABHAWlAQuP6/lj7dJ26dXJV9enG3/s896m65+599j5pnb1X2ubuCCGEmL1M6LsDQgjxr4iErxBC9ICErxBC9ICErxBC9ICErxBC9MDEpgU33f4auUUIIURLrrtgcyvarpGvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0wMS+OyDmbg66eJ/aModve3xtncEyQrzcMXdvVHDT7a9pVlAIIcQsrrtgcyvaLrWDEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0wMS+OyDmbg66eJ/aModve3xtncEyQrzcMXdvVHDT7a9pVlAIIcQsrrtgcyvaLrWDEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0wMS+OyDmbg66eJ/aModve3xtncEyQrzcMXdvVHDT7a9pVlAIIcQsrrtgcyvaLrWDEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0wMS+OyDmbg66eJ/aModve3xtncEyQrzcMXdvVHDT7a9pVlAIIcQsrrtgcyvaLrWDEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gbu3+gD7tK3Ttd7sqvNybWtO75/OxdzTP52Lsak3bB8dGp3SsbOt682uOi/Xtub0/ulczD3907kYm3r5j9QOQgjRAxK+QgjRA12E7/Ed2+pSb3bVebm2Naf3b3a2pf7NPW3N6f0bTb1ZWNJfCCGEmI1I7SCEED0g4SuEED0g4SvmWMxs+b77IMR4IeE7TpjZElWfhvuY3GRb35jZwuO06/NybZzdpqKZTTCzt4x5j8SY8q/8gm1kcDOzTYCp7v6Mme0JrA8c7e6/b1B3bWAtYIFsm7v/pKTsBGC6u6/dsP/5uisAq7r75Wa2IDDR3We03c9YYWYPAg4YsDzwRPp/MeBhd1+pwT5uc/f1B7bd7u7r1dT7D+DfGH7OD6upswlwKLACMDH11d39dRV13gL8CFjE3Zc3s3WAj7r7xysPrCH5Y21y3AX1b3T3jRuW3cjdb+rSz9w+3gKsSJw/oPxeT+XnB95TUKfwWpnZHcQ9NeKnqOZvqGhrZeAP7v68mW0BvAH4ibs/2Wf/8ve4mZ3t7u8p609Jm6sBn2Xovs36uFVB2QOq9uXu365ox4A9gNe5+2HppbGsu9/Spr95JtYXAeA4YJ30cB0InAD8BNi8qpKZHQJsQQjfC4G3A9eluiNw93+a2TQzW97dH27YN8zsI8A+wBLAysBywPeBrQvKvh74IfAa4CLgc+7+RPrtFnd/U0kbM6i+sSYNHMtKqd73gfPd/cL0/e3ANjXHsxuwO7CSmZ2f+2kS8Leaut8HFgK2JATjTkCTG+QE4FPArcBLDcoD/A/w78D5AO4+zcw2q+hb23PvJf835VIzew9wjtePMo4lBhWthHaGmZ1C3HtTGTp/Tsm9nvgF8HfinD/foJnt2vRpgLOBDcxsFeJanw+cBryj5/5Z7v/SF30FPyOe9R9Sf9++osP+M44F/glsBRwGzCDO6Yad99gwlO629PdLwN75bTX17iBUG9PS92WAC2rqXJkO7AriBjmfEF5VdaYC8wG359suKXsdsC0xAv0McBewcvrt9qp2unyAWwu2VYYmEm/xLYAbiRdc9lmfGNFX1Z0+8HcR4NIG/by5w7HdPHjesms9FueeeJieSvfDzNz/M4CnGvRvBvHAvJCrW1hv4Bha3wfAPaSZZIs6d471/VbRVvYMfxbYr8lxzo7+5eVIE5lSUH/E8zXO5y9/n5Te600+TUe+M8zsIGBPYDMzmweYt0G9f3iMZmea2STgL9S/3b7csE95nnf3F2JmAGY2kfKR0iLufnH6/0gzuxW42MzeV1FnBGa2NMOn9WUj9b+a2ReAn6b970nN6NVDnfN7M9uGoXO4GrAG8UKr4h/p77Nm9urUVq2KA7jKzL4JnENulOPut1XUeSRNtd3M5gP2J4RQGa3OvbvP06Dfpbh7m5HOBDNbnBgsZP/PGpW5++M19e8ElgX+3KLNG8zs9e5ed02HYWYbAd8F1iQGHfMAz/jA7GuAF9OM6gPA9mlb3TM8O/q3jpk9RZzrBXP/Q8GMMtdGZje5wMw+DpzL8Pu29HqZ2QLA3oxUzX2o4rBeTHLP0z6WIl7snWkqfHclpsF7u/ujSd/xzQb1ppjZYsSU4FbgaWqmwO5+TcM+5bnGzA4mLt5bgY8DF5SUNTNb1N3/ntq7Kk1NzybUFpWY2TuBbwGvJl4mKxAC599KquwGHELcHADXpm1NuBb4f0kQXAFMIa7FHhV1fpnO+TeB24ib5UcN2npz+rtBbpsT06wy/hM4mlAj/AG4FNi3onyrc29mCwEvuvuL6fvqxDT5IXc/d7B8rt4a7n6vma1f9HvJC2VR4h7NHvx8Gadk0GBmF6TfXwHcbWa3MFwIvLOgTqYbnQjsZWYPpDq1utvEMcB7iSn3BsD7gVVq6uxFXK+vuvuDZrYSMSAoOqbZ1r9RvGBvZcimAjGin7Vbqgd5pwD3Eiqzw4jnqWrQAPAd4hlexsy+SqjzvtC+20M0NbgtDDzn7i/lRmAXZQ9Fo4bMVgQmufv0mnJ53ep8xNu58q2eDHV7A28jLsYlwI+84ODMbHfgAR8wrqQXyhfd/SM1/ZtGCKTL3X09M9sS2M3d96mq14XMGGFm+wELuvsRbQxPyWCyQCbsxrhv8wAnu/ueLeq0Ovdmdi3xwv9t0lXeApxK2BB+4+6fL2nneHffx8yuKvjZvcAY0xUzq7R7FA0mLIzDVXUqDdlmNsXdNzCz6ZkgNLMb3L3Su8PCEL28u99XU2629a/rC3Y0ZM9Q1j8zmxe4pO6+MLM1GLIjXenudQK7mob6jlsJI85rgEeIN8CpDeoZMc3+Uvq+PPCmlrqWHYCvNSg3H2HBfT0wX1v9S4v+TEl/pwET0v+3FJS7gJzOevDTsK3bgY2Bm4B/S9sKddm5OgsBXwR+mL6vCmzXoK1FgW8To+spxOh+0Zo6l4zmXAML1/x+R+7/rwDfy13ryvNQsc95S7avkD9ewmB5NGGErD1G4BtNtg38fkqTbQVlrk3n4CfAEamPlfpHQtVwH/Bg+r5u3X1IGBDnT/9vQaiVFhvL/qWyq6b/VwEeJ1QWVwBfb9DWvvk+AYsDH6+pc0uu7bWBJYlBQV1b66dzsB+wfpf7L/9p6udr7v4ssCPwXXd/N+XT7DzHEsIjm2bPAL7XsE0A3P08qqe+mWvV74ipwTHA/cmroKrOamb2QzO71MyuzD4NuvSkmS1CXLhTzexowhg0yJGEAHuQ0MP+MH2eJvSDTZgMHASc6+53mdnrgKLRXJ6TiCliZq3/A/DfDdo6kbg+u6TPU2lfVTwEXG9mXzSzA7JPXUNm9hYzu5s01TOzdczs2IKi+ZnLVsBlAO7+Ai30bRZsZWY/Is5HEWcBC6fy6xJT5ocJIVXUt0HeWrCt8h5k4BlKs4k3NmjrfYQe9RPAM8BrCZewKg4F3gQ8CeDuU6m3BZwNvJTzkFiJ8JAYy/4t7u6/Tf9/ADjd3fcjzt1/NGjrI55zl/PwnqmcvQLHJ1XeF4nB0N3AN6oqmNmXgJMJ9diSwEnJltOdJhKaDiOwVKa1hZAQ8NlnJ+DrwI01de4FVhl4Y99bU2ca8DHihnxj9mlwTAsTN9ZE4mbZH3hl1Sigybax+jA0Mm98zlOZqU22Dfx+SNGnQVs3Ew9kvo8jLOuETvJI4ADgMWChtH2xhsf0ZmL0+jDx0vsA8bAXlZ2e+/9I4Ij0/4T8bwX1PkYYQZ8Bpuc+D1IyOyReqHkPjswT42/A4eN0XxR5ppQeV/o9e34PpKGHRId+5c/79cAOLe/b6eS8TNKzeVdJ2buB/yJ52LTs5z2ECi/7viBwz2iOvanBrcsIDLpZCLfP/T+TGF29q6bOX9z9/tz3BwhjWBUz3f24mjIjcPdncl9PblBlKTN7nbs/AJAMHUu1bTcj02dWFHkh6fayc74yzXw0/2Fmm7r7daneJgx5ThTi7l08U7K6j5jlXTwLfTQ/Qtx7ywNv85h9Qeh8jyzbdzKI7EII3dMJo8oUd6+6XvnObEXc73h4mlQdymmEz/LhQF4HPcNLLO7ufjhwuJkd7u4HVe28sKNDATyD+60yMt2ZdO7zmNmqxKDhhpqmMg+J99PcQ6Jt/6ab2ZHAHwm1w6VpH4vVtZO4FDjLwr/dCaPixSVldyMMgZea2V+Je+NMd2/iofIQ4RnxXPo+PzHb7sy4ppQ0sz0I6/z6hKDaCfiCu/9sjNs5jtDZnUVcgJ0J/db1AO5+TkGdQwkBfS4NXVRSvVYGQTPblsj9+UDatCIRBXZJRRtlXhdGjAaWq6j7VsIKuxZxY24CfNDdry6rk+qtS1yjRVM7j6d60yrqXEXxQ1anJvo5oV8+BtiIEAQbuPt7S8pPdvej67blfvs/4vofBfzS3Z8zsweqhFNSH70KeJQQNKu5+4tm9irCN32DsroD+2nqgpiVX5zQy+frXFtT55W5rwsQ9/sS7v6lijoLEaO+t6VNlwD/7e7PVdRZixBmN7r76WngsKu7f32s+pcGCpOJc39idr9ZuDCu7O6n1LRlwEeJwCUj7vkfuXtlwIWFO9yuhDrkfkLd8cOK8ucRARWXEff8Wwm/9b8AuPv+Ve0V7rOJ8E0j1gMZ6RdX+pAlD4SNiId4a+LEXOE1FkIzW45QuG9CHOR1wGR3L9PVYWZVukn3Av+99HYuKtsqysbMdiCMiAdXlJmf8BCBUJEs5u6PVZR/Cfg9w0djnr6/xt3nK6k3gXjBXUGcewNucve/tjieSQDu/lSDsnn95ALEjTzT3Q+sqbckoQ7YhpjWX0Jc40L/Z2sZZp1mW28jRjpbEbO0bYDXunuRfj57iHclfHV/5u5/TNvXA5auelmmctsTL5RhLojuXmobMbMPE4JnOSJQaCNC0LX2xjCz69x907b1Zhd1/TOzN7r7rQPbtnf3MpfR7H7vlI4gt48tiEjNtdx9/opyH6jaT82sqrRSE33HpYQr1z1EpNWJ1FhyU71KXW1JncsIn8SJ6fNB4LLR6FbG+0MIuLoyiwIfAi4H/lhT9reES1DRb4/U1G2lTwb2TH8PKPp0OBfXjOF53Y3wGnmC4d4iVxGufk32sQDxQjqb0BufVlF2nqb7Lag7DXglSSdKeEscX1PnjtS/qen7GsQ0uK6t9XOfDYjRaZ0t5TJGegVcUlL2rFz/pg9+xql/twGvH7j2tVGXhOth4bNSUWdD4kX5e+AaQm+/ZE2d7UjeTWP1aarzfaW7n5CmetcQQQ1NgiHaxNZnLOXu+ZHsj83sk1UVOo6W5yVOepaL4GrgB17ju2xmO+a+TiBursJjS1OqdxIBKusTjvg7EJ4SVRxFPBxFU9YjaupeZmafAc4kjEBApToly0hWFA1Wec0G1CMTCKPlsjX9I9kMjiZGek6EUX/Kk148xw1ExNiShOdIxgxCENTiMa3+OfBzM3sFYcgtK/uSmT1ruUCQFrzo7n+zyKY2wSOApNKCTvjOP2dmmNn8HoEhqzdoK38uMrvILjV1lvQBr4CkIilicvrbNZdEl/7tRFyjPYBNCT3z26qrAKGuuMsiuCV/vxcFt3yNmN08AZwBbFIlIwZ4L3C0RXa9k3y0Pr40j3DLBNKfLdy6/kRMleo4gHi4Z5rZc1CchGaAv1pkTjs9fd+NmnBcwiXqNEK3BOFbfBLF7j8ZxxH62syN6H1p24dr2mpkEDSzUwnBfimh27wSuN9rdK8A7v699BC/xd1vGPjtuzXVMxVLPtLMKYn4cfcfpH8vd/frB45hk5q28lFGMwkL/941dSCu1feAd6fv7yWu95vzhTyFWZPc5pJKJLtnJxEqrRFYA3e3Cp4D7jCzyxj+MNfp9AZdEP9CsQtinj8kw9J5xEvzCeLZqsTdt6wrU8A/LZewyiKQovDl6skA5Q2yFo5V/9z9ATN7L3EuHiEMrJUG30Qbo+/zwNvd/X879G/PdP/tRriZOSFjTveO2ROb6ny3A35NuAd9l7jxv+zu53dptKat5QlhtTFxc9wA7O8Vhgszm+ru69ZtG/h9mruvU7etKxaRcEY4mp/pYd2vNPoU7KN1dq2S/czn4RtbVaZIrzpi21hgZje7+5sHtt3k7huVlN+HCLL4B+EtU5nu0iKbXile4aVRptvzGp2eRRToP4gZwB6EmulUL9FjF9TfPNW5uOpaJR30pwmDKkRAzBHufr+ZTfRynXZm+M1mrJsB+3i14XdHwv91aeKc1w6e2vbPRqahXJrIpPY80VhdKDNmtgxD2cVucfdKTycz25e4Nk+m74sTUaq1/tzJXrEn8ElCDbsK8J0Gg6KRjKUOo0ZnsjJhba3MlERMBWq3Dfx+eToh86TPnoRxr07HtHLu++uoyapEjHCvJ0ZcjxOj2k3Tb4sWlF+DcHO6j3h5/R+RA7TpOfsyYcRqlS0r1TXC2PQj4LGKchsTD8sjDNf3Hkq9nm5n4BXp/y8QSXlqI38I3+3PE54fKxDG3C8SDuxLFJT/LTU6uTnpQ6hJGl8zIipxA0LlVlUus8x/iIjmXCf9PzVdx7p7fklClbB9k/OZ2lqzxXG07l+6/qWfBm3uQsyOTiYGOg8CO9XUmVqw7faSsjumv9sTnlHTiTwSS+eu3e873Sc1nfwuETVW+GlwYl5FhBbeQkznDiGnVC+pM0IAFm0b+H15whDzf4Sl+by6C0d4YDxM6HqvIdQHW1aU/zjxFt+KGPlPSv/fQOiR6gTVBoSS/2HghoY3c5YS8UVqUiLm6jQOLEjlN0/X5c8MD5Y4gBT2WVE3S1u5KfFyeRfNjCQPVnxGhHkSfpsLtb65w5i1L6FaOjH7lJQtNC5RY2Qi9NZXEy+e9YjoxUfTfbhtSZ13pvvtNiKPwYNEANOjwAeqzjewYsH2FdPzVRmGT9gR3kSMejcDNqspf33L8925f+k8viL3/RXAmxu0OY0kCNP3pRo8i20CM7JAk5+UnS9g67b3prtXqx26uldYJDffjdALn5U+v/CK1RvMbGPgLcRw/n9yP00C3u1jpA4YaHN+YHVilHivu5cGI5jZPcQI/PGB7a8kQlYP8AZBG8mlaTPvlr2tar+DgQXnEoEFTdJJYmYreEsdnw0lKDmciHg8rcoFrCtpKnsSERmX98mu1MOa2c8I177dyWWvcvfJBWVXqNpX2bkxsynAwYTK4HhCp3iTRRKW04vORVJJ7ZzqXAW8wUPnuTQxOnx9SVt3u/taJb/d5+6lxroubm0Wvs/LEoOZ/Hkf4Tc/Bv27nZg1efo+gbh/K9VeZnZH/nyletPKzmEq803ihZAPzHjE3T9dUHZcVG9Qb3A7k3gb/d9Ah5YmRmJlfI+wYO/u7lNSnTrl8nxE4u+JDLe8P0VYQkdgZt+lwiJf9HCa2VbufuWA1wLAymZWemOl/Y0w8HhYuH8/KHjr+saQ7q0SixSWszwy3P2XJUX3IdQbxzEUWFCv0B/i2XRTNvblBv5oZj8gfGi/kV5mE+oaSgLrRMLt68kGffsBYbC8g3Y5VFdx953N7F3ufrKZnUb4FBfxKu+2jNBEd8+isg7L9uHhuVBW55+ejD5m9qAnLw93/4uZVRnpXrSCVV7Si6MuinEyoRe9yd23TC+HOmPVJOBZhnsdODHKH+v+WSZ4YVZkYROHgIvN7BKGDPS7EqvmVPE54nn5GLnAjJKya5hZkWdN0/SapdQd3HeIKd/gyX4rMdX8WEm9VxNv9m8nZfhZ1IQl+pAL249bjMCmNCyXZ3PiQd6+4LeqG+spM1vHByK+LJZWKnJLyvq2CWF8ODN935nwEqjFzL5OPDCnpk2TLUKAi1IpLstQYMFRFtFnC1YZYQY4NfVxO2Ik8AFCjVPFLsTKFEe6+5MW0WCfrakD4d2wF5HveQoxqr00//ANMNPdu3gwZF46T1qsJfgoMeIpousyQvmXwaB1vux48onb/2nDE7dXvbwOAS63cJnKPE02JPTnn6vpZ2u3Nnffq2afY9m/B8xsf2LwAKHmG3Q9LOrjZ9NAalPiHB7vNako3f2fxKj3+xbukst5eUTcgxTLitFToxu5u+K3Qh1JQbnliCVjbiWsg3V6qdWI6dulhJC8ksid2VTvtDgNjB3ASk225X7blFDsH5ouxnbEyOEhktGtpN5V5NIYEi+hqxoey3Ryjt2EbqqJk3vjwIJcnVuzNnPbagMmCKPKJ9JnnabXKdWdQOg//0gY/L5MscHtq8RI5VUko1xRuYJ6H073w2YM5fv4aEnZ24v+b9BG0VJH2fcXS+o8mPrTSOddcL5/kp6n24jE4LXnnVBDLZbu32uJ9dkuLCl7YPpbaPNp2b+fNOzf0oTv7V+ye5acLreg/KrpGO4kRr2vaXHNriZG9UsQarpbgW/X3Rdj/anrZGnWnqrfcmXmH/i+OjVZm2iRbYxYU26NrC1CUD+eLuA2Ne0UGfYq14MiRpeHEULtHML9qdJ7gVAFLJH7vjhwX8ObZPpA3SWoNv5MAHYZ2DaJCiNOrtxN6e8lRCq/9YDf1dSZnG7+w9LnDlL2qwbtvYHQ7d+XHuo3E14XUwvKdhFSI85Fg/tucSJKLfu/saBv82HIQ2aBsdxvi/Y3J156hXmKSfmfidnPiE+LdhYZx2P4NZF4aXVicHdOi7q3p78fJlxmKXuugGPG7RhqOnkNBcnPialEbRhriYCr81xovCAesQBjZjTchxhlzkOsHTUiwXkqtwbhEvM7hqev/CANR/Mtb5K9iBHzj9PnwaY3MKFCyOqenOq+t6ZOp3SVxEh+USK59FXEaOCdNXWmk0uITgTUVL0cLs2uMZF/YndGvqAbP0QNjqnxuSBmMK1Ho3kBXfSpusfrnoWKNhvPDjv278e5/xvdqwP1NybSNz6cvq8DHNvgmK4guaISL+cvVJSfOvC98bkkBgmvSudvw+xerqmzDJHT+KL0fS3SYsJdP3U6388S6dp+zJCeMluTqTADFYCZLUuserFgslRn+qxJhF9cFW0WxHvB05kg1mM6w0N3c0+Fsn51QtAsxnBdzgwqkjAXOIPP+okKxbu7n2RmFxEjOwc+7+6PlrUzUPd0M7uaeNkZsdR6Xd224cXZ75kh7+9EXoImGMNTQb7E8GRAgyyZ/u7sI0OJs36MCP81s52J4IMZFgms1we+4u631/Sv8blw9xVr9lXG4Fpiw3ZLcWThixbJoJYzs+8U9KUumi5bLv1H1C+X3qV/+Xt5Ms1Sp+Y5ingezwdw92lmtllljVho4LOEcRV3n54MpGULASwwIFuGyRqvXvj1MGKGd527/8Yi3P23FeUhBkAnEbEKAP9L3Fcn1NQrpTbCLXk27EuMiCCmmd/ziiiS5KL2QUJQ541iM4i3aqlHgbXINmZmNxFTh8eI6esb3f3B9Nu97r7GYJ1c3Y3d/cay3wvKr1D1u1cYCQc8Fq7xikxNqXyXBSCzuo3PXyrf2mMkV/cAYip6LnHTv4u4vkeVlH+AmCKWtVXmwpSttbUpkTf3SOBgH4iSK6jX5l6qdCeqeZhbkaKktiGix0akWfT6aLpb3b3JihedyLtXdXG1shTBmHc7tJroUTP7jbtvOFBnqpdEqVrx+nwZ7mO4Tl+X/jWh1pUjCdlDLJYGX5Ow7j5ZU+dk4GQze4+7n92mQ97QLzXxSSJpylLA/+QE7zuI1TdGYGYHuvsRwO4WiaIH2y8UNlXCtYoCj4X9LXI2VCXRPoBQo3yr4DenYlmllucPRuGV4e7fTiPzLFXgXjWj0UWJWUfZKKzspZyN7v4DOM7df2GRj7mONX0gX63FsuFFZOd6AWLQkIWHv4HwLy5Mh9hFaHuk+DzDzO7xinzJFbReLj31NfMKcODXHkt0FZGNyI2C0XmDkfkjFvl4PcmN/alfHfivFon/PfV1JyLwpxDvkD8ie/bLBhw1x/WMhU9/1r+NKPZyat6fupFvaugdxHTgd8QFWYmwGl9UU28x4s0+a9QHHOYVGaOsY7axpljKEWotY/hteBL1YT9REe+efATX9XBvwSLX7O1laopcvQnAxj6Q7KYJ6cZfkdzL1d1/UlPnKiKZSbaK7LyEjrbyJk/C5/8RL+Xra0blnRzWzeyXhEfENoQB9h+ETr90JFXWXl0fzOwMYnn1O9L3tYHPuPsHS8q3HoGNZraR6rea3aQ6xxJ5CPL+sL9z930LyhY+G7mG6kbm+XzNmR/tZK/Ic5Gm/scTgVZPELr2PZoMepre712f/VR3fcL7Y21i9r8UEcZc5APciKbC917CAnp/+r4y8KuqaX0qd3bqaHZQ7yPcTkbo9XJ1fkS4Y+XrvOTuI7KNWU32Knf/dtXvs4MkfLfIRiXJr/DqOuGbyrZOrGNmpxB5NKYyNGL0Bg/0fYSwz/q5OOEBURWV9CVihHw28ZDtQCQiL9TTWcfoN4tVGLYlouh+a+FP/HpPwQ0F5TObw08Jo17e5vD9GnXUiKnkaKeXBW2MSrh1bPMuYG1PD3x6ud/hFcnec3UX9uHLZ40bFgmKJnjDTGFd7/eOfZvIUETsfaMdEDZNKdlljTSIxDX5VUu/bGZTa+psODCiudIiHLOILBJudWJqf376vj0lOXPN7AKqRx0j8oCW7KfpcjGHA7enEZIRI/qm63Z1yYe8AZGVv2n5jK/n+gnhjnRoTZ3dgPWyqX1SsdxGuZHkfS37BIDH2m3nmNlCZrYBkcikUPAm/p2wOSxH5NPImEGEAldxTxoA/JS4T/akfsqMmb2/pO8jRmCjFa7pZXQAkUR8H4s12Vb38uhHCJvI8oT3DESGwspRm0XI/wlE5OnyFgFFH3X3j9fUG2FEJKboU9z9FwXlVyfUbNlL8R6LtQqbpH5sfL+b2flVvxc9+zYyEjZjNauJiK2jUvjmGr7LzC5k+Bppv2mw/9aLMhJLVa/s7r9LdV5HiUXXU2pAM7uUiAufkb4fSliEi8gWXtyR8Nv9afq+G+FuVImF8exbDCwXw8Ay4Lk+dvFYyDiAcN96ycz+QY2KI3EncVxNFgXM9zPvlQHNvDIeosWigu5+J8y6r2pTFaZz/R3Cd/sLRNj6Y8CKZva5MiE2GpsD4Rr4MYYSil/LUNRVFRvm/l+ASNyUBRkUYrE81+cIXXvTkG4Iq/utxBQdIrfIz4Aq4ftKQqjdkuvvjZlAKhl0HEV7rwWIY1mDoWfwPYRb6N5mtqW7fzIrmAT8OYRa83jiXlgPuNrMdvT6kO829/vGRDDP6YQev8ozJ6Mquq3KTlFLXWKdk6oa9oK10Qbqr0PcfIumTU8QfoOlb1wz25q4uR4gTs4KhCGnVLeW1CLreEqMY5FjYFrN9PJad9+sbltBvWmEwetyj6QyWxK5QEtXFDazNzBSJ9X5otX07ypgXSKTXN4YUzuiN7PXEOc738/CGUQqfx4dFhU0s/uB7b1+Pb9OCWhy9ecnHvwVB47psKp6Y4GZLQqcUnXe06DhTMIDZFZIt7tXhuKa2RR338DaeRNsXrVPL0j0ZB28FlKZKwn7wcz0fSKh930roepYK1f2ImJJsqsL+vt5d397TVuN7/dkb3krMdB6A/ArIvnRXVVtjBeVI19vH9s9WH8asI7lFmW0WBKoVPi6+xXZNArqs40lTgFuMbNzCSHwbipGHImuS7q3Wi7GzE4kLvRdDOUBaPTGNDMjMnGt5O5fMbPXEglgbqmodmiDYyhq6xuEEWawn6XCl7C2n5v7fnXD5h6rE7yJrgloMn5BTHdvpT6xC6mdTYhzOPgSKjVmlfAsEQJbRdfluV6wWKIq09+uTM3xufs1Fu6Sq7r75an+xBrdahevBQh9+8IMeQMsDLzaY5mmwX6uPCh4c/09vkFbhzYok+3zJSJXzcXpxbwbMcI+zBskQ7dYxWcw8VTnF3kjnW8aARe5ZlSOfHPlnsp9PYCYzgy2sScxEj8lCdvpaftHzOwZdz+tYv9fNbOLae7yBJFn+GoL31NIS7o3OJwnrd1yMRt5SZq9BhxLCMKtiFDmp4mp94aDBc3sGCKHQ6NsaQXsQOgNGwmpxEU+4O9tZqu7+3019aaY2ZnUpyrsmoAmYzl337ZBuTwnEPfGrdQHMMxiwJYwgVAlnFVTrevyXIcQQuS1FstVbULouKv69xFCr7oEYaBajgjU2Lqi2n8SXguvIVQblzJ8eaoyjgCmJnVbZuf4moUx7fKBslXCv4mRbwrwD48saKsR6o5SL6wkdP+DELwrEmqtJgOh7xMBYlsSwS07EaPtzjT1dsgbzRYgRpZ/6mJRNLNH3P21BdtvJ/LczhjYPolIRFPpVJ6mFMswfLRSZgTL6gxb0r2J4LGWy8WY2QnAt9z97rp9F9S9zd3XbzLtM7PJRNThq4ip7OnuPrVFWxcRkWdPt6hzH/BFdz8rff80EXJZ+bIpUWeNUGOZ2UMMLRtUVL5yNJpGTt/15DbWBCtY4qhhvfy0fiZhFKxcnNFGsTyXhc/pRsS5ucnDd7iq/FQiX8rNuXtpWC7cscTCI+VNqX+3uHvh2nRp8HJG0U9Ebo5latq5lXB1XJxISD8FeNbd9ygoezLhKnYREQ17Z4vjyQJ9sr+LEIbwJot8Fu+zifAt6MgEQufZOorEzB529+ULtk/3Everqt/S7/sRo4HHGApx9ao6qV4rf9gk4C9x922q9jtQZzNi+fNHiVFeo76lujcTRpXfJCG8FOF7W+qulaaW702fBQjjwhleYzm2cAtch4ivb5SwPD1gxxMGt2WIKemn2wjw8cTM7iZ8Wx+k4bm38NiYhxgN5c9Dof+yRQrUD6b/P+Dj4CZW0m5b/fww/W3Sw95Wcy5aeS0M1F2cULvkp+gj+mej9ynOBij7AQt6BFEUugaa2T8ZGk3nBV+tITt3/m4ijPWPE/rrOtVSKU1dzQZZlXBbKcSqAxIWLKk2rxX4E1os9z1fTX8mE1PmRosVpv0W+gdSoSv2bkuLn0i4WLVNBA4xJToXWNpipYqdiLXOSvFwSv8Gkdx8vdT+IYRAqeJ8hlz1GuHuf07qnoOIYzuoSvBaywgjG33Ib6WxpoRs1LtBvinKowrzs5BGeRDKjn9WY/U+2V3089eY2cFEDoS3EvlyK8PcaeG1MNC/wlUzKDiHY/CyMguPiT0YWjm78F539yaqqjJ+aRE0dgRDkZ9lCdgb0VTnmwlTS38fpSI5sru/ouy3Ck4Afm5mH3P3h1K7KxI6zrrkFY/QPtSvqz9s26XFH24yjSzC3U9N06qtiXO/Q52hyiIybVti5Ls1EVX45QZttX4I0jn4MzGVWw440cJjpCx/Q6Z6aZoEvyi8OqMyzBriRWSRD2JVD1e6pQif1UIsVnf4b2Jq/nRue5UQbz91HH78XyZejm3Ygfb6+c8ReVDuIGwbF1IvPFYBtvIhr4XjyHktVNSbTMNVM2z0fveTiZf/ue5+l4VralXUYSvMbENiiaGvpO+LEMd+L8OXO2u/7y5qh/HCzP6TOJHZA/I08HWvWRst6VVXJ1xH8lPF0gg3i/W99nf3Vv6wZdOkMuFlEda5GDHKqF0Ha6DuKe7+vrptaXvmQrMd4cN4BnDe4Eyioq0HKR6NVoWs7uC5/ABpKntQdqMWlJ+tU3SLJeQ3IATVamb2aiICb5OCsvsTxqR7CNelydnU2ipCknM6SyNGo8P0lw1GsbP0+S2Oq5V+PqkJp7v72rWFh9e7j0gp+/f0fVHixbRGVb9tKAnNVGIRzOcrVAGZrrzQ797d64Ji8vtaHHiyw4Cqap+3EbnBH08qxDOA/Yh7ZE13L1zirAl1QRYrEAeTnfwtibfuQ0Rmsxe6NlyEu2dLeyxCvBgahRgS2egfJtQTdSqKjCWBuy2czhv7w3qsBbYgEV1UZ9WHULM8T/N1sPIMC9xIOucyw+PBRPb/z3hNgpUS8tPsBQj/2iWKClrKuubu51ksSfM8gLvPTKPhMjqnKrTIsTAYjFDnTvhuwmH/tlT+T0mNVcRHiKx4T6cZ18/NbEV3P5pig19GftmkLstaNRYUOXXFs4Q3QSP9vIcnwDQrWF+thjZeC3n+kKbo5xFpPZ8gPDmK+nZNOrav+HAf+wvMrEqH/SXgLI/sf/MTRrR1gZlmtru7V/WvDfPknqddiWWKzgbOtvpo3Urq1A5nETfw381sXUL3czhxkMcS05gxwQryNFhuAcKqUaynSLeWHNqhDma2PRElNx+wUjovh5UJbe/gK21mBxHCdEEzy9z0DHiByHta1M6Wqe7KFq55z5vZFoTA+4nXLFRZoC8/ysyuoyDlISHks5Hgjbn/IbcW2liRRrBbEML3QkKXex31vtwvuLtbWkg0CY0y5slGku7+UDp3P08DkFLhm43ezWxndx8WVWmRh3gsyYT7rbTUzxNeMHelwUZeXVY62PDwQb6QIa+Fg33Ia6F0rT53f3f691CLIIhFCde4Ktr63e9KuF9CBKdMSOVXI17qYyZ8bWgdxK0Jd72MrjazRpUXzJ3sPYET3f1baRozdTQNF9A6T0NG0uUdSIuVdz2cuJdhyGf2Fq/IUZzjUOJmvDrtZ2q6Ucr6thoRnrqMu69tEe32Ti9JPpP2eThwuJkd7tWpJ4s4G9jAzFYhdOXnE8LyHVWVBoxbE4iRcNko0Ur+L/qep2uqwp0Iw9bt7r5Xum5NjB1nWayuvJiFn+uHKHl5AY+a2bqe3PPSCHg7wmDZxB3rIEaGtBdtGzRILzTwgnUvsbrnBP3CxIKYL6Xv8xCh3VV0GaBA2Dj+TDxXq5jZKl7tVTFMxeHN/c6L/O5Lo0YZuZDC6V6/kEIXTieMlX8lXEx/DZCer7Z2pmHUdTL/IG1FSgiTpjGjaXcE3i1PQ0brlXfNbBfgm4QQNeC7ZvZZd/95TVsz3f3vA8dfNXVsm6E/Tz6ZUfaQfaFmpP/PNP1/N3CUu3/Xwoe6jrxxayahWtqlpKyX/F/0PU/XKXrmRD/Twu/7LxSvwADMejCWcfcjky78KeKlfhHly4q/n4FgmTTaeX8S4GVtvZ14sb1m4GUyaXB/uf12MUjnuYJI15jpfBckDGFvKavQQgjOwlp4LeTa6aTicPeLLSJbm/rdP59UUY8BWzI8SX/dajmN8QjguoK07FBO4E8gdL+dqRO+V5rZWcSbb3FirajMv3NM9b05lh/Y9wuUL/ed0SVM87+IDGp/gVmj58uJ5OxV3GlmuxPTkVWJkMsbKsov5O63DAjrJqGxAFtbBLjsTeioTyS8F6p40SJJ/AcYSgoyb11D3i45ddkI1ohoqLI2uk7RpyQd4g+JKffTVEcXHUXKXubulxG5J7DIiHYUBclSvCIgwqtzKv+JeJG8k+HJ52cQo7nxYIG8sS2N0gsFjpld5+6b2kj3z1rfVlp4LQzQWsVh4aXzUXJ5vM2sKo/3J2m5kEJXvCC5jzfLuFZJnfD9JKFbeRWx4mp2IpZlaC2jsaZLnoYuYZoTBtQMf6NZyOp+xLE/T0znL6F6FNsqQ38ed9/dzHYlXFueJRL41CVX34sY/X/V3R9MKpGf1tTJLNmH0CzxfdUItsmItvEUHcCHUhh+38KveJJXJ7Feseh3d59iYUwbMzzyl0wzs9MqBMVY84yZre/Jz9nMsgTzReyR+tlltP2cuz9nZlgYVu+1SP9YRxcVx3HEIOHY9P19aVuhXcndb7LwPPqnxzpsaxEulve6+4gVauZEWrmaWYQ0bkb4rlYuMTOqTsXNlOVpuNZr8jRYcZjmoV6xVpqZfZMwRuUz+0/3+oxS69X1Z6D8aDL0r0oYD+4glnC6GzjAI7/tmGLdEt8XjmAHt+V+y6bouzC0XBHE9VrL3d9UUu8Kd9+6blvut/vdfZW2v42GdA9+haGosyYjy65tbUi4PGX2mFcRq1qPePHZ8PXYzvbh+bXr2jmXeJl/klA1PAHM6+6V9oMuWEHYfNG23G+HEIbXicTM5s2ECnEbIgr1q2PdxzHHq5dL/iWR/R7iAv+Z8Fe9G/hkVd3RfIgIlVcTKojlCbeutvso7B/hOL5J+n9HItn2/xBW/ZUb7PcqwsH6K8C/tejPwoQBayIhfJvUuRfYOv1vwKepWd6eiD78ebpGD2SfBm1NbbJt4PcRy3UXbcv9tg6hDvl9+pt9dgQWLyi/AOHuNo1Qey2RPisC91S0czrwkYLtewNnjtM9ez/xMrfx2P9AW/MTo8S1CWPgvMD8JWVvL/q/Q5ubE6qV+RqU3YjI9/00oTZ8CXiq7l7KP3+ETr/qXrojyYmFCJ3+pLR9QWqWgZ9TPnUn8a7c/wcTLkskITIuB0hM6/9KhDFOTye5dVvE6Lxo+y+JvLCD2zcALmi472UJXe/1qX9fKCgziZhKH0NEBBnwCcKQ9YuG7Uwq2LZqTZ3rCJeY6cQo7FAiWUtdWzcSqqXs+ybAjSVl307MMB4jQqCzz48Jr5G6tuZtePyTGcrL8GDuMw34REW9ZQg9/NWEIfFbhBrlRmDZcbpvryJUWWO+74K2Gr/08turhFlBvQnAnR37N4UY5NyeBORewNdq6mxN+Opfna7VQ8CWFeVvL/o/fZ86O67DqK9jzQmZmvv/CmJqM64HSIwgXjkG+3mkZHvpDUUkymjTxusJHfULBb/9IgmjjxL+0pelm2rdBvs9MPf/zgO/1d3Etw4eC7FSbV2b6ySh9lD63E7BSypXtvEItqD+dmn/jxOjlhlUjIyA/TreA1sSL/P9iDDZMb9fc21tSPiyHkSkTT2AUBGNZRvLEkE29xDBI+unzxaErrOozku5czwz/V97zlPdU+k265yS/k7PbbuhQb35idnDOpSM5HNlbyaM2ZB76RE+xY1fMn1+6gxuj1hkC/pDusgXA1hEeNVa0DvSJU9DEWXK7LKlw6E86c8szGxNQj+8E2GkO5NQBwzyOk/p+izWBPsrcSM3idp7LxFdBCMNUdtSvQ7Zc8nX8rdm9gli1d+lK45neXd/2AsS35fV8dEbmY4iBPUdnp6YGn5gEf47yxJOgxWtPVY/uapD/7rwVWKavQDNoyzb0nptOnevS6hURWuvhcSzFsnXp5nZEYS6sirApYu3w2Y+FFmZT1g1LzEQmOOpW0ZoaeAw4iJ8z9OihRZhxm909yNLK3ftUIs8DQXuM7N+IgJERrxczOx04Ep3/+HA9r2JpU92renfzYTq4moi1eNzJeWG5QMY/F7Txu0+lHN11v9F3wvqbkiMjBYj9NKLAkd4yVpYozTIdDIyWUQ9bT3w0FSVb7yidV9YWtpnNrXVZW26Lu1sXrTda3yGLaICHyNeQp8iVHDH+fBFeAfrzPHXeKyZoxLrwCwr5gi8Wwhx0f6XIdI0vsCQX+YGxI3ybi9ZNNIiauZrRJTUwyQfV2K9uf8afEOb2UvEaCFz8F2QcBdrkjs0LxA7C/EmVAn6BnXvp90INqu3ISG0r6HiBWsprLOtJbwPLPIAX+nVqyqPto093f2nFknrR5zvogHK7MTM3kWsHvK99P1mYtblhCqt1Id+brjGY01dYp3K+PEG04/WjJWQrdj/Y8Bb0ug9y/L0K3e/sqbqNwlD40o+FH03icjzcCRDq91m7YxmureORcipMTK/Q6HaZBTXykv+b8IjhA69bb2mU/RbCHVX4xWte2Rf4ECLNcpeZHxczbKpe1FazDEfRZnZRoRhdU3iOs0DPFNxTAcSKrOM+Qkd9SLEIKUqgGluuMZjSp3Ot8tSy6PCOuRp6EIHfeB2wGp5QeOxIOjHCJewyYMVrGMqv46Cu+u1qhL0dcLjQOBCi2jCRqk8E0t4s+VXsmP4DHCVDY/7b52waDzx0YcMN+FXqa0RAxSLhE9jzTGEMP0ZMTt8P9WLgs7n7o/kvl/nkRHscStJamSxoO71wOeJiNoH008rErPMly11wndZhpZa3p3Zs9Ry6zwNswkvGuF5rG5ROOrw7qn8utDpWo1yhN7VyHS5mb2twRR9KRvKdvcD0sgrtbces8+YVovFqsdT3f0Zi8Vg1ydya4zldb/CzP7d02IDubb3Ar5A/coUrXH3+81sHo+kNSeZWVUo/eIDdT+R+1qWoWw5YpHONYH/JTxgbgVO8pJ1314u1C0dP6qlljvSdTnt8eZuM3u/D+SQTQ/avRX1ulqMW9HTtWo6gh2k6RR9HmLKmh/FZ1Pu2THSbMNxxCxiHWJGcALhhlhotOrIp4j8uO9w998CWKQf3X2M28lo67Vws5l9pMCY/VFKcnF4WvUktbMBEQm6MbCvmT3p3Vf+nuOpTb1mHZdaHgVdl9Meb/YFzjGzDxFvZid8Oxck8k+UMa467Dw9XKumI9hhtJii/9ndD+vQrz6Y6e6ejE5HpwHEmLo8ufuF6YV1kZntQOQ92JBwu3piLNtKvI8IttiXEPzLEeu4lfEp4DyLxFPZ+npvJHS/O9S0tSDhFbFo+vyJ6qWK5nrqXM1OpuNSy5071CFPw+zEzLYi9NFGRABe0XOXgN6u1QxiJNTKyNR0it7W+6JP0uzsYkIXvRmhKpvq47A0u8W6dOcRUXy7eIm74yj239lrIZXPnhGIZ6TUmG1mx6eyMwhbxU1EFrXxeJnMUdQJ385LLY8lZvZJdz9qdrQ11nSwGHdtZ464Vk0ws+lEFNMbiKn5CcCO7r75QLklvNuSSLMdM1uWmP7/xt1/bWbLA1sMqqlG2UZ+Idv5iRfeS4zxNTaz64lo1kfS96lEYp1FCF1sYUKjjm1dTKRLvZN4mdxINw+auY45zs+3CDN72N1Ll6qfkzGzKRRYjL3FwoBzKl2NTJmvssU6XH9MU/Qx9V/uEzNbEvjb3CpALC2Amft+TGY8M7Ob3H2jMW7PiNHvW9JnbcLwdqO7F/r9vxwYzTr2s5Nxd3EbTzwie+Zx95fc/SQiFv/lwHGEUSYzMv2eGMnWMSMZivYEfmWxQsd4hauPK2a2kZldbWbnmNl6ZnYnMYp7zMy27bt/HenitdAZD+4kVhm5iHA9W5kC982XE3OL8J0rRxCJzGI81cyOMLNPURPnPhcxM43uMiPT0TTzQtiV0BPv7RFR+BoiiGVu5Bgi8vF0YqWXD7v7soTe9/A+OzYKbrZY824YVV4LXTGz/c3sDDN7hFircTvgPiJysnD17JcLc4zawTrkaZgbsJFx7osCx3pFnPvcwlgYmV4GU/Sp7r5u+v8ed18z99tcYzDMY5HT5TziBTnCa8EjSnSs2vo2oeu93t0brfDycmGOEb4vZyyywC3v7vf13ZexpK2RKRkfv07o875CqCiWJGZg73f3uuXF5zhsNubhmN208VoQ7ZHwHWdS2OeRROjlSma2LrE22pjnxeiTJiPYZHw8mBj9Hw+83WMtrjWIaLy5cZSYT6CUJU8ifV/A3edKXbYYf+YWne/czKHAm4AnAdx9KvWrMc/RjMLINNHdL/VY4+1RT2ku3b0qQnCOxt3ncfdJ7v4Kd5+Y/s++S/CKUuZKPepcxkx3/7vZXO2wMcgxDI1gr2RgBEtKul9APn/v4Gq7moKJfykkfMcJM7uQCMu8M4VbzmOxGvH+hIFhbmaiDyXWPyw/gq15ybROlSnEyxWpHcaPHwOXEOuhrU1Yjk8jlkia2/0XO41gNUUXYggZ3MYRixymXyLWXTuFIcHk3vOqA6NBRiYhRo/UDuPLi4SQmp+Ii39ZvOl8dDmAhRBI+I4byer/beB8YH13f7amihDiXwipHcYJM/s18J8+vqt+CCHmUiR8hRCiB+TtIIQQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPfD/AdgK4DVYKbfDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtExposure']=df['BsmtExposure'].fillna(df['BsmtExposure'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtFinType2']=df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 75)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg         Lvl   \n",
       "1          20       RL         80.0     9600   Pave      Reg         Lvl   \n",
       "2          60       RL         68.0    11250   Pave      IR1         Lvl   \n",
       "3          70       RL         60.0     9550   Pave      IR1         Lvl   \n",
       "4          60       RL         84.0    14260   Pave      IR1         Lvl   \n",
       "\n",
       "  Utilities LotConfig LandSlope  ... EnclosedPorch 3SsnPorch ScreenPorch  \\\n",
       "0    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "1    AllPub       FR2       Gtl  ...             0         0           0   \n",
       "2    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "3    AllPub    Corner       Gtl  ...           272         0           0   \n",
       "4    AllPub       FR2       Gtl  ...             0         0           0   \n",
       "\n",
       "  PoolArea MiscVal  MoSold  YrSold  SaleType  SaleCondition SalePrice  \n",
       "0        0       0       2    2008        WD         Normal    208500  \n",
       "1        0       0       5    2007        WD         Normal    181500  \n",
       "2        0       0       9    2008        WD         Normal    223500  \n",
       "3        0       0       2    2006        WD        Abnorml    140000  \n",
       "4        0       0      12    2008        WD         Normal    250000  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',\n",
    "         'Condition2','BldgType','Condition1','HouseStyle','SaleType',\n",
    "        'SaleCondition','ExterCond',\n",
    "         'ExterQual','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n",
    "        'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Heating','HeatingQC',\n",
    "         'CentralAir',\n",
    "         'Electrical','KitchenQual','Functional',\n",
    "         'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_onehot_multcols(multcolumns):\n",
    "    df_final=final_df\n",
    "    i=0\n",
    "    for fields in multcolumns:\n",
    "        \n",
    "        print(fields)\n",
    "        df1=pd.get_dummies(final_df[fields],drop_first=True)\n",
    "        \n",
    "        final_df.drop([fields],axis=1,inplace=True)\n",
    "        if i==0:\n",
    "            df_final=df1.copy()\n",
    "        else:\n",
    "            \n",
    "            df_final=pd.concat([df_final,df1],axis=1)\n",
    "        i=i+1\n",
    "       \n",
    "        \n",
    "    df_final=pd.concat([final_df,df_final],axis=1)\n",
    "        \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine Test Data \n",
    "\n",
    "test_df=pd.read_csv('formulatedtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 74)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0          20       RH         80.0    11622   Pave      Reg         Lvl   \n",
       "1          20       RL         81.0    14267   Pave      IR1         Lvl   \n",
       "2          60       RL         74.0    13830   Pave      IR1         Lvl   \n",
       "3          60       RL         78.0     9978   Pave      IR1         Lvl   \n",
       "4         120       RL         43.0     5005   Pave      IR1         HLS   \n",
       "\n",
       "  Utilities LotConfig LandSlope  ... OpenPorchSF EnclosedPorch 3SsnPorch  \\\n",
       "0    AllPub    Inside       Gtl  ...           0             0         0   \n",
       "1    AllPub    Corner       Gtl  ...          36             0         0   \n",
       "2    AllPub    Inside       Gtl  ...          34             0         0   \n",
       "3    AllPub    Inside       Gtl  ...          36             0         0   \n",
       "4    AllPub    Inside       Gtl  ...          82             0         0   \n",
       "\n",
       "  ScreenPorch PoolArea  MiscVal  MoSold  YrSold  SaleType SaleCondition  \n",
       "0         120        0        0       6    2010        WD        Normal  \n",
       "1           0        0    12500       6    2010        WD        Normal  \n",
       "2           0        0        0       3    2010        WD        Normal  \n",
       "3           0        0        0       6    2010        WD        Normal  \n",
       "4         144        0        0       1    2010        WD        Normal  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 74)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.concat([df,test_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500.0\n",
       "1       181500.0\n",
       "2       223500.0\n",
       "3       140000.0\n",
       "4       250000.0\n",
       "          ...   \n",
       "1454         NaN\n",
       "1455         NaN\n",
       "1456         NaN\n",
       "1457         NaN\n",
       "1458         NaN\n",
       "Name: SalePrice, Length: 2881, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 75)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "Street\n",
      "LotShape\n",
      "LandContour\n",
      "Utilities\n",
      "LotConfig\n",
      "LandSlope\n",
      "Neighborhood\n",
      "Condition2\n",
      "BldgType\n",
      "Condition1\n",
      "HouseStyle\n",
      "SaleType\n",
      "SaleCondition\n",
      "ExterCond\n",
      "ExterQual\n",
      "Foundation\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "RoofStyle\n",
      "RoofMatl\n",
      "Exterior1st\n",
      "Exterior2nd\n",
      "MasVnrType\n",
      "Heating\n",
      "HeatingQC\n",
      "CentralAir\n",
      "Electrical\n",
      "KitchenQual\n",
      "Functional\n",
      "FireplaceQu\n",
      "GarageType\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PavedDrive\n"
     ]
    }
   ],
   "source": [
    "final_df=category_onehot_multcols(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 235)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df =final_df.loc[:,~final_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 175)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>160</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>160</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>20</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1960</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>85</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td>94.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2881 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0             60         65.0     8450            7            5       2003   \n",
       "1             20         80.0     9600            6            8       1976   \n",
       "2             60         68.0    11250            7            5       2001   \n",
       "3             70         60.0     9550            7            5       1915   \n",
       "4             60         84.0    14260            8            5       2000   \n",
       "...          ...          ...      ...          ...          ...        ...   \n",
       "1454         160         21.0     1936            4            7       1970   \n",
       "1455         160         21.0     1894            4            5       1970   \n",
       "1456          20        160.0    20000            5            7       1960   \n",
       "1457          85         62.0    10441            5            5       1992   \n",
       "1458          60         74.0     9627            7            5       1993   \n",
       "\n",
       "      YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0             2003       196.0       706.0         0.0  ...     0     0    1   \n",
       "1             1976         0.0       978.0         0.0  ...     0     0    1   \n",
       "2             2002       162.0       486.0         0.0  ...     0     0    1   \n",
       "3             1970         0.0       216.0         0.0  ...     0     0    1   \n",
       "4             2000       350.0       655.0         0.0  ...     0     0    1   \n",
       "...            ...         ...         ...         ...  ...   ...   ...  ...   \n",
       "1454          1970         0.0         0.0         0.0  ...     0     0    1   \n",
       "1455          1970         0.0       252.0         0.0  ...     0     0    1   \n",
       "1456          1996         0.0      1224.0         0.0  ...     0     0    1   \n",
       "1457          1992         0.0       337.0         0.0  ...     0     0    1   \n",
       "1458          1994        94.0       758.0         0.0  ...     0     0    1   \n",
       "\n",
       "      Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0          1        0        0        0       0    1  0  \n",
       "1          1        0        0        0       0    1  0  \n",
       "2          1        0        0        0       0    1  0  \n",
       "3          0        0        0        0       1    0  0  \n",
       "4          1        0        0        0       0    1  0  \n",
       "...      ...      ...      ...      ...     ...  ... ..  \n",
       "1454       1        0        0        0       0    0  0  \n",
       "1455       0        0        0        1       0    0  0  \n",
       "1456       0        0        0        0       1    0  0  \n",
       "1457       1        0        0        0       0    0  0  \n",
       "1458       1        0        0        0       0    0  0  \n",
       "\n",
       "[2881 rows x 175 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=final_df.iloc[:1422,:]\n",
    "df_Test=final_df.iloc[1422:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          2003       196.0       706.0         0.0  ...     0     0    1   \n",
       "1          1976         0.0       978.0         0.0  ...     0     0    1   \n",
       "2          2002       162.0       486.0         0.0  ...     0     0    1   \n",
       "3          1970         0.0       216.0         0.0  ...     0     0    1   \n",
       "4          2000       350.0       655.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    1  0  \n",
       "1       1        0        0        0       0    1  0  \n",
       "2       1        0        0        0       0    1  0  \n",
       "3       0        0        0        0       1    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>20.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          20         80.0    11622            5            6       1961   \n",
       "1          20         81.0    14267            6            6       1958   \n",
       "2          60         74.0    13830            5            5       1997   \n",
       "3          60         78.0     9978            6            6       1998   \n",
       "4         120         43.0     5005            8            5       1992   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          1961         0.0       468.0       144.0  ...     0     0    1   \n",
       "1          1958       108.0       923.0         0.0  ...     0     0    1   \n",
       "2          1998         0.0       791.0         0.0  ...     0     0    1   \n",
       "3          1998        20.0       602.0         0.0  ...     0     0    1   \n",
       "4          1992         0.0       263.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    0  0  \n",
       "1       1        0        0        0       0    0  0  \n",
       "2       1        0        0        0       0    0  0  \n",
       "3       1        0        0        0       0    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 175)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olalekantaofeek/opt/anaconda3/envs/sklearn/lib/python3.8/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "df_Test.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction Algortithms\n",
    "import xgboost\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost classifier\n",
    "classifier = xgboost.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = ['gbtree','gblinear']\n",
    "base_score = [0.25,0.5,0.75,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters Optimization\n",
    "n_estimators = [100,500,900,1100,1500]\n",
    "max_depth = [2,3,5,10,15]\n",
    "booster = ['gbtree','gblinear']\n",
    "learning_rate = [0.05,0.1,0.15,0.20]\n",
    "min_child_weight = [1,2,3,4,5]\n",
    "\n",
    "# define the grid of Hyper Parameters to search\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators':n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster':booster,\n",
    "    'base_score':base_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a random search with 4 folds cross-validation\n",
    "random_cv = RandomizedSearchCV(estimator=regressor,\n",
    "                              param_distributions=hyperparameter_grid,\n",
    "                              cv=5,\n",
    "                              n_iter=50,\n",
    "                              scoring='neg_mean_absolute_error',\n",
    "                              n_jobs=4,\n",
    "                              verbose=5,\n",
    "                              return_train_score=True,\n",
    "                              random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[02:53:57] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END base_score=0.25, booster=gblinear, learning_rate=0.05, max_depth=5, min_child_weight=2, n_estimators=100;, score=(train=-19294.990, test=-19210.279) total time=   0.3s\n",
      "[CV 2/5] END base_score=1, booster=gbtree, learning_rate=0.2, max_depth=15, min_child_weight=4, n_estimators=500;, score=(train=-7.323, test=-18429.068) total time=  23.2s\n",
      "[02:54:20] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END base_score=0.25, booster=gblinear, learning_rate=0.05, max_depth=3, min_child_weight=1, n_estimators=900;, score=(train=-16754.448, test=-22594.241) total time=   2.4s\n",
      "[02:54:23] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END base_score=1, booster=gblinear, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=100;, score=(train=-18312.891, test=-18364.280) total time=   0.3s\n",
      "[02:54:23] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/5] END base_score=1, booster=gblinear, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=100;, score=(train=-17509.687, test=-22176.352) total time=   0.3s\n",
      "[02:54:23] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END base_score=1, booster=gblinear, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=100;, score=(train=-17156.172, test=-22903.928) total time=   0.3s\n",
      "[02:54:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 5/5] END base_score=1, booster=gblinear, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=100;, score=(train=-16812.571, test=-22395.773) total time=   0.3s\n",
      "[02:54:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END base_score=0.75, booster=gblinear, learning_rate=0.2, max_depth=15, min_child_weight=3, n_estimators=1500;, score=(train=-17620.890, test=-18969.345) total time=   4.1s\n",
      "[02:54:28] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 5/5] END base_score=0.75, booster=gblinear, learning_rate=0.2, max_depth=15, min_child_weight=3, n_estimators=1500;, score=(train=-15525.458, test=-21780.996) total time=   4.4s\n",
      "[02:54:33] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END base_score=0.5, booster=gblinear, learning_rate=0.15, max_depth=2, min_child_weight=2, n_estimators=500;, score=(train=-17780.729, test=-18484.081) total time=   1.4s\n",
      "[02:54:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 4/5] END base_score=0.5, booster=gblinear, learning_rate=0.15, max_depth=2, min_child_weight=2, n_estimators=500;, score=(train=-17652.521, test=-19331.381) total time=   1.3s\n",
      "[CV 3/5] END base_score=0.25, booster=gbtree, learning_rate=0.1, max_depth=10, min_child_weight=3, n_estimators=500;, score=(train=-11.150, test=-17488.039) total time=  15.0s\n",
      "[CV 5/5] END base_score=0.25, booster=gbtree, learning_rate=0.1, max_depth=10, min_child_weight=3, n_estimators=500;, score=(train=-9.807, test=-18148.126) total time=  16.7s\n",
      "[02:55:07] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 5/5] END base_score=0.25, booster=gblinear, learning_rate=0.15, max_depth=5, min_child_weight=1, n_estimators=1100;, score=(train=-15504.513, test=-21619.616) total time=   3.1s\n",
      "[CV 4/5] END base_score=1, booster=gbtree, learning_rate=0.2, max_depth=3, min_child_weight=2, n_estimators=1500;, score=(train=-229.349, test=-14939.006) total time=  14.1s\n",
      "[02:55:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END base_score=1, booster=gblinear, learning_rate=0.15, max_depth=10, min_child_weight=3, n_estimators=500;, score=(train=-16568.493, test=-22377.510) total time=   1.3s\n",
      "[CV 1/5] END base_score=0.5, booster=gbtree, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=500;, score=(train=-8870.753, test=-15999.518) total time=   2.9s\n",
      "[CV 4/5] END base_score=0.5, booster=gbtree, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=500;, score=(train=-9274.330, test=-14260.380) total time=   3.2s\n",
      "[CV 2/5] END base_score=0.5, booster=gbtree, learning_rate=0.2, max_depth=15, min_child_weight=5, n_estimators=1100;, score=(train=-7.319, test=-18419.736) total time=  48.5s\n",
      "[02:56:20] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END base_score=1, booster=gblinear, learning_rate=0.15, max_depth=15, min_child_weight=2, n_estimators=1100;, score=(train=-17665.563, test=-18855.176) total time=   3.1s\n",
      "[02:56:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 4/5] END base_score=1, booster=gblinear, learning_rate=0.15, max_depth=15, min_child_weight=2, n_estimators=1100;, score=(train=-17511.123, test=-19837.498) total time=   3.0s\n",
      "[CV 2/5] END base_score=1, booster=gbtree, learning_rate=0.2, max_depth=15, min_child_weight=2, n_estimators=900;, score=(train=-7.324, test=-18399.209) total time=  49.8s\n",
      "[CV 5/5] END base_score=0.5, booster=gbtree, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100;, score=(train=-6275.629, test=-17265.555) total time=   1.5s\n",
      "[CV 2/5] END base_score=0.25, booster=gbtree, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=500;, score=(train=-7.686, test=-18524.523) total time=  14.8s\n",
      "[CV 4/5] END base_score=0.25, booster=gbtree, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=500;, score=(train=-8.249, test=-15228.272) total time=  14.0s\n",
      "[02:57:47] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END base_score=0.25, booster=gblinear, learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=1500;, score=(train=-17767.691, test=-18544.418) total time=   4.2s\n",
      "[02:57:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 4/5] END base_score=0.25, booster=gblinear, learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=1500;, score=(train=-17622.136, test=-19274.893) total time=   4.2s\n",
      "[CV 3/5] END base_score=0.25, booster=gbtree, learning_rate=0.15, max_depth=5, min_child_weight=5, n_estimators=500;, score=(train=-371.470, test=-17491.238) total time=   8.1s\n",
      "[CV 2/5] END base_score=0.5, booster=gbtree, learning_rate=0.2, max_depth=3, min_child_weight=4, n_estimators=900;, score=(train=-840.329, test=-17745.919) total time=   8.4s\n",
      "[02:58:12] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END base_score=0.5, booster=gblinear, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=1500;, score=(train=-17679.854, test=-18785.546) total time=   4.5s\n",
      "[02:58:16] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 4/5] END base_score=0.5, booster=gblinear, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=1500;, score=(train=-17512.544, test=-19711.436) total time=   3.7s\n",
      "[02:58:20] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END base_score=0.5, booster=gblinear, learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=1100;, score=(train=-16542.665, test=-22267.523) total time=   2.6s\n",
      "[02:58:23] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END base_score=0.75, booster=gblinear, learning_rate=0.05, max_depth=3, min_child_weight=4, n_estimators=100;, score=(train=-18283.467, test=-23521.206) total time=   0.2s\n",
      "[02:58:23] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 5/5] END base_score=0.75, booster=gblinear, learning_rate=0.05, max_depth=3, min_child_weight=4, n_estimators=100;, score=(train=-17658.813, test=-22787.350) total time=   0.3s\n",
      "[CV 3/5] END base_score=0.25, booster=gbtree, learning_rate=0.1, max_depth=10, min_child_weight=2, n_estimators=100;, score=(train=-1174.732, test=-17282.390) total time=   3.1s\n",
      "[CV 5/5] END base_score=0.25, booster=gbtree, learning_rate=0.1, max_depth=10, min_child_weight=2, n_estimators=100;, score=(train=-1296.121, test=-17738.677) total time=   3.0s\n",
      "[CV 2/5] END base_score=1, booster=gbtree, learning_rate=0.1, max_depth=15, min_child_weight=5, n_estimators=1100;, score=(train=-7.425, test=-18167.850) total time=  47.5s\n",
      "[CV 2/5] END base_score=0.75, booster=gbtree, learning_rate=0.1, max_depth=2, min_child_weight=2, n_estimators=500;, score=(train=-8941.916, test=-17118.993) total time=   3.0s\n",
      "[CV 4/5] END base_score=0.75, booster=gbtree, learning_rate=0.1, max_depth=2, min_child_weight=2, n_estimators=500;, score=(train=-9196.885, test=-14466.900) total time=   3.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None,\n",
       "                                          enable_categorical=False, gamma=None,\n",
       "                                          gpu_id=None, importance_type=None,\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_constraints=...\n",
       "                                          validate_parameters=None,\n",
       "                                          verbosity=None),\n",
       "                   n_iter=50, n_jobs=4,\n",
       "                   param_distributions={'base_score': [0.25, 0.5, 0.75, 1],\n",
       "                                        'booster': ['gbtree', 'gblinear'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
       "                                        'max_depth': [2, 3, 5, 10, 15],\n",
       "                                        'min_child_weight': [1, 2, 3, 4, 5],\n",
       "                                        'n_estimators': [100, 500, 900, 1100,\n",
       "                                                         1500]},\n",
       "                   random_state=42, return_train_score=True,\n",
       "                   scoring='neg_mean_absolute_error', verbose=5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.75, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=2, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=500, n_jobs=8,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the best estimator\n",
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.75, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=2, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=500, n_jobs=8,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the best estimator\n",
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgboost.XGBRegressor(base_score=0.25,booster='gbtree',colsample_bylabel=1,\n",
    "                                colsample_bytree=1,\n",
    "                                gamma=0,\n",
    "                                learning_rate=0.1,\n",
    "                                max_deltastep=0,\n",
    "                                max_depth=2,\n",
    "                                min_child_weight=1,\n",
    "                                missing=None,\n",
    "                                n_estimators=900,\n",
    "                                n_jobs=1,\n",
    "                                nthread=None,\n",
    "                                objective='reg:linear',\n",
    "                                random_state=0,\n",
    "                                reg_alpha=0,\n",
    "                                reg_lambda=1,\n",
    "                                scale_pos_weight=1,\n",
    "                                seed=None,\n",
    "                                silent=True,\n",
    "                                subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:15:58] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[03:15:58] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylabel\", \"max_deltastep\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylabel=1,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             enable_categorical=False, gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
       "             max_deltastep=0, max_depth=2, min_child_weight=1, missing=None,\n",
       "             monotone_constraints='()', n_estimators=900, n_jobs=1, nthread=1,\n",
       "             num_parallel_tree=1, objective='reg:linear', predictor='auto',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=0, silent=True, ...)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename= 'finalized_model.pkl'\n",
    "pickle.dump(regressor,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 174)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>20.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          20         80.0    11622            5            6       1961   \n",
       "1          20         81.0    14267            6            6       1958   \n",
       "2          60         74.0    13830            5            5       1997   \n",
       "3          60         78.0     9978            6            6       1998   \n",
       "4         120         43.0     5005            8            5       1992   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          1961         0.0       468.0       144.0  ...     0     0    1   \n",
       "1          1958       108.0       923.0         0.0  ...     0     0    1   \n",
       "2          1998         0.0       791.0         0.0  ...     0     0    1   \n",
       "3          1998        20.0       602.0         0.0  ...     0     0    1   \n",
       "4          1992         0.0       263.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    0  0  \n",
       "1       1        0        0        0       0    0  0  \n",
       "2       1        0        0        0       0    0  0  \n",
       "3       1        0        0        0       0    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 174 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[03:17:58] /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/c_api/c_api_utils.h:161: Invalid missing value: null\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000012f9e18b0 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000012f9db150 xgboost::GetMissing(xgboost::Json const&) + 340\n  [bt] (2) 3   libxgboost.dylib                    0x000000012f9e8c80 void InplacePredictImpl<xgboost::data::ArrayAdapter>(std::__1::shared_ptr<xgboost::data::ArrayAdapter>, std::__1::shared_ptr<xgboost::DMatrix>, char const*, xgboost::Learner*, unsigned long, unsigned long, unsigned long long const**, unsigned long long*, float const**) + 500\n  [bt] (3) 4   libxgboost.dylib                    0x000000012f9e8708 XGBoosterPredictFromDense + 344\n  [bt] (4) 5   libffi.8.dylib                      0x000000010558c04c ffi_call_SYSV + 76\n  [bt] (5) 6   libffi.8.dylib                      0x0000000105589790 ffi_call_int + 1256\n  [bt] (6) 7   _ctypes.cpython-38-darwin.so        0x0000000105560590 _ctypes_callproc + 1200\n  [bt] (7) 8   _ctypes.cpython-38-darwin.so        0x000000010555a7e4 PyCFuncPtr_call + 1172\n  [bt] (8) 9   python                              0x00000001044acf50 _PyObject_MakeTpCall + 640\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xz/nfk7q05d0mlgj2g1c6wxmczr0000gn/T/ipykernel_28314/4157387341.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_Test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/sklearn/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m                 predts = self.get_booster().inplace_predict(\n\u001b[0m\u001b[1;32m    882\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m                     \u001b[0miteration_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sklearn/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2032\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ensure_np_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_np_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2034\u001b[0;31m             _check_call(\n\u001b[0m\u001b[1;32m   2035\u001b[0m                 _LIB.XGBoosterPredictFromDense(\n\u001b[1;32m   2036\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sklearn/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \"\"\"\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [03:17:58] /Users/runner/miniforge3/conda-bld/xgboost-split_1637426411619/work/src/c_api/c_api_utils.h:161: Invalid missing value: null\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000012f9e18b0 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000012f9db150 xgboost::GetMissing(xgboost::Json const&) + 340\n  [bt] (2) 3   libxgboost.dylib                    0x000000012f9e8c80 void InplacePredictImpl<xgboost::data::ArrayAdapter>(std::__1::shared_ptr<xgboost::data::ArrayAdapter>, std::__1::shared_ptr<xgboost::DMatrix>, char const*, xgboost::Learner*, unsigned long, unsigned long, unsigned long long const**, unsigned long long*, float const**) + 500\n  [bt] (3) 4   libxgboost.dylib                    0x000000012f9e8708 XGBoosterPredictFromDense + 344\n  [bt] (4) 5   libffi.8.dylib                      0x000000010558c04c ffi_call_SYSV + 76\n  [bt] (5) 6   libffi.8.dylib                      0x0000000105589790 ffi_call_int + 1256\n  [bt] (6) 7   _ctypes.cpython-38-darwin.so        0x0000000105560590 _ctypes_callproc + 1200\n  [bt] (7) 8   _ctypes.cpython-38-darwin.so        0x000000010555a7e4 PyCFuncPtr_call + 1172\n  [bt] (8) 9   python                              0x00000001044acf50 _PyObject_MakeTpCall + 640\n\n"
     ]
    }
   ],
   "source": [
    "y_pred = regressor.predict(df_Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xz/nfk7q05d0mlgj2g1c6wxmczr0000gn/T/ipykernel_28314/3562620839.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample submission file\n",
    "pred= pd.DataFrame(y_pred)\n",
    "sub_df=pd.read_csv('sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2\n",
    "pred.columns=['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df=df_Train['SalePrice'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.column=['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taofe\\Anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_Train.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=pd.concat([df_Train,temp_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1329</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>928</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>926</td>\n",
       "      <td>678</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n",
       "0       896         0          0             2       468.0       144.0   \n",
       "1      1329         0          0             3       923.0         0.0   \n",
       "2       928       701          0             3       791.0         0.0   \n",
       "3       926       678          0             3       602.0         0.0   \n",
       "4      1280         0          0             2       263.0         0.0   \n",
       "\n",
       "   BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  ...  Min1  Min2  Typ  \\\n",
       "0           0.0           0.0      270.0              0  ...     0     0    1   \n",
       "1           0.0           0.0      406.0              0  ...     0     0    1   \n",
       "2           0.0           0.0      137.0              0  ...     0     0    1   \n",
       "3           0.0           0.0      324.0              0  ...     0     0    1   \n",
       "4           0.0           0.0     1017.0              0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    0  0  \n",
       "1       1        0        0        0       0    0  0  \n",
       "2       1        0        0        0       0    0  0  \n",
       "3       1        0        0        0       0    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 174 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test=pd.concat([df_Test,pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=pd.concat([df_Train,df_Test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 175)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Implementation\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adamax\n",
    "from keras import losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\taofe\\Anaconda3\\envs\\python36\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\taofe\\Anaconda3\\envs\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taofe\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=174, units=50, kernel_initializer=\"he_uniform\")`\n",
      "  \n",
      "C:\\Users\\taofe\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=25, kernel_initializer=\"he_uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\taofe\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=50, kernel_initializer=\"he_uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\taofe\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, kernel_initializer=\"he_uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\taofe\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\taofe\\Anaconda3\\envs\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 2304 samples, validate on 577 samples\n",
      "Epoch 1/1000\n",
      "2304/2304 [==============================] - 0s 216us/step - loss: 18192305512.0556 - val_loss: 3927272240.9151\n",
      "Epoch 2/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 6190288895.4722 - val_loss: 3520605098.9255\n",
      "Epoch 3/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 5471404724.5556 - val_loss: 3249874720.1664\n",
      "Epoch 4/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 4887242467.9167 - val_loss: 2996825276.5615\n",
      "Epoch 5/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 4389887234.8611 - val_loss: 2799707306.7036\n",
      "Epoch 6/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 3896144545.9444 - val_loss: 2524436591.3622\n",
      "Epoch 7/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 3500956319.6944 - val_loss: 2372914006.2946\n",
      "Epoch 8/1000\n",
      "2304/2304 [==============================] - 0s 98us/step - loss: 3173665868.9167 - val_loss: 2198289681.1924\n",
      "Epoch 9/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 2820672227.0833 - val_loss: 1954731609.2340\n",
      "Epoch 10/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 2552649133.9722 - val_loss: 1793685251.6049\n",
      "Epoch 11/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 2308831675.9444 - val_loss: 1640823453.2270\n",
      "Epoch 12/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 2098439680.7569 - val_loss: 1579685282.9393\n",
      "Epoch 13/1000\n",
      "2304/2304 [==============================] - 0s 124us/step - loss: 1929762263.8889 - val_loss: 1467921006.9740\n",
      "Epoch 14/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 1784418962.0208 - val_loss: 1433704567.1820\n",
      "Epoch 15/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 1692518528.9861 - val_loss: 1370652661.5737\n",
      "Epoch 16/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 1635859236.2361 - val_loss: 1352937676.5615\n",
      "Epoch 17/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 1606700578.4028 - val_loss: 1393185888.6101\n",
      "Epoch 18/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 1618156999.89 - 0s 108us/step - loss: 1590929612.8194 - val_loss: 1351303856.9705\n",
      "Epoch 19/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 1579113391.8333 - val_loss: 1369015252.7972\n",
      "Epoch 20/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 1568167401.8194 - val_loss: 1406816651.3137\n",
      "Epoch 21/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 1574048600.6458 - val_loss: 1366626015.5286\n",
      "Epoch 22/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 1553588335.6528 - val_loss: 1372502840.8735\n",
      "Epoch 23/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 1540150185.5833 - val_loss: 1386415341.8925\n",
      "Epoch 24/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 1531895555.7500 - val_loss: 1367327273.7054\n",
      "Epoch 25/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 1522345170.5556 - val_loss: 1372059354.4818\n",
      "Epoch 26/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 1519409529.8472 - val_loss: 1394526833.9965\n",
      "Epoch 27/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 1509123801.9167 - val_loss: 1377659742.0589\n",
      "Epoch 28/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 1492947045.5556 - val_loss: 1365632615.0433\n",
      "Epoch 29/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 1483429151.5278 - val_loss: 1402220066.4957\n",
      "Epoch 30/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 1483502244.5556 - val_loss: 1371117538.8284\n",
      "Epoch 31/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 1478013935.6389 - val_loss: 1372355770.2600\n",
      "Epoch 32/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 1472991442.3750 - val_loss: 1369591249.8302\n",
      "Epoch 33/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 1462311738.3403 - val_loss: 1373030706.1629\n",
      "Epoch 34/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 1458670141.5712 - val_loss: 1365104368.1109\n",
      "Epoch 35/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 1456224074.4306 - val_loss: 1352182596.3536\n",
      "Epoch 36/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 1437062943.7222 - val_loss: 1415866819.8267\n",
      "Epoch 37/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 1454252145.0694 - val_loss: 1338597898.7314\n",
      "Epoch 38/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 1427140722.0590 - val_loss: 1350545081.7608\n",
      "Epoch 39/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 1419771827.3958 - val_loss: 1346388290.7730\n",
      "Epoch 40/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 1416461891.7083 - val_loss: 1339255615.6672\n",
      "Epoch 41/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 1417979784.0417 - val_loss: 1342693657.6222\n",
      "Epoch 42/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 1415439629.4861 - val_loss: 1350119930.6205\n",
      "Epoch 43/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 1404117778.9583 - val_loss: 1328826385.0953\n",
      "Epoch 44/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 1399469875.6667 - val_loss: 1322897021.8787\n",
      "Epoch 45/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 1397860884.6806 - val_loss: 1321115823.4731\n",
      "Epoch 46/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 1391557240.0833 - val_loss: 1306687414.1282\n",
      "Epoch 47/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 1386804947.3264 - val_loss: 1318150120.6932\n",
      "Epoch 48/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 1373636244.9444 - val_loss: 1306983264.5685\n",
      "Epoch 49/1000\n",
      "2304/2304 [==============================] - 0s 128us/step - loss: 1372748785.4375 - val_loss: 1300762378.9255\n",
      "Epoch 50/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 1371080244.1181 - val_loss: 1291207874.4679\n",
      "Epoch 51/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 1366110734.0486 - val_loss: 1275288417.7054\n",
      "Epoch 52/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 1362069581.9653 - val_loss: 1271076200.5685\n",
      "Epoch 53/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 1350201195.6806 - val_loss: 1272488647.8475\n",
      "Epoch 54/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 1350413996.9861 - val_loss: 1276645896.5823\n",
      "Epoch 55/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 1341095567.6667 - val_loss: 1260197730.3016\n",
      "Epoch 56/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 1333593370.7431 - val_loss: 1252853576.8873\n",
      "Epoch 57/1000\n",
      "2304/2304 [==============================] - 0s 139us/step - loss: 1326736260.3056 - val_loss: 1245334946.4541\n",
      "Epoch 58/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 1315495793.1389 - val_loss: 1260157008.0416\n",
      "Epoch 59/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 1318449430.0625 - val_loss: 1270928148.0347\n",
      "Epoch 60/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 1316281915.1597 - val_loss: 1268035787.3414\n",
      "Epoch 61/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 1307171739.9444 - val_loss: 1233283884.3674\n",
      "Epoch 62/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 1318101699.1354 - val_loss: 1234030027.5355\n",
      "Epoch 63/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 1296206108.6944 - val_loss: 1259808431.8198\n",
      "Epoch 64/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 1290354121.0556 - val_loss: 1224928653.3241\n",
      "Epoch 65/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 1288034297.2639 - val_loss: 1234210109.9619\n",
      "Epoch 66/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 1289000231.1944 - val_loss: 1227541301.6153\n",
      "Epoch 67/1000\n",
      "2304/2304 [==============================] - 0s 124us/step - loss: 1284146131.0278 - val_loss: 1214951347.3692\n",
      "Epoch 68/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 1275459393.91 - 0s 127us/step - loss: 1273707278.8264 - val_loss: 1218278265.0815\n",
      "Epoch 69/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 1271091097.2153 - val_loss: 1195470285.6153\n",
      "Epoch 70/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 1261402270.7083 - val_loss: 1186440838.2946\n",
      "Epoch 71/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 1253306056.2500 - val_loss: 1179880135.9307\n",
      "Epoch 72/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 1261655167.9514 - val_loss: 1171272147.7712\n",
      "Epoch 73/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 1240994823.3194 - val_loss: 1176836686.8908\n",
      "Epoch 74/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 1234795414.3056 - val_loss: 1226095964.6586\n",
      "Epoch 75/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 1244846731.0764 - val_loss: 1157643580.6170\n",
      "Epoch 76/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 1238510364.1250 - val_loss: 1161121631.7504\n",
      "Epoch 77/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 1223228637.5764 - val_loss: 1173440068.3258\n",
      "Epoch 78/1000\n",
      "2304/2304 [==============================] - 0s 123us/step - loss: 1230378518.0208 - val_loss: 1175010657.4627\n",
      "Epoch 79/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 1218591645.1736 - val_loss: 1157482810.7036\n",
      "Epoch 80/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 1214169805.0278 - val_loss: 1168139439.1334\n",
      "Epoch 81/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 1201947324.0000 - val_loss: 1177471625.0745\n",
      "Epoch 82/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 1203352234.3542 - val_loss: 1186053400.1386\n",
      "Epoch 83/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 1196440587.2292 - val_loss: 1129568170.8007\n",
      "Epoch 84/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 1189427918.4722 - val_loss: 1164193754.4818\n",
      "Epoch 85/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 1188080267.5625 - val_loss: 1137505135.3345\n",
      "Epoch 86/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 1177537204.6806 - val_loss: 1167402210.4263\n",
      "Epoch 87/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 1176100827.3889 - val_loss: 1119806060.1040\n",
      "Epoch 88/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 1173398731.3403 - val_loss: 1110222604.8111\n",
      "Epoch 89/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 1163186858.8264 - val_loss: 1145873362.1768\n",
      "Epoch 90/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 1158192063.9514 - val_loss: 1114473751.9029\n",
      "Epoch 91/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 1152800123.8403 - val_loss: 1120603698.5095\n",
      "Epoch 92/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 1152401787.7014 - val_loss: 1134621894.2392\n",
      "Epoch 93/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 1144697092.7153 - val_loss: 1098620929.8024\n",
      "Epoch 94/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 1139651024.8611 - val_loss: 1088167304.2357\n",
      "Epoch 95/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 1127375081.9583 - val_loss: 1115488966.6828\n",
      "Epoch 96/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 1132673921.3542 - val_loss: 1070599203.6049\n",
      "Epoch 97/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 1123566312.8889 - val_loss: 1080794770.5234\n",
      "Epoch 98/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 1124357132.5208 - val_loss: 1059267271.1265\n",
      "Epoch 99/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 1114186901.5625 - val_loss: 1155735220.9636\n",
      "Epoch 100/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 1112961129.8958 - val_loss: 1060561916.7140\n",
      "Epoch 101/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 1108592661.2639 - val_loss: 1175456602.2877\n",
      "Epoch 102/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 1098047862.5278 - val_loss: 1084379775.3899\n",
      "Epoch 103/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 1101524796.0556 - val_loss: 1050600164.5477\n",
      "Epoch 104/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 1096084518.0278 - val_loss: 1041687972.2288\n",
      "Epoch 105/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 1088991179.3542 - val_loss: 1038082704.2773\n",
      "Epoch 106/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 1090559945.7083 - val_loss: 1025992623.0295\n",
      "Epoch 107/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 1082819764.8472 - val_loss: 1053702287.6118\n",
      "Epoch 108/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 1077353546.2083 - val_loss: 1033833634.2461\n",
      "Epoch 109/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 1084070476.4722 - val_loss: 1015195339.7019\n",
      "Epoch 110/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 1076578980.2708 - val_loss: 1020073431.5702\n",
      "Epoch 111/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 1071006327.8889 - val_loss: 1035375566.2253\n",
      "Epoch 112/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 1067162526.1111 - val_loss: 1052394727.2374\n",
      "Epoch 113/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 1053896905.8958 - val_loss: 994735306.7868\n",
      "Epoch 114/1000\n",
      "2304/2304 [==============================] - 0s 156us/step - loss: 1059043404.3472 - val_loss: 1004825120.3328\n",
      "Epoch 115/1000\n",
      "2304/2304 [==============================] - 0s 135us/step - loss: 1057050020.0972 - val_loss: 1000083571.6603\n",
      "Epoch 116/1000\n",
      "2304/2304 [==============================] - 0s 141us/step - loss: 1047149708.7622 - val_loss: 972846605.1161\n",
      "Epoch 117/1000\n",
      "2304/2304 [==============================] - 0s 133us/step - loss: 1045521156.2847 - val_loss: 999003491.3276\n",
      "Epoch 118/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 1049058436.6111 - val_loss: 1017026322.2738\n",
      "Epoch 119/1000\n",
      "2304/2304 [==============================] - 0s 128us/step - loss: 1047238038.4653 - val_loss: 1043942695.5979\n",
      "Epoch 120/1000\n",
      "2304/2304 [==============================] - 0s 136us/step - loss: 1034262402.7292 - val_loss: 982881290.7036\n",
      "Epoch 121/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 1036441387.1806 - val_loss: 1023400645.7678\n",
      "Epoch 122/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 1029207825.8611 - val_loss: 940803522.3570\n",
      "Epoch 123/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 1024719065.1528 - val_loss: 956759213.7262\n",
      "Epoch 124/1000\n",
      "2304/2304 [==============================] - 0s 132us/step - loss: 1027437888.0833 - val_loss: 946825482.1906\n",
      "Epoch 125/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 1020896876.0625 - val_loss: 938417984.4437\n",
      "Epoch 126/1000\n",
      "2304/2304 [==============================] - 0s 135us/step - loss: 1027013954.4931 - val_loss: 926919492.5477\n",
      "Epoch 127/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 1018014999.3611 - val_loss: 955802444.1733\n",
      "Epoch 128/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 1015859211.7778 - val_loss: 939137749.9341\n",
      "Epoch 129/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 0s 132us/step - loss: 1015317196.2500 - val_loss: 921783310.8076\n",
      "Epoch 130/1000\n",
      "2304/2304 [==============================] - 0s 155us/step - loss: 1010602860.1458 - val_loss: 914936549.7678\n",
      "Epoch 131/1000\n",
      "2304/2304 [==============================] - 0s 141us/step - loss: 1010947982.5347 - val_loss: 911078504.4437\n",
      "Epoch 132/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 1006230756.7361 - val_loss: 910740501.1993\n",
      "Epoch 133/1000\n",
      "2304/2304 [==============================] - 0s 137us/step - loss: 1014694934.1701 - val_loss: 917023809.3310\n",
      "Epoch 134/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 1005945873.3403 - val_loss: 909884638.0589\n",
      "Epoch 135/1000\n",
      "2304/2304 [==============================] - 0s 131us/step - loss: 997919952.7292 - val_loss: 923337941.9480\n",
      "Epoch 136/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 995232503.6597 - val_loss: 1047278510.6412\n",
      "Epoch 137/1000\n",
      "2304/2304 [==============================] - 0s 126us/step - loss: 997226294.8472 - val_loss: 889207419.7574\n",
      "Epoch 138/1000\n",
      "2304/2304 [==============================] - 0s 136us/step - loss: 986802167.3542 - val_loss: 902176792.9151\n",
      "Epoch 139/1000\n",
      "2304/2304 [==============================] - 0s 136us/step - loss: 990488928.7431 - val_loss: 920027965.6430\n",
      "Epoch 140/1000\n",
      "2304/2304 [==============================] - 0s 133us/step - loss: 987989345.5486 - val_loss: 936187114.6759\n",
      "Epoch 141/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 984834125.9375 - val_loss: 937763174.1005\n",
      "Epoch 142/1000\n",
      "2304/2304 [==============================] - 0s 134us/step - loss: 979809367.0486 - val_loss: 886069223.8752\n",
      "Epoch 143/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 982000065.9792 - val_loss: 867857951.3068\n",
      "Epoch 144/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 979422208.7708 - val_loss: 873808801.0399\n",
      "Epoch 145/1000\n",
      "2304/2304 [==============================] - 0s 141us/step - loss: 974085954.2778 - val_loss: 895841786.4818\n",
      "Epoch 146/1000\n",
      "2304/2304 [==============================] - 0s 136us/step - loss: 971086930.8819 - val_loss: 1014127790.0867\n",
      "Epoch 147/1000\n",
      "2304/2304 [==============================] - 0s 133us/step - loss: 974378572.2569 - val_loss: 891769654.0728\n",
      "Epoch 148/1000\n",
      "2304/2304 [==============================] - 0s 137us/step - loss: 963390419.6111 - val_loss: 864593754.2184\n",
      "Epoch 149/1000\n",
      "2304/2304 [==============================] - 0s 139us/step - loss: 955254822.4201 - val_loss: 883903195.7574\n",
      "Epoch 150/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 958643023.4097 - val_loss: 869947120.3882\n",
      "Epoch 151/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 947973301.8472 - val_loss: 865932188.8804\n",
      "Epoch 152/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 957324529.9792 - val_loss: 840670641.2756\n",
      "Epoch 153/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 949278177.4097 - val_loss: 837257652.4783\n",
      "Epoch 154/1000\n",
      "2304/2304 [==============================] - 0s 128us/step - loss: 946692123.4236 - val_loss: 831752584.7764\n",
      "Epoch 155/1000\n",
      "2304/2304 [==============================] - 0s 132us/step - loss: 944397219.1806 - val_loss: 835902959.7088\n",
      "Epoch 156/1000\n",
      "2304/2304 [==============================] - 0s 132us/step - loss: 948328714.8611 - val_loss: 826471641.9549\n",
      "Epoch 157/1000\n",
      "2304/2304 [==============================] - 0s 132us/step - loss: 938378779.8264 - val_loss: 871642146.3570\n",
      "Epoch 158/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 943557561.6806 - val_loss: 815827143.4454\n",
      "Epoch 159/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 938027250.1493 - val_loss: 842516277.4627\n",
      "Epoch 160/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 939022535.6528 - val_loss: 804532634.9116\n",
      "Epoch 161/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 934691975.7083 - val_loss: 810446950.6828\n",
      "Epoch 162/1000\n",
      "2304/2304 [==============================] - 0s 132us/step - loss: 925653029.0764 - val_loss: 835353671.5009\n",
      "Epoch 163/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 920215527.8750 - val_loss: 851360642.0243\n",
      "Epoch 164/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 917993947.3750 - val_loss: 789149338.7036\n",
      "Epoch 165/1000\n",
      "2304/2304 [==============================] - 0s 126us/step - loss: 914476475.3611 - val_loss: 810529976.0139\n",
      "Epoch 166/1000\n",
      "2304/2304 [==============================] - 0s 126us/step - loss: 915874602.7639 - val_loss: 887774176.9151\n",
      "Epoch 167/1000\n",
      "2304/2304 [==============================] - 0s 137us/step - loss: 910957276.3681 - val_loss: 786012062.0728\n",
      "Epoch 168/1000\n",
      "2304/2304 [==============================] - 0s 141us/step - loss: 900141076.3819 - val_loss: 781597106.3570\n",
      "Epoch 169/1000\n",
      "2304/2304 [==============================] - 0s 141us/step - loss: 908685795.7917 - val_loss: 809212504.3466\n",
      "Epoch 170/1000\n",
      "2304/2304 [==============================] - 0s 150us/step - loss: 891086551.9861 - val_loss: 793999728.7348\n",
      "Epoch 171/1000\n",
      "2304/2304 [==============================] - 0s 143us/step - loss: 900487470.7847 - val_loss: 785638547.4523\n",
      "Epoch 172/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 899841858.7083 - val_loss: 808144045.8510\n",
      "Epoch 173/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 898739084.4514 - val_loss: 794192834.2184\n",
      "Epoch 174/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 892880587.4653 - val_loss: 809724648.4714\n",
      "Epoch 175/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 884084208.0764 - val_loss: 812350937.9272\n",
      "Epoch 176/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 884154039.3611 - val_loss: 790007123.7990\n",
      "Epoch 177/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 870348430.0833 - val_loss: 883331635.4662\n",
      "Epoch 178/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 882636393.2361 - val_loss: 777953009.3310\n",
      "Epoch 179/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 877131791.3333 - val_loss: 798167142.9463\n",
      "Epoch 180/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 890514323.4375 - val_loss: 752176237.8094\n",
      "Epoch 181/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 868717255.5833 - val_loss: 745497754.8007\n",
      "Epoch 182/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 877109191.6111 - val_loss: 740908562.7175\n",
      "Epoch 183/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 872851358.4444 - val_loss: 805076648.5407\n",
      "Epoch 184/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 871329381.2708 - val_loss: 773264611.1889\n",
      "Epoch 185/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 874001461.7986 - val_loss: 767054917.9341\n",
      "Epoch 186/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 850348891.9306 - val_loss: 732492375.2513\n",
      "Epoch 187/1000\n",
      "2304/2304 [==============================] - 0s 132us/step - loss: 852459938.3819 - val_loss: 851175156.7695\n",
      "Epoch 188/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 853110505.8889 - val_loss: 732434240.8735\n",
      "Epoch 189/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 855369081.2118 - val_loss: 784378674.5511\n",
      "Epoch 190/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 838963355.6111 - val_loss: 742276808.5407\n",
      "Epoch 191/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 847606031.9236 - val_loss: 700193364.4645\n",
      "Epoch 192/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 844048643.1562 - val_loss: 718515946.1906\n",
      "Epoch 193/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 852919452.8194 - val_loss: 722460390.1560\n",
      "Epoch 194/1000\n",
      "2304/2304 [==============================] - 0s 100us/step - loss: 838489678.1875 - val_loss: 692796244.9081\n",
      "Epoch 195/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 852307207.6562 - val_loss: 702678625.8579\n",
      "Epoch 196/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 837608636.1944 - val_loss: 734002762.7314\n",
      "Epoch 197/1000\n",
      "2304/2304 [==============================] - 0s 100us/step - loss: 838837394.2708 - val_loss: 716645481.0537\n",
      "Epoch 198/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 825514181.5694 - val_loss: 705487292.7834\n",
      "Epoch 199/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 818780609.3611 - val_loss: 719122947.3276\n",
      "Epoch 200/1000\n",
      "2304/2304 [==============================] - 0s 124us/step - loss: 825304288.0069 - val_loss: 685759921.9549\n",
      "Epoch 201/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 805635661.2951 - val_loss: 744395557.1300\n",
      "Epoch 202/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 836390463.0903 - val_loss: 687129917.8925\n",
      "Epoch 203/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 818137003.5868 - val_loss: 670985141.2548\n",
      "Epoch 204/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 807521630.7014 - val_loss: 664582599.5563\n",
      "Epoch 205/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 799251541.9583 - val_loss: 672333365.6291\n",
      "Epoch 206/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 797892182.1597 - val_loss: 669186754.4679\n",
      "Epoch 207/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 798344980.0764 - val_loss: 662272776.2080\n",
      "Epoch 208/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 807321015.5000 - val_loss: 682103639.2513\n",
      "Epoch 209/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 799217416.7500 - val_loss: 668627215.2374\n",
      "Epoch 210/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 810742810.4236 - val_loss: 691003873.9965\n",
      "Epoch 211/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 783041852.0521 - val_loss: 743101165.0052\n",
      "Epoch 212/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 783641790.4410 - val_loss: 669496094.0451\n",
      "Epoch 213/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 774576694.4688 - val_loss: 663234331.9792\n",
      "Epoch 214/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 769136917.3056 - val_loss: 679592854.7106\n",
      "Epoch 215/1000\n",
      "2304/2304 [==============================] - 0s 124us/step - loss: 774653910.2917 - val_loss: 661003116.3397\n",
      "Epoch 216/1000\n",
      "2304/2304 [==============================] - 0s 131us/step - loss: 770403064.9722 - val_loss: 650180921.2340\n",
      "Epoch 217/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 770849404.4201 - val_loss: 653676895.6950\n",
      "Epoch 218/1000\n",
      "2304/2304 [==============================] - 0s 133us/step - loss: 754628483.3333 - val_loss: 809533641.2062\n",
      "Epoch 219/1000\n",
      "2304/2304 [==============================] - 0s 132us/step - loss: 768595498.3438 - val_loss: 671971312.7903\n",
      "Epoch 220/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 752564534.4201 - val_loss: 622493251.7574\n",
      "Epoch 221/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 754705760.3889 - val_loss: 628811527.8128\n",
      "Epoch 222/1000\n",
      "2304/2304 [==============================] - 0s 123us/step - loss: 756044977.6319 - val_loss: 641077346.3709\n",
      "Epoch 223/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 760800284.1250 - val_loss: 620388017.5113\n",
      "Epoch 224/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 751255168.4306 - val_loss: 618454711.3692\n",
      "Epoch 225/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 751107370.7986 - val_loss: 630225226.0936\n",
      "Epoch 226/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 744562682.4514 - val_loss: 619585963.6742\n",
      "Epoch 227/1000\n",
      "2304/2304 [==============================] - 0s 138us/step - loss: 728074221.0556 - val_loss: 638133887.6811\n",
      "Epoch 228/1000\n",
      "2304/2304 [==============================] - 0s 146us/step - loss: 727750757.8715 - val_loss: 614751198.1976\n",
      "Epoch 229/1000\n",
      "2304/2304 [==============================] - 0s 138us/step - loss: 732403995.7847 - val_loss: 609281200.6794\n",
      "Epoch 230/1000\n",
      "2304/2304 [==============================] - 0s 136us/step - loss: 724623120.5625 - val_loss: 650726618.6898\n",
      "Epoch 231/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 714488498.0347 - val_loss: 590404326.3224\n",
      "Epoch 232/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 721833303.0938 - val_loss: 607261789.0607\n",
      "Epoch 233/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 712826790.0278 - val_loss: 604008398.0728\n",
      "Epoch 234/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 697604460.1146 - val_loss: 663093833.8856\n",
      "Epoch 235/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 722032949.9722 - val_loss: 678441406.8631\n",
      "Epoch 236/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 714000790.0000 - val_loss: 595248694.9601\n",
      "Epoch 237/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 714408151.6111 - val_loss: 591044428.7140\n",
      "Epoch 238/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 704297355.7014 - val_loss: 747854765.2964\n",
      "Epoch 239/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 701748642.4097 - val_loss: 566868411.2721\n",
      "Epoch 240/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 688498452.9028 - val_loss: 581138322.5511\n",
      "Epoch 241/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 707451786.5208 - val_loss: 575334214.6551\n",
      "Epoch 242/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 705371590.8854 - val_loss: 557393076.3951\n",
      "Epoch 243/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 682621812.8090 - val_loss: 574034114.5927\n",
      "Epoch 244/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 689001398.7639 - val_loss: 575546597.9064\n",
      "Epoch 245/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 681149417.6215 - val_loss: 584973013.0745\n",
      "Epoch 246/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 682841474.1007 - val_loss: 615765095.9861\n",
      "Epoch 247/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 689400351.8646 - val_loss: 570947555.3276\n",
      "Epoch 248/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 677677496.8333 - val_loss: 570058653.7262\n",
      "Epoch 249/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 659748532.8681 - val_loss: 622010200.9289\n",
      "Epoch 250/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 662278126.6528 - val_loss: 542150998.4471\n",
      "Epoch 251/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 661809311.6181 - val_loss: 558433432.5962\n",
      "Epoch 252/1000\n",
      "2304/2304 [==============================] - 0s 123us/step - loss: 663676042.8438 - val_loss: 702031035.0503\n",
      "Epoch 253/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 668191789.9097 - val_loss: 549054078.1282\n",
      "Epoch 254/1000\n",
      "2304/2304 [==============================] - 0s 123us/step - loss: 659046759.5000 - val_loss: 546197041.2894\n",
      "Epoch 255/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 642274742.5347 - val_loss: 600329419.8752\n",
      "Epoch 256/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 648770828.9792 - val_loss: 553951202.6482\n",
      "Epoch 257/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 636096729.5972 - val_loss: 568537711.4593\n",
      "Epoch 258/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 638487617.0243 - val_loss: 556270554.5927\n",
      "Epoch 259/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 647845606.2500 - val_loss: 549213480.1664\n",
      "Epoch 260/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 642069560.2292 - val_loss: 542558153.3449\n",
      "Epoch 261/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 0s 120us/step - loss: 647830060.8576 - val_loss: 541548852.5338\n",
      "Epoch 262/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 649870923.8472 - val_loss: 523643908.0624\n",
      "Epoch 263/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 635994841.3542 - val_loss: 630134999.1404\n",
      "Epoch 264/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 638517366.4583 - val_loss: 537463620.4922\n",
      "Epoch 265/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 626015742.8646 - val_loss: 506049405.7123\n",
      "Epoch 266/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 615248739.9271 - val_loss: 536097001.5529\n",
      "Epoch 267/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 619115154.6597 - val_loss: 514748582.6412\n",
      "Epoch 268/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 614696221.7292 - val_loss: 745362184.8458\n",
      "Epoch 269/1000\n",
      "2304/2304 [==============================] - 0s 123us/step - loss: 619971979.7639 - val_loss: 490337906.8562\n",
      "Epoch 270/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 598230664.4861 - val_loss: 510605191.9445\n",
      "Epoch 271/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 607943830.0694 - val_loss: 538225711.5286\n",
      "Epoch 272/1000\n",
      "2304/2304 [==============================] - 0s 140us/step - loss: 608186094.9688 - val_loss: 557698309.7678\n",
      "Epoch 273/1000\n",
      "2304/2304 [==============================] - 0s 137us/step - loss: 627233014.5312 - val_loss: 504474619.0919\n",
      "Epoch 274/1000\n",
      "2304/2304 [==============================] - 0s 132us/step - loss: 603127156.9306 - val_loss: 542328083.4107\n",
      "Epoch 275/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 591584136.7674 - val_loss: 481974708.6724\n",
      "Epoch 276/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 611022707.2257 - val_loss: 623928087.1265\n",
      "Epoch 277/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 607473522.3368 - val_loss: 489201267.7435\n",
      "Epoch 278/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 608976349.7118 - val_loss: 470607520.4159\n",
      "Epoch 279/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 581826430.8715 - val_loss: 537097318.9740\n",
      "Epoch 280/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 588163740.4549 - val_loss: 628087382.5165\n",
      "Epoch 281/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 598928323.9410 - val_loss: 749192951.4454\n",
      "Epoch 282/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 609387585.7986 - val_loss: 515387722.3016\n",
      "Epoch 283/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 578867994.0000 - val_loss: 498883518.5026\n",
      "Epoch 284/1000\n",
      "2304/2304 [==============================] - 0s 132us/step - loss: 576427653.6111 - val_loss: 465147710.6135\n",
      "Epoch 285/1000\n",
      "2304/2304 [==============================] - 0s 137us/step - loss: 575221442.9514 - val_loss: 449024514.9671\n",
      "Epoch 286/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 568127860.1007 - val_loss: 474366135.5841\n",
      "Epoch 287/1000\n",
      "2304/2304 [==============================] - 0s 133us/step - loss: 577277610.3802 - val_loss: 453587114.5095\n",
      "Epoch 288/1000\n",
      "2304/2304 [==============================] - 0s 134us/step - loss: 574955765.5139 - val_loss: 501403132.1733\n",
      "Epoch 289/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 564072927.7917 - val_loss: 434771463.2236\n",
      "Epoch 290/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 564628002.9757 - val_loss: 442566180.9913\n",
      "Epoch 291/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 564018729.6597 - val_loss: 498098261.3795\n",
      "Epoch 292/1000\n",
      "2304/2304 [==============================] - 0s 140us/step - loss: 565092896.0764 - val_loss: 443559748.9359\n",
      "Epoch 293/1000\n",
      "2304/2304 [==============================] - 0s 142us/step - loss: 565678656.8715 - val_loss: 452099478.6135\n",
      "Epoch 294/1000\n",
      "2304/2304 [==============================] - 0s 140us/step - loss: 558560058.6424 - val_loss: 424382200.9705\n",
      "Epoch 295/1000\n",
      "2304/2304 [==============================] - 0s 128us/step - loss: 545241213.8611 - val_loss: 423173777.6638\n",
      "Epoch 296/1000\n",
      "2304/2304 [==============================] - 0s 135us/step - loss: 548438498.9826 - val_loss: 448984133.8787\n",
      "Epoch 297/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 540758041.9062 - val_loss: 441097051.4523\n",
      "Epoch 298/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 543432276.9306 - val_loss: 414809459.5633\n",
      "Epoch 299/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 548698622.5556 - val_loss: 443600881.9133\n",
      "Epoch 300/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 557208616.0035 - val_loss: 436501937.2617\n",
      "Epoch 301/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 565503306.9097 - val_loss: 420178479.5009\n",
      "Epoch 302/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 560339460.1667 - val_loss: 446492981.0745\n",
      "Epoch 303/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 528661142.8715 - val_loss: 527155185.7886\n",
      "Epoch 304/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 522583462.2292 - val_loss: 440361591.9584\n",
      "Epoch 305/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 534723625.8056 - val_loss: 452729028.0208\n",
      "Epoch 306/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 531622905.6042 - val_loss: 419249958.2808\n",
      "Epoch 307/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 525328449.8403 - val_loss: 451382397.2548\n",
      "Epoch 308/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 543656168.6319 - val_loss: 463976775.5147\n",
      "Epoch 309/1000\n",
      "2304/2304 [==============================] - 0s 128us/step - loss: 516970077.5972 - val_loss: 402701352.8735\n",
      "Epoch 310/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 520956364.5938 - val_loss: 518631038.4679\n",
      "Epoch 311/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 536840951.9688 - val_loss: 406244729.9272\n",
      "Epoch 312/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 537099846.1354 - val_loss: 405965122.5511\n",
      "Epoch 313/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 532155882.2014 - val_loss: 408135499.5910\n",
      "Epoch 314/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 518652998.6875 - val_loss: 405296981.7816\n",
      "Epoch 315/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 533525157.2431 - val_loss: 420730996.4367\n",
      "Epoch 316/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 509864287.5729 - val_loss: 382698417.4419\n",
      "Epoch 317/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 540758291.0660 - val_loss: 404479738.1768\n",
      "Epoch 318/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 526017403.7569 - val_loss: 400501602.8284\n",
      "Epoch 319/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 526107490.3819 - val_loss: 392179656.8458\n",
      "Epoch 320/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 533327838.6111 - val_loss: 405709318.6274\n",
      "Epoch 321/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 535755600.9306 - val_loss: 424180614.1421\n",
      "Epoch 322/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 514120168.5278 - val_loss: 433073669.1369\n",
      "Epoch 323/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 508807677.8299 - val_loss: 441590980.2288\n",
      "Epoch 324/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 516760192.6979 - val_loss: 366106186.9255\n",
      "Epoch 325/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 547513814.2465 - val_loss: 367358578.2045\n",
      "Epoch 326/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 509984980.1771 - val_loss: 491749615.9931\n",
      "Epoch 327/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 504915829.0521 - val_loss: 413121480.9428\n",
      "Epoch 328/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 501825340.3542 - val_loss: 396881873.1785\n",
      "Epoch 329/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 507326967.1701 - val_loss: 384720356.4159\n",
      "Epoch 330/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 500829048.8993 - val_loss: 365434661.9896\n",
      "Epoch 331/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 524101737.4861 - val_loss: 385679600.1802\n",
      "Epoch 332/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 516777587.3750 - val_loss: 408447553.8302\n",
      "Epoch 333/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 499900537.6354 - val_loss: 465303254.5789\n",
      "Epoch 334/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 502140804.5972 - val_loss: 374252917.3102\n",
      "Epoch 335/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 510410591.2118 - val_loss: 368673618.1490\n",
      "Epoch 336/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 500943668.2361 - val_loss: 618636174.5858\n",
      "Epoch 337/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 509035419.0660 - val_loss: 394210681.2340\n",
      "Epoch 338/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 487567707.2743 - val_loss: 430240763.2998\n",
      "Epoch 339/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 484042713.5903 - val_loss: 355827332.2842\n",
      "Epoch 340/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 491925365.0486 - val_loss: 402907017.6083\n",
      "Epoch 341/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 516609622.4306 - val_loss: 365928376.3328\n",
      "Epoch 342/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 514708408.7083 - val_loss: 376661140.1525\n",
      "Epoch 343/1000\n",
      "2304/2304 [==============================] - 0s 140us/step - loss: 495855961.5174 - val_loss: 358122224.9289\n",
      "Epoch 344/1000\n",
      "2304/2304 [==============================] - 0s 136us/step - loss: 486193929.7257 - val_loss: 362526719.5286\n",
      "Epoch 345/1000\n",
      "2304/2304 [==============================] - 0s 135us/step - loss: 506708405.9826 - val_loss: 454249581.1300\n",
      "Epoch 346/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 497796928.2257 - val_loss: 355095910.7244\n",
      "Epoch 347/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 488860335.4271 - val_loss: 422347020.2288\n",
      "Epoch 348/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 489109454.9896 - val_loss: 357668319.4870\n",
      "Epoch 349/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 480190860.6493 - val_loss: 362094815.1681\n",
      "Epoch 350/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 479886411.3889 - val_loss: 390060227.1750\n",
      "Epoch 351/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 467312730.8090 - val_loss: 564672506.4263\n",
      "Epoch 352/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 491984026.4514 - val_loss: 377379017.6222\n",
      "Epoch 353/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 490175485.0712 - val_loss: 361781967.1265\n",
      "Epoch 354/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 475836925.3681 - val_loss: 635977139.5217\n",
      "Epoch 355/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 483498063.9306 - val_loss: 554937658.9393\n",
      "Epoch 356/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 483910844.3125 - val_loss: 353407748.3120\n",
      "Epoch 357/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 483383575.1354 - val_loss: 369979084.8250\n",
      "Epoch 358/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 478433116.6736 - val_loss: 351330288.7348\n",
      "Epoch 359/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 470705525.2569 - val_loss: 347114043.1196\n",
      "Epoch 360/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 485037220.6979 - val_loss: 405021831.7227\n",
      "Epoch 361/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 470267225.4097 - val_loss: 411807989.6222\n",
      "Epoch 362/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 468164732.1285 - val_loss: 463264246.7106\n",
      "Epoch 363/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 476541633.8264 - val_loss: 349762472.9151\n",
      "Epoch 364/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 471197923.9375 - val_loss: 374490212.7764\n",
      "Epoch 365/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 475204483.0417 - val_loss: 340677466.9809\n",
      "Epoch 366/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 476180223.9549 - val_loss: 401854453.1716\n",
      "Epoch 367/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 482361090.1111 - val_loss: 331785951.1820\n",
      "Epoch 368/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 463770818.6493 - val_loss: 333618990.1144\n",
      "Epoch 369/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 467924573.1840 - val_loss: 352977405.6014\n",
      "Epoch 370/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 471120992.5312 - val_loss: 381142558.2392\n",
      "Epoch 371/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 471755508.8351 - val_loss: 344209629.2686\n",
      "Epoch 372/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 456849067.8854 - val_loss: 354539859.9515\n",
      "Epoch 373/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 455800799.8507 - val_loss: 380965760.7903\n",
      "Epoch 374/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 466795603.0000 - val_loss: 351324583.5841\n",
      "Epoch 375/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 449676525.8611 - val_loss: 333976095.5979\n",
      "Epoch 376/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 460233084.9826 - val_loss: 471047453.1924\n",
      "Epoch 377/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 479554692.1910 - val_loss: 330088923.8406\n",
      "Epoch 378/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 457184048.4479 - val_loss: 354095728.3050\n",
      "Epoch 379/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 483833927.4132 - val_loss: 372700266.2184\n",
      "Epoch 380/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 458333095.4896 - val_loss: 346522758.9185\n",
      "Epoch 381/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 462708110.2882 - val_loss: 352380506.8007\n",
      "Epoch 382/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 463251676.9219 - val_loss: 324825154.3432\n",
      "Epoch 383/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 457311639.7778 - val_loss: 334488753.0953\n",
      "Epoch 384/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 444678184.8611 - val_loss: 342215889.9133\n",
      "Epoch 385/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 471682363.8889 - val_loss: 333642290.0243\n",
      "Epoch 386/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 459598844.5903 - val_loss: 319330549.7123\n",
      "Epoch 387/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 451495956.2743 - val_loss: 347481433.2756\n",
      "Epoch 388/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 432036643.6910 - val_loss: 329528449.3033\n",
      "Epoch 389/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 465359850.2083 - val_loss: 345590795.6742\n",
      "Epoch 390/1000\n",
      "2304/2304 [==============================] - 0s 135us/step - loss: 450981126.4757 - val_loss: 363735323.2028\n",
      "Epoch 391/1000\n",
      "2304/2304 [==============================] - 0s 137us/step - loss: 444637261.6215 - val_loss: 385039561.7886\n",
      "Epoch 392/1000\n",
      "2304/2304 [==============================] - 0s 128us/step - loss: 431058978.4479 - val_loss: 329845769.7054\n",
      "Epoch 393/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 0s 122us/step - loss: 454560825.4444 - val_loss: 350305918.8908\n",
      "Epoch 394/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 475677955.2465 - val_loss: 327258514.8700\n",
      "Epoch 395/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 448847001.5938 - val_loss: 340474169.9133\n",
      "Epoch 396/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 450690687.9167 - val_loss: 372439678.9948\n",
      "Epoch 397/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 461218014.2743 - val_loss: 352208469.0052\n",
      "Epoch 398/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 446176222.2917 - val_loss: 326150574.1560\n",
      "Epoch 399/1000\n",
      "2304/2304 [==============================] - 0s 123us/step - loss: 456799438.9653 - val_loss: 314557631.4593\n",
      "Epoch 400/1000\n",
      "2304/2304 [==============================] - 0s 128us/step - loss: 432883278.3403 - val_loss: 328892608.6378\n",
      "Epoch 401/1000\n",
      "2304/2304 [==============================] - 0s 144us/step - loss: 433034681.0521 - val_loss: 333110359.6672\n",
      "Epoch 402/1000\n",
      "2304/2304 [==============================] - 0s 150us/step - loss: 431722971.3056 - val_loss: 334494662.9324\n",
      "Epoch 403/1000\n",
      "2304/2304 [==============================] - 0s 145us/step - loss: 440732572.2049 - val_loss: 314198602.2322\n",
      "Epoch 404/1000\n",
      "2304/2304 [==============================] - 0s 140us/step - loss: 437522046.1944 - val_loss: 330456790.7799\n",
      "Epoch 405/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 461595816.5000 - val_loss: 318465825.8302\n",
      "Epoch 406/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 449682263.2188 - val_loss: 307893499.3414\n",
      "Epoch 407/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 440073922.3160 - val_loss: 308267617.8995\n",
      "Epoch 408/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 428468767.9028 - val_loss: 323530062.3224\n",
      "Epoch 409/1000\n",
      "2304/2304 [==============================] - 0s 123us/step - loss: 464890606.3611 - val_loss: 309387407.5286\n",
      "Epoch 410/1000\n",
      "2304/2304 [==============================] - 0s 123us/step - loss: 434111858.3403 - val_loss: 319080152.4021\n",
      "Epoch 411/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 446976270.3785 - val_loss: 327667837.6430\n",
      "Epoch 412/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 438784085.7674 - val_loss: 318390535.7643\n",
      "Epoch 413/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 444768190.9931 - val_loss: 317904578.4679\n",
      "Epoch 414/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 434736469.8368 - val_loss: 306998751.9029\n",
      "Epoch 415/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 442464207.1250 - val_loss: 350468354.5095\n",
      "Epoch 416/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 426815679.0625 - val_loss: 313219830.8492\n",
      "Epoch 417/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 430724293.7188 - val_loss: 322009074.2322\n",
      "Epoch 418/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 418373514.5312 - val_loss: 372799421.5043\n",
      "Epoch 419/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 466742151.2674 - val_loss: 303182519.7643\n",
      "Epoch 420/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 450963245.0660 - val_loss: 327791688.3744\n",
      "Epoch 421/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 421985125.2049 - val_loss: 330774353.6222\n",
      "Epoch 422/1000\n",
      "2304/2304 [==============================] - 0s 124us/step - loss: 441007380.4427 - val_loss: 326205798.9740\n",
      "Epoch 423/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 438612243.4583 - val_loss: 304651222.6690\n",
      "Epoch 424/1000\n",
      "2304/2304 [==============================] - 0s 126us/step - loss: 426646994.1181 - val_loss: 335836454.0589\n",
      "Epoch 425/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 422550065.4670 - val_loss: 315855609.9133\n",
      "Epoch 426/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 427750802.2500 - val_loss: 319859536.4021\n",
      "Epoch 427/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 418362248.4549 - val_loss: 319253605.7539\n",
      "Epoch 428/1000\n",
      "2304/2304 [==============================] - 0s 145us/step - loss: 418770493.3056 - val_loss: 299891174.6967\n",
      "Epoch 429/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 424693099.6285 - val_loss: 304687466.3986\n",
      "Epoch 430/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 404726229.0903 - val_loss: 318371041.8440\n",
      "Epoch 431/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 410381591.5139 - val_loss: 334391345.8024\n",
      "Epoch 432/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 430942620.1736 - val_loss: 316619839.9029\n",
      "Epoch 433/1000\n",
      "2304/2304 [==============================] - 0s 132us/step - loss: 428280640.9514 - val_loss: 299066509.3518\n",
      "Epoch 434/1000\n",
      "2304/2304 [==============================] - 0s 131us/step - loss: 418995614.0139 - val_loss: 330582001.8718\n",
      "Epoch 435/1000\n",
      "2304/2304 [==============================] - 0s 147us/step - loss: 419479382.3247 - val_loss: 308789231.5841\n",
      "Epoch 436/1000\n",
      "2304/2304 [==============================] - 0s 135us/step - loss: 437458290.0694 - val_loss: 411913061.5875\n",
      "Epoch 437/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 446507730.2708 - val_loss: 303335757.7123\n",
      "Epoch 438/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 435646835.5486 - val_loss: 324491363.0225\n",
      "Epoch 439/1000\n",
      "2304/2304 [==============================] - 0s 140us/step - loss: 434838989.9583 - val_loss: 431843528.6101\n",
      "Epoch 440/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 423595922.9757 - val_loss: 300170978.2738\n",
      "Epoch 441/1000\n",
      "2304/2304 [==============================] - 0s 132us/step - loss: 440490027.7257 - val_loss: 344721412.9220\n",
      "Epoch 442/1000\n",
      "2304/2304 [==============================] - 0s 138us/step - loss: 420116315.5868 - val_loss: 311142069.9896\n",
      "Epoch 443/1000\n",
      "2304/2304 [==============================] - 0s 137us/step - loss: 422277349.3368 - val_loss: 343469061.3518\n",
      "Epoch 444/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 404704789.6146 - val_loss: 403419331.8891\n",
      "Epoch 445/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 418967147.3750 - val_loss: 306700049.5667\n",
      "Epoch 446/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 424337691.3229 - val_loss: 310020187.3137\n",
      "Epoch 447/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 409517824.2326 - val_loss: 361597918.4887\n",
      "Epoch 448/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 416278386.1042 - val_loss: 310809105.2617\n",
      "Epoch 449/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 420838169.1840 - val_loss: 291480980.8111\n",
      "Epoch 450/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 435975442.0729 - val_loss: 301381055.5563\n",
      "Epoch 451/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 406752618.3090 - val_loss: 306444657.8995\n",
      "Epoch 452/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 400823929.8438 - val_loss: 302323817.4558\n",
      "Epoch 453/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 404792525.9757 - val_loss: 317247612.0624\n",
      "Epoch 454/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 423765667.8785 - val_loss: 313132028.5338\n",
      "Epoch 455/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 402093138.8576 - val_loss: 288226552.6794\n",
      "Epoch 456/1000\n",
      "2304/2304 [==============================] - 0s 145us/step - loss: 402719945.5417 - val_loss: 339079160.5130\n",
      "Epoch 457/1000\n",
      "2304/2304 [==============================] - 0s 138us/step - loss: 412325162.3611 - val_loss: 291782289.7192\n",
      "Epoch 458/1000\n",
      "2304/2304 [==============================] - 0s 140us/step - loss: 397511537.6875 - val_loss: 338017327.2860\n",
      "Epoch 459/1000\n",
      "2304/2304 [==============================] - 0s 133us/step - loss: 405576070.5833 - val_loss: 298747835.8128\n",
      "Epoch 460/1000\n",
      "2304/2304 [==============================] - 0s 128us/step - loss: 405462627.9757 - val_loss: 304735338.8146\n",
      "Epoch 461/1000\n",
      "2304/2304 [==============================] - 0s 155us/step - loss: 393271781.8681 - val_loss: 295841380.5199\n",
      "Epoch 462/1000\n",
      "2304/2304 [==============================] - 0s 139us/step - loss: 403614463.9896 - val_loss: 430219387.5009\n",
      "Epoch 463/1000\n",
      "2304/2304 [==============================] - 0s 124us/step - loss: 406318301.4896 - val_loss: 300114794.1629\n",
      "Epoch 464/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 408279933.6806 - val_loss: 294589847.5563\n",
      "Epoch 465/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 385294107.7396 - val_loss: 342462843.4246\n",
      "Epoch 466/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 415204033.9062 - val_loss: 301072859.5771\n",
      "Epoch 467/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 403559857.2569 - val_loss: 310595564.9220\n",
      "Epoch 468/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 385832786.3403 - val_loss: 385197397.5737\n",
      "Epoch 469/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 394099792.5451 - val_loss: 298577688.7903\n",
      "Epoch 470/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 406962121.8264 - val_loss: 325523441.1646\n",
      "Epoch 471/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 419063583.4236 - val_loss: 302356056.3744\n",
      "Epoch 472/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 401751977.8090 - val_loss: 299707420.9775\n",
      "Epoch 473/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 403191430.9514 - val_loss: 296354970.2322\n",
      "Epoch 474/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 391206431.3819 - val_loss: 319133744.6516\n",
      "Epoch 475/1000\n",
      "2304/2304 [==============================] - 0s 144us/step - loss: 410156454.2431 - val_loss: 325899007.7920\n",
      "Epoch 476/1000\n",
      "2304/2304 [==============================] - 0s 123us/step - loss: 406716901.4931 - val_loss: 306349765.3795\n",
      "Epoch 477/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 386959563.0694 - val_loss: 432443820.3050\n",
      "Epoch 478/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 394509541.9236 - val_loss: 485114291.1473\n",
      "Epoch 479/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 406888655.8056 - val_loss: 291665243.0641\n",
      "Epoch 480/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 380761654.0625 - val_loss: 308444127.2929\n",
      "Epoch 481/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 384491121.4271 - val_loss: 293818866.4541\n",
      "Epoch 482/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 386427867.7569 - val_loss: 294592322.0520\n",
      "Epoch 483/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 404798259.5486 - val_loss: 289594534.5997\n",
      "Epoch 484/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 391684832.5972 - val_loss: 284725435.0087\n",
      "Epoch 485/1000\n",
      "2304/2304 [==============================] - 0s 99us/step - loss: 388893333.6927 - val_loss: 326316032.4853\n",
      "Epoch 486/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 383886749.3455 - val_loss: 417626051.8821\n",
      "Epoch 487/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 397745622.0347 - val_loss: 329021052.8804\n",
      "Epoch 488/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 386577168.3472 - val_loss: 307010964.0208\n",
      "Epoch 489/1000\n",
      "2304/2304 [==============================] - 0s 99us/step - loss: 387291054.2396 - val_loss: 358223275.7158\n",
      "Epoch 490/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 378244239.9653 - val_loss: 301868917.0745\n",
      "Epoch 491/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 377329040.6215 - val_loss: 404036249.2132\n",
      "Epoch 492/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 382641906.4531 - val_loss: 291731218.7175\n",
      "Epoch 493/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 382800902.5938 - val_loss: 336030924.6308\n",
      "Epoch 494/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 388069408.0174 - val_loss: 315239141.7955\n",
      "Epoch 495/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 385791713.1840 - val_loss: 288203329.3865\n",
      "Epoch 496/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 379096317.5174 - val_loss: 343318103.1681\n",
      "Epoch 497/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 383559226.3333 - val_loss: 312283451.7019\n",
      "Epoch 498/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 388929511.6736 - val_loss: 310215639.6811\n",
      "Epoch 499/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 393188885.9340 - val_loss: 578748788.5199\n",
      "Epoch 500/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 411679654.3056 - val_loss: 284292310.5442\n",
      "Epoch 501/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 376147607.4792 - val_loss: 277086369.4558\n",
      "Epoch 502/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 379954888.4583 - val_loss: 310442908.8527\n",
      "Epoch 503/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 371099960.3056 - val_loss: 295166794.6759\n",
      "Epoch 504/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 369827579.8090 - val_loss: 316222611.5494\n",
      "Epoch 505/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 389516746.8368 - val_loss: 287987060.7140\n",
      "Epoch 506/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 389050672.9861 - val_loss: 312690183.3484\n",
      "Epoch 507/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 378579842.4167 - val_loss: 362319333.2964\n",
      "Epoch 508/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 386897495.6007 - val_loss: 315238113.1646\n",
      "Epoch 509/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 382808608.0625 - val_loss: 285179774.0589\n",
      "Epoch 510/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 381297219.1076 - val_loss: 415564340.8804\n",
      "Epoch 511/1000\n",
      "2304/2304 [==============================] - 0s 100us/step - loss: 381368715.6736 - val_loss: 298133047.7643\n",
      "Epoch 512/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 381910359.8264 - val_loss: 324419997.4350\n",
      "Epoch 513/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 371911225.4792 - val_loss: 284355896.0832\n",
      "Epoch 514/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 382448637.0243 - val_loss: 309875050.0104\n",
      "Epoch 515/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 387325145.7014 - val_loss: 551504144.3882\n",
      "Epoch 516/1000\n",
      "2304/2304 [==============================] - 0s 100us/step - loss: 382609199.8715 - val_loss: 284987097.7747\n",
      "Epoch 517/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 371848333.3299 - val_loss: 295215033.0260\n",
      "Epoch 518/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 365188165.4965 - val_loss: 287850892.7695\n",
      "Epoch 519/1000\n",
      "2304/2304 [==============================] - 0s 124us/step - loss: 378436570.6024 - val_loss: 314650559.0988\n",
      "Epoch 520/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 375327495.1771 - val_loss: 348800813.7608\n",
      "Epoch 521/1000\n",
      "2304/2304 [==============================] - 0s 126us/step - loss: 377175895.0104 - val_loss: 334124114.3570\n",
      "Epoch 522/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 377973915.9948 - val_loss: 294241404.8250\n",
      "Epoch 523/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 369770291.7465 - val_loss: 300086253.5459\n",
      "Epoch 524/1000\n",
      "2304/2304 [==============================] - 0s 100us/step - loss: 371662966.8681 - val_loss: 339570304.7348\n",
      "Epoch 525/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 0s 105us/step - loss: 355142929.1007 - val_loss: 301402956.2426\n",
      "Epoch 526/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 379188310.0729 - val_loss: 325846460.4090\n",
      "Epoch 527/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 369513237.7604 - val_loss: 274706515.2166\n",
      "Epoch 528/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 372278904.2674 - val_loss: 296114703.0017\n",
      "Epoch 529/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 369356281.8750 - val_loss: 363793849.5945\n",
      "Epoch 530/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 357950041.2188 - val_loss: 330424746.6759\n",
      "Epoch 531/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 364775837.3125 - val_loss: 297958290.8977\n",
      "Epoch 532/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 356119586.9340 - val_loss: 370144906.0104\n",
      "Epoch 533/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 376761103.6111 - val_loss: 282776931.1334\n",
      "Epoch 534/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 361068770.4653 - val_loss: 291032380.7695\n",
      "Epoch 535/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 355864327.3542 - val_loss: 402158895.9168\n",
      "Epoch 536/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 357897629.8715 - val_loss: 348278102.5303\n",
      "Epoch 537/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 369996649.8958 - val_loss: 293958777.2756\n",
      "Epoch 538/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 386864467.7257 - val_loss: 295858881.1092\n",
      "Epoch 539/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 367425707.9444 - val_loss: 284319688.1802\n",
      "Epoch 540/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 364432326.0208 - val_loss: 320558330.1490\n",
      "Epoch 541/1000\n",
      "2304/2304 [==============================] - 0s 100us/step - loss: 387932704.6493 - val_loss: 300140439.2236\n",
      "Epoch 542/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 365821955.1128 - val_loss: 274301131.0919\n",
      "Epoch 543/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 358054764.9688 - val_loss: 289264214.7383\n",
      "Epoch 544/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 373044138.6528 - val_loss: 288352878.4055\n",
      "Epoch 545/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 353564186.9444 - val_loss: 322402177.8856\n",
      "Epoch 546/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 382258280.1250 - val_loss: 342160367.1196\n",
      "Epoch 547/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 366463003.8854 - val_loss: 292826609.6638\n",
      "Epoch 548/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 369491363.5833 - val_loss: 322516001.8302\n",
      "Epoch 549/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 351086905.5729 - val_loss: 290679010.8562\n",
      "Epoch 550/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 400656409.7465 - val_loss: 308420470.3085\n",
      "Epoch 551/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 347184217.5799 - val_loss: 285341585.5251\n",
      "Epoch 552/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 351667680.6979 - val_loss: 298574711.6118\n",
      "Epoch 553/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 387625306.3368 - val_loss: 316951409.2617\n",
      "Epoch 554/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 354738009.6007 - val_loss: 296421001.9272\n",
      "Epoch 555/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 366537595.2188 - val_loss: 342504572.0069\n",
      "Epoch 556/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 349983558.4618 - val_loss: 288295088.8596\n",
      "Epoch 557/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 370477045.9010 - val_loss: 362766979.7990\n",
      "Epoch 558/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 364049024.5087 - val_loss: 357855709.4627\n",
      "Epoch 559/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 350480153.7292 - val_loss: 326436283.8544\n",
      "Epoch 560/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 365040155.8924 - val_loss: 316236247.6256\n",
      "Epoch 561/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 344944948.8125 - val_loss: 317310638.2808\n",
      "Epoch 562/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 358884771.4097 - val_loss: 292065034.9255\n",
      "Epoch 563/1000\n",
      "2304/2304 [==============================] - 0s 100us/step - loss: 373323395.7917 - val_loss: 315507644.5338\n",
      "Epoch 564/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 343279180.4583 - val_loss: 297175618.4055\n",
      "Epoch 565/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 371795946.5521 - val_loss: 295651806.8076\n",
      "Epoch 566/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 338844455.1354 - val_loss: 323695515.7435\n",
      "Epoch 567/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 360587704.7361 - val_loss: 328553529.6776\n",
      "Epoch 568/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 373503050.5174 - val_loss: 354904157.6430\n",
      "Epoch 569/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 358471011.4653 - val_loss: 299795493.4489\n",
      "Epoch 570/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 344071261.5139 - val_loss: 441397915.2998\n",
      "Epoch 571/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 361193561.7188 - val_loss: 293241560.5962\n",
      "Epoch 572/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 375676872.7361 - val_loss: 363361278.5511\n",
      "Epoch 573/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 349713186.0174 - val_loss: 283969779.6742\n",
      "Epoch 574/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 338082050.6059 - val_loss: 308217033.4419\n",
      "Epoch 575/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 355368213.8472 - val_loss: 292222520.6932\n",
      "Epoch 576/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 358419715.6007 - val_loss: 320993492.9497\n",
      "Epoch 577/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 336453163.4826 - val_loss: 443757612.0139\n",
      "Epoch 578/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 352052717.3819 - val_loss: 309640438.3778\n",
      "Epoch 579/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 339640874.1806 - val_loss: 283531004.7140\n",
      "Epoch 580/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 350145757.3819 - val_loss: 296903900.1594\n",
      "Epoch 581/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 349908791.9861 - val_loss: 347045705.6776\n",
      "Epoch 582/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 342883971.6597 - val_loss: 273854261.7262\n",
      "Epoch 583/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 355194823.5972 - val_loss: 277530376.3466\n",
      "Epoch 584/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 341549499.7344 - val_loss: 282053755.9237\n",
      "Epoch 585/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 355295672.6528 - val_loss: 310443039.8198\n",
      "Epoch 586/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 356977746.6910 - val_loss: 283757614.4887\n",
      "Epoch 587/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 350112884.6458 - val_loss: 287308270.3501\n",
      "Epoch 588/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 347156049.1528 - val_loss: 285744270.3778\n",
      "Epoch 589/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 365870491.7431 - val_loss: 304035652.5338\n",
      "Epoch 590/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 336832401.1337 - val_loss: 279893590.1144\n",
      "Epoch 591/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 352987008.1319 - val_loss: 279473528.1664\n",
      "Epoch 592/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 357962133.4618 - val_loss: 373454937.1716\n",
      "Epoch 593/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 347059492.1806 - val_loss: 278330324.4783\n",
      "Epoch 594/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 347683933.7674 - val_loss: 295132698.6620\n",
      "Epoch 595/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 351217131.3715 - val_loss: 271642768.0832\n",
      "Epoch 596/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 356827447.3333 - val_loss: 296923123.0364\n",
      "Epoch 597/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 339265477.5104 - val_loss: 285534694.5858\n",
      "Epoch 598/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 335884090.4566 - val_loss: 271203591.0156\n",
      "Epoch 599/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 333644355.6944 - val_loss: 291082429.0052\n",
      "Epoch 600/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 348405310.6389 - val_loss: 332954309.1369\n",
      "Epoch 601/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 339482539.4392 - val_loss: 269079705.2201\n",
      "Epoch 602/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 355434133.9201 - val_loss: 474315996.7834\n",
      "Epoch 603/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 352358993.9410 - val_loss: 267921868.3674\n",
      "Epoch 604/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 336292652.6328 - val_loss: 267210861.4766\n",
      "Epoch 605/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 326385839.8403 - val_loss: 310118568.3050\n",
      "Epoch 606/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 347403569.1806 - val_loss: 273809199.0433\n",
      "Epoch 607/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 337359050.1597 - val_loss: 286583651.8406\n",
      "Epoch 608/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 339423038.9514 - val_loss: 272864033.3310\n",
      "Epoch 609/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 325690838.9514 - val_loss: 263743967.8891\n",
      "Epoch 610/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 321165295.8056 - val_loss: 276781209.8302\n",
      "Epoch 611/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 348809782.3854 - val_loss: 286271810.5927\n",
      "Epoch 612/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 331170708.5521 - val_loss: 291939942.4055\n",
      "Epoch 613/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 325825165.6181 - val_loss: 287451656.1802\n",
      "Epoch 614/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 341023898.7569 - val_loss: 283077256.2080\n",
      "Epoch 615/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 323086517.8090 - val_loss: 288807382.6828\n",
      "Epoch 616/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 329114550.1632 - val_loss: 307748836.5338\n",
      "Epoch 617/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 344676713.0868 - val_loss: 417390459.5355\n",
      "Epoch 618/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 336821632.1424 - val_loss: 286708550.4749\n",
      "Epoch 619/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 341948008.4757 - val_loss: 260016389.6568\n",
      "Epoch 620/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 319550406.5868 - val_loss: 263252080.4853\n",
      "Epoch 621/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 347040398.5729 - val_loss: 249792066.5650\n",
      "Epoch 622/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 343069025.7257 - val_loss: 274926148.6863\n",
      "Epoch 623/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 344282640.6910 - val_loss: 277507494.5858\n",
      "Epoch 624/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 319599334.7101 - val_loss: 287076680.7071\n",
      "Epoch 625/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 340634632.9670 - val_loss: 295596629.7747\n",
      "Epoch 626/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 340679044.7205 - val_loss: 256127003.5217\n",
      "Epoch 627/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 323204083.4965 - val_loss: 262353643.7158\n",
      "Epoch 628/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 326855523.3403 - val_loss: 267184186.8562\n",
      "Epoch 629/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 335425082.269 - 0s 104us/step - loss: 320124115.6562 - val_loss: 290597595.9653\n",
      "Epoch 630/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 338338362.7500 - val_loss: 268741374.1005\n",
      "Epoch 631/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 345071064.8056 - val_loss: 272939847.5009\n",
      "Epoch 632/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 334428620.5174 - val_loss: 275940154.7036\n",
      "Epoch 633/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 360780143.5938 - val_loss: 359590308.0485\n",
      "Epoch 634/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 327751220.8715 - val_loss: 318741095.3068\n",
      "Epoch 635/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 331769364.3420 - val_loss: 374972046.2808\n",
      "Epoch 636/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 331326104.4896 - val_loss: 290107509.1438\n",
      "Epoch 637/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 343808386.6840 - val_loss: 326007754.0520\n",
      "Epoch 638/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 324874470.0330 - val_loss: 281646441.5667\n",
      "Epoch 639/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 329983978.4306 - val_loss: 261533326.6135\n",
      "Epoch 640/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 317427046.2465 - val_loss: 257531860.4298\n",
      "Epoch 641/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 320969584.8542 - val_loss: 325167171.6603\n",
      "Epoch 642/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 333201989.2812 - val_loss: 263904157.0884\n",
      "Epoch 643/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 328483839.7847 - val_loss: 273401826.2045\n",
      "Epoch 644/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 335589054.8681 - val_loss: 266136797.4766\n",
      "Epoch 645/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 326036318.7535 - val_loss: 310730638.7522\n",
      "Epoch 646/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 324916706.6771 - val_loss: 265946932.2981\n",
      "Epoch 647/1000\n",
      "2304/2304 [==============================] - 0s 126us/step - loss: 314982361.9757 - val_loss: 265481571.8891\n",
      "Epoch 648/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 321856511.6042 - val_loss: 289182879.2582\n",
      "Epoch 649/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 309015878.5382 - val_loss: 273317749.2894\n",
      "Epoch 650/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 332505517.8854 - val_loss: 248167168.7071\n",
      "Epoch 651/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 319453982.0382 - val_loss: 264803185.3726\n",
      "Epoch 652/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 310357128.1528 - val_loss: 262784944.9220\n",
      "Epoch 653/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 327897774.6007 - val_loss: 287858158.1421\n",
      "Epoch 654/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 323107405.2535 - val_loss: 311732904.1941\n",
      "Epoch 655/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 330374711.1181 - val_loss: 254049463.7088\n",
      "Epoch 656/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 333900567.5625 - val_loss: 255418731.5633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 657/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 313833499.4271 - val_loss: 262721054.8769\n",
      "Epoch 658/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 304749305.3056 - val_loss: 246182219.0780\n",
      "Epoch 659/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 325685329.1562 - val_loss: 259879263.7227\n",
      "Epoch 660/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 304327188.7344 - val_loss: 305374772.5893\n",
      "Epoch 661/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 328263227.9028 - val_loss: 364665228.9497\n",
      "Epoch 662/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 348985402.1424 - val_loss: 275932563.5633\n",
      "Epoch 663/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 331608112.5521 - val_loss: 260720291.5078\n",
      "Epoch 664/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 300547853.5573 - val_loss: 291592351.1958\n",
      "Epoch 665/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 313109438.3507 - val_loss: 278250388.0347\n",
      "Epoch 666/1000\n",
      "2304/2304 [==============================] - 0s 100us/step - loss: 303892691.9757 - val_loss: 302224140.2149\n",
      "Epoch 667/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 330023160.7812 - val_loss: 261177202.9393\n",
      "Epoch 668/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 316183511.1806 - val_loss: 256224119.9723\n",
      "Epoch 669/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 324350436.1597 - val_loss: 284805310.3362\n",
      "Epoch 670/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 318357384.7396 - val_loss: 259187070.3501\n",
      "Epoch 671/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 324246087.8264 - val_loss: 399325494.4887\n",
      "Epoch 672/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 341027083.8160 - val_loss: 283928618.9324\n",
      "Epoch 673/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 320676787.1736 - val_loss: 253583754.7730\n",
      "Epoch 674/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 315085312.4826 - val_loss: 249634498.7868\n",
      "Epoch 675/1000\n",
      "2304/2304 [==============================] - 0s 99us/step - loss: 328454709.5174 - val_loss: 253482957.1438\n",
      "Epoch 676/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 315247100.5104 - val_loss: 305489341.4489\n",
      "Epoch 677/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 323788646.4201 - val_loss: 432900890.9532\n",
      "Epoch 678/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 328391587.1597 - val_loss: 266668710.6412\n",
      "Epoch 679/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 308983733.1042 - val_loss: 291547587.9515\n",
      "Epoch 680/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 311806166.5417 - val_loss: 256397386.5511\n",
      "Epoch 681/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 316913392.3264 - val_loss: 335297392.9289\n",
      "Epoch 682/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 310217180.5382 - val_loss: 252253285.1300\n",
      "Epoch 683/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 307823780.3438 - val_loss: 254367669.2825\n",
      "Epoch 684/1000\n",
      "2304/2304 [==============================] - 0s 100us/step - loss: 312354087.1493 - val_loss: 357423176.4298\n",
      "Epoch 685/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 326010051.3108 - val_loss: 271971961.3172\n",
      "Epoch 686/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 301308240.0833 - val_loss: 272804531.9792\n",
      "Epoch 687/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 312580048.0347 - val_loss: 286476363.6950\n",
      "Epoch 688/1000\n",
      "2304/2304 [==============================] - 0s 100us/step - loss: 314970401.6562 - val_loss: 255843814.2392\n",
      "Epoch 689/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 305566147.7292 - val_loss: 242159775.5009\n",
      "Epoch 690/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 300234444.5278 - val_loss: 268478903.0225\n",
      "Epoch 691/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 314078290.4583 - val_loss: 242557661.8925\n",
      "Epoch 692/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 297165133.1198 - val_loss: 243242930.9393\n",
      "Epoch 693/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 305107305.1771 - val_loss: 265238710.3640\n",
      "Epoch 694/1000\n",
      "2304/2304 [==============================] - 0s 99us/step - loss: 328948171.6007 - val_loss: 284100567.7782\n",
      "Epoch 695/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 301993718.1771 - val_loss: 242880176.5061\n",
      "Epoch 696/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 311675593.6927 - val_loss: 248915262.9047\n",
      "Epoch 697/1000\n",
      "2304/2304 [==============================] - 0s 99us/step - loss: 313848293.5799 - val_loss: 249934959.2929\n",
      "Epoch 698/1000\n",
      "2304/2304 [==============================] - 0s 99us/step - loss: 310630718.8160 - val_loss: 245094826.1144\n",
      "Epoch 699/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 339527366.3194 - val_loss: 285472839.1265\n",
      "Epoch 700/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 302209535.9306 - val_loss: 256426650.8700\n",
      "Epoch 701/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 313401844.6128 - val_loss: 429972539.2444\n",
      "Epoch 702/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 327592130.1076 - val_loss: 252083164.0485\n",
      "Epoch 703/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 315052412.8438 - val_loss: 258018105.3033\n",
      "Epoch 704/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 309503543.8194 - val_loss: 245310963.9723\n",
      "Epoch 705/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 312619948.7431 - val_loss: 243420493.7608\n",
      "Epoch 706/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 312610260.9653 - val_loss: 294141695.1404\n",
      "Epoch 707/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 308220829.6458 - val_loss: 233224658.9532\n",
      "Epoch 708/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 313271763.1736 - val_loss: 248656955.8267\n",
      "Epoch 709/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 301675807.2500 - val_loss: 261145562.0659\n",
      "Epoch 710/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 307507383.5035 - val_loss: 265907141.9549\n",
      "Epoch 711/1000\n",
      "2304/2304 [==============================] - 0s 99us/step - loss: 294829288.9878 - val_loss: 230910613.2270\n",
      "Epoch 712/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 302553063.8385 - val_loss: 352402237.6222\n",
      "Epoch 713/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 302249137.9132 - val_loss: 248260158.3501\n",
      "Epoch 714/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 310548725.9115 - val_loss: 255208611.5494\n",
      "Epoch 715/1000\n",
      "2304/2304 [==============================] - 0s 124us/step - loss: 310807515.0625 - val_loss: 259595403.1612\n",
      "Epoch 716/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 305612219.7222 - val_loss: 291625475.5217\n",
      "Epoch 717/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 332741350.0694 - val_loss: 238658928.0624\n",
      "Epoch 718/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 332580060.4306 - val_loss: 233046087.2097\n",
      "Epoch 719/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 309332191.6319 - val_loss: 422759576.0693\n",
      "Epoch 720/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 298830117.0330 - val_loss: 222619999.4593\n",
      "Epoch 721/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 288900954.4913 - val_loss: 235503511.0711\n",
      "Epoch 722/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 299517916.4375 - val_loss: 327154824.4645\n",
      "Epoch 723/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 0s 105us/step - loss: 305078587.0104 - val_loss: 245871803.3276\n",
      "Epoch 724/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 298040120.3472 - val_loss: 286817418.3432\n",
      "Epoch 725/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 302913381.2882 - val_loss: 287952433.6360\n",
      "Epoch 726/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 315003737.8611 - val_loss: 285893001.1508\n",
      "Epoch 727/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 298740440.8976 - val_loss: 237665764.6586\n",
      "Epoch 728/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 297773309.2587 - val_loss: 255110476.3951\n",
      "Epoch 729/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 291840572.9583 - val_loss: 281141555.4662\n",
      "Epoch 730/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 293743823.4427 - val_loss: 361328315.6395\n",
      "Epoch 731/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 298130505.7465 - val_loss: 232068304.9289\n",
      "Epoch 732/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 318749061.7066 - val_loss: 227154366.9047\n",
      "Epoch 733/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 294621033.9688 - val_loss: 249965041.2756\n",
      "Epoch 734/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 311621963.9514 - val_loss: 231621125.9896\n",
      "Epoch 735/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 302696195.5382 - val_loss: 231488826.0659\n",
      "Epoch 736/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 299408653.1944 - val_loss: 247669212.6031\n",
      "Epoch 737/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 299329565.5069 - val_loss: 300539666.5789\n",
      "Epoch 738/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 300184870.8854 - val_loss: 279137629.6846\n",
      "Epoch 739/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 297649273.0590 - val_loss: 244231413.5598\n",
      "Epoch 740/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 308385554.0660 - val_loss: 239128682.8076\n",
      "Epoch 741/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 300669502.7083 - val_loss: 255164232.7348\n",
      "Epoch 742/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 290086524.2865 - val_loss: 251119023.8891\n",
      "Epoch 743/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 293566062.3889 - val_loss: 230204905.4766\n",
      "Epoch 744/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 290096704.4653 - val_loss: 227135433.6430\n",
      "Epoch 745/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 290696964.7535 - val_loss: 232638097.6915\n",
      "Epoch 746/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 305657447.0017 - val_loss: 226711234.6066\n",
      "Epoch 747/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 300434892.7292 - val_loss: 230383716.6447\n",
      "Epoch 748/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 301722511.5816 - val_loss: 230330552.5130\n",
      "Epoch 749/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 287075505.1441 - val_loss: 244092415.2652\n",
      "Epoch 750/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 307646888.6979 - val_loss: 260478011.6326\n",
      "Epoch 751/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 284166086.0938 - val_loss: 313734300.1872\n",
      "Epoch 752/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 296939486.3090 - val_loss: 233636703.4454\n",
      "Epoch 753/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 302770955.0972 - val_loss: 230871097.1369\n",
      "Epoch 754/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 289497681.2153 - val_loss: 249313712.2704\n",
      "Epoch 755/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 298941428.2569 - val_loss: 278823127.0780\n",
      "Epoch 756/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 298765148.7986 - val_loss: 246576246.0797\n",
      "Epoch 757/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 297453520.1215 - val_loss: 253586994.1629\n",
      "Epoch 758/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 301823578.9132 - val_loss: 232806953.4835\n",
      "Epoch 759/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 298280505.6215 - val_loss: 246781948.4714\n",
      "Epoch 760/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 287296199.8264 - val_loss: 276091853.4419\n",
      "Epoch 761/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 294890732.1979 - val_loss: 236468503.9445\n",
      "Epoch 762/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 295043872.2500 - val_loss: 230215361.1924\n",
      "Epoch 763/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 296136278.9722 - val_loss: 315436691.8683\n",
      "Epoch 764/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 285301675.5139 - val_loss: 236909926.3778\n",
      "Epoch 765/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 287685810.7153 - val_loss: 311419624.7348\n",
      "Epoch 766/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 297397942.9306 - val_loss: 272085937.2756\n",
      "Epoch 767/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 285513071.8056 - val_loss: 288786032.5893\n",
      "Epoch 768/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 296399628.5330 - val_loss: 248133730.7660\n",
      "Epoch 769/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 297260812.4965 - val_loss: 247642947.8821\n",
      "Epoch 770/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 282020217.2431 - val_loss: 283178875.5771\n",
      "Epoch 771/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 283594019.5764 - val_loss: 244155895.7435\n",
      "Epoch 772/1000\n",
      "2304/2304 [==============================] - 0s 99us/step - loss: 282972791.6007 - val_loss: 275722614.4887\n",
      "Epoch 773/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 296432805.1311 - val_loss: 311390468.2288\n",
      "Epoch 774/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 290138774.3663 - val_loss: 283960198.9879\n",
      "Epoch 775/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 299540531.8837 - val_loss: 224447427.6534\n",
      "Epoch 776/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 296075218.6007 - val_loss: 247691258.0243\n",
      "Epoch 777/1000\n",
      "2304/2304 [==============================] - 0s 123us/step - loss: 287347363.5538 - val_loss: 261036779.0919\n",
      "Epoch 778/1000\n",
      "2304/2304 [==============================] - 0s 131us/step - loss: 300641590.7604 - val_loss: 233652911.3484\n",
      "Epoch 779/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 293898629.5000 - val_loss: 277214347.0225\n",
      "Epoch 780/1000\n",
      "2304/2304 [==============================] - 0s 126us/step - loss: 292823768.9688 - val_loss: 249684502.0589\n",
      "Epoch 781/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 304602309.6684 - val_loss: 237744292.1872\n",
      "Epoch 782/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 301731651.9167 - val_loss: 254464306.7175\n",
      "Epoch 783/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 310584150.5625 - val_loss: 243553475.7712\n",
      "Epoch 784/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 296523281.4132 - val_loss: 267785222.8354\n",
      "Epoch 785/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 308595880.6007 - val_loss: 359105771.4870\n",
      "Epoch 786/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 283708607.3715 - val_loss: 263501503.8059\n",
      "Epoch 787/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 294332662.9635 - val_loss: 367783052.7834\n",
      "Epoch 788/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 289972416.4410 - val_loss: 450479127.2236\n",
      "Epoch 789/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 296246028.5903 - val_loss: 251743605.1023\n",
      "Epoch 790/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 279950566.1910 - val_loss: 247686080.8250\n",
      "Epoch 791/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 277788569.5868 - val_loss: 250371996.8527\n",
      "Epoch 792/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 288601899.4635 - val_loss: 269789169.5251\n",
      "Epoch 793/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 285948573.7378 - val_loss: 254115813.0884\n",
      "Epoch 794/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 277514129.8194 - val_loss: 264212554.8423\n",
      "Epoch 795/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 274674334.7205 - val_loss: 255489390.0173\n",
      "Epoch 796/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 285320665.9896 - val_loss: 243733479.7088\n",
      "Epoch 797/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 279881202.8854 - val_loss: 286282456.1941\n",
      "Epoch 798/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 293582428.7049 - val_loss: 296202148.7140\n",
      "Epoch 799/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 278400651.0104 - val_loss: 241479020.5615\n",
      "Epoch 800/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 285001089.5312 - val_loss: 261938565.1924\n",
      "Epoch 801/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 275886808.9618 - val_loss: 259545560.4159\n",
      "Epoch 802/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 291565507.4670 - val_loss: 273528367.0711\n",
      "Epoch 803/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 296232120.6771 - val_loss: 258733632.4367\n",
      "Epoch 804/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 290519612.6076 - val_loss: 268533189.9272\n",
      "Epoch 805/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 292627349.3993 - val_loss: 248747510.6135\n",
      "Epoch 806/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 295012276.3299 - val_loss: 249142961.3588\n",
      "Epoch 807/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 296617406.9149 - val_loss: 412957866.8284\n",
      "Epoch 808/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 304954760.6997 - val_loss: 267506985.2617\n",
      "Epoch 809/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 286851324.8351 - val_loss: 249753637.6153\n",
      "Epoch 810/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 285072038.7604 - val_loss: 238057641.4419\n",
      "Epoch 811/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 279453107.1632 - val_loss: 292695057.1993\n",
      "Epoch 812/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 279518232.5174 - val_loss: 240225129.0607\n",
      "Epoch 813/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 289419424.0990 - val_loss: 289391395.1196\n",
      "Epoch 814/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 274987481.1667 - val_loss: 257564101.7400\n",
      "Epoch 815/1000\n",
      "2304/2304 [==============================] - 0s 101us/step - loss: 276533095.2847 - val_loss: 272079844.1872\n",
      "Epoch 816/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 289340014.2465 - val_loss: 239306977.3518\n",
      "Epoch 817/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 301663823.3438 - val_loss: 252775268.4229\n",
      "Epoch 818/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 282254504.7639 - val_loss: 272369240.6655\n",
      "Epoch 819/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 275852352.3264 - val_loss: 277221028.7556\n",
      "Epoch 820/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 274342476.2205 - val_loss: 280396725.5043\n",
      "Epoch 821/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 296402912.9965 - val_loss: 250558741.8787\n",
      "Epoch 822/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 294901291.7448 - val_loss: 234084898.8354\n",
      "Epoch 823/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 274053527.7517 - val_loss: 249119057.6083\n",
      "Epoch 824/1000\n",
      "2304/2304 [==============================] - 0s 103us/step - loss: 296854450.7431 - val_loss: 287098594.2114\n",
      "Epoch 825/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 273877168.7917 - val_loss: 230427741.6707\n",
      "Epoch 826/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 265356040.6562 - val_loss: 261099950.9879\n",
      "Epoch 827/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 287761585.6875 - val_loss: 243833573.2478\n",
      "Epoch 828/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 286124588.5694 - val_loss: 296755938.7036\n",
      "Epoch 829/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 290707728.2925 - val_loss: 276809271.6118\n",
      "Epoch 830/1000\n",
      "2304/2304 [==============================] - 0s 104us/step - loss: 282447137.6701 - val_loss: 324024881.0260\n",
      "Epoch 831/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 269363210.9288 - val_loss: 422740350.8492\n",
      "Epoch 832/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 289595272.0955 - val_loss: 283517645.1231\n",
      "Epoch 833/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 278072120.2743 - val_loss: 242679818.8700\n",
      "Epoch 834/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 281376379.9097 - val_loss: 465520186.5095\n",
      "Epoch 835/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 267575229.7378 - val_loss: 239524666.8284\n",
      "Epoch 836/1000\n",
      "2304/2304 [==============================] - 0s 102us/step - loss: 286637763.6319 - val_loss: 241255283.8128\n",
      "Epoch 837/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 286144853.8628 - val_loss: 292938473.8856\n",
      "Epoch 838/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 277034918.9653 - val_loss: 263358247.7088\n",
      "Epoch 839/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 273757267.8906 - val_loss: 263340778.4125\n",
      "Epoch 840/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 276224609.9861 - val_loss: 256479909.0745\n",
      "Epoch 841/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 283475365.8750 - val_loss: 297419981.3102\n",
      "Epoch 842/1000\n",
      "2304/2304 [==============================] - 0s 128us/step - loss: 293842105.0868 - val_loss: 256744855.0572\n",
      "Epoch 843/1000\n",
      "2304/2304 [==============================] - 0s 148us/step - loss: 274706978.1701 - val_loss: 452602752.0347\n",
      "Epoch 844/1000\n",
      "2304/2304 [==============================] - 0s 141us/step - loss: 292688391.2153 - val_loss: 269196509.2132\n",
      "Epoch 845/1000\n",
      "2304/2304 [==============================] - 0s 128us/step - loss: 269383311.0174 - val_loss: 304826323.2374\n",
      "Epoch 846/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 264360479.6806 - val_loss: 258490309.7539\n",
      "Epoch 847/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 272427566.0486 - val_loss: 255442738.4402\n",
      "Epoch 848/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 291259453.5833 - val_loss: 345679802.8458\n",
      "Epoch 849/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 273358758.9896 - val_loss: 255386121.6984\n",
      "Epoch 850/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 278982299.8090 - val_loss: 252293773.3518\n",
      "Epoch 851/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 295815591.3559 - val_loss: 293084616.9567\n",
      "Epoch 852/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 276697289.3212 - val_loss: 255019508.2426\n",
      "Epoch 853/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 292185841.7361 - val_loss: 260835420.3189\n",
      "Epoch 854/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 271219280.9792 - val_loss: 281778108.5615\n",
      "Epoch 855/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 0s 112us/step - loss: 269247963.2934 - val_loss: 267481775.0988\n",
      "Epoch 856/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 280007350.0226 - val_loss: 287041575.2236\n",
      "Epoch 857/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 283312298.1840 - val_loss: 291170499.5147\n",
      "Epoch 858/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 278686506.9410 - val_loss: 272195391.3484\n",
      "Epoch 859/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 281756246.9375 - val_loss: 254318650.8769\n",
      "Epoch 860/1000\n",
      "2304/2304 [==============================] - 0s 108us/step - loss: 266611017.8924 - val_loss: 293190957.5737\n",
      "Epoch 861/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 280829262.8047 - val_loss: 284839572.5615\n",
      "Epoch 862/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 272054191.1389 - val_loss: 323174449.4558\n",
      "Epoch 863/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 296000099.1701 - val_loss: 276154164.0069\n",
      "Epoch 864/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 279835446.6632 - val_loss: 351247381.0468\n",
      "Epoch 865/1000\n",
      "2304/2304 [==============================] - 0s 128us/step - loss: 274181157.6007 - val_loss: 258910014.9463\n",
      "Epoch 866/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 269383386.2708 - val_loss: 238070669.7262\n",
      "Epoch 867/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 274240188.1111 - val_loss: 252632803.2305\n",
      "Epoch 868/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 280206374.9705 - val_loss: 266583890.5442\n",
      "Epoch 869/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 263021282.8420 - val_loss: 441313266.3154\n",
      "Epoch 870/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 262267970.3490 - val_loss: 321140918.8908\n",
      "Epoch 871/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 280563660.9306 - val_loss: 282130465.5390\n",
      "Epoch 872/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 276336921.4514 - val_loss: 246668121.4142\n",
      "Epoch 873/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 260770054.4922 - val_loss: 266211392.9012\n",
      "Epoch 874/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 273167924.3663 - val_loss: 378232456.1802\n",
      "Epoch 875/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 293780590.6632 - val_loss: 258821600.4506\n",
      "Epoch 876/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 270193158.3715 - val_loss: 280202975.8336\n",
      "Epoch 877/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 272987875.1667 - val_loss: 330703754.9809\n",
      "Epoch 878/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 272148292.8785 - val_loss: 269193620.7556\n",
      "Epoch 879/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 266255913.0938 - val_loss: 260489981.8094\n",
      "Epoch 880/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 271635292.0799 - val_loss: 264383902.5997\n",
      "Epoch 881/1000\n",
      "2304/2304 [==============================] - 0s 128us/step - loss: 272246087.3177 - val_loss: 267694293.3588\n",
      "Epoch 882/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 291492254.8819 - val_loss: 317222153.2201\n",
      "Epoch 883/1000\n",
      "2304/2304 [==============================] - 0s 126us/step - loss: 270379005.6372 - val_loss: 280851841.7816\n",
      "Epoch 884/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 274820143.5799 - val_loss: 260253821.4627\n",
      "Epoch 885/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 269946982.5799 - val_loss: 288969717.0884\n",
      "Epoch 886/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 266914089.9948 - val_loss: 262687504.0901\n",
      "Epoch 887/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 262066297.7969 - val_loss: 251035800.9567\n",
      "Epoch 888/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 273736970.6181 - val_loss: 623718448.9289\n",
      "Epoch 889/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 283502941.3160 - val_loss: 265685853.5043\n",
      "Epoch 890/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 278246465.7031 - val_loss: 301305832.4783\n",
      "Epoch 891/1000\n",
      "2304/2304 [==============================] - 0s 106us/step - loss: 269695468.6684 - val_loss: 263143912.6447\n",
      "Epoch 892/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 274264389.6406 - val_loss: 498773934.9948\n",
      "Epoch 893/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 269635439.4010 - val_loss: 285173989.3380\n",
      "Epoch 894/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 259469365.9045 - val_loss: 254950363.4315\n",
      "Epoch 895/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 253885007.1128 - val_loss: 305754366.9601\n",
      "Epoch 896/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 264621719.1597 - val_loss: 279840467.0225\n",
      "Epoch 897/1000\n",
      "2304/2304 [==============================] - 0s 129us/step - loss: 260745640.6493 - val_loss: 403382606.5719\n",
      "Epoch 898/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 274812596.5312 - val_loss: 296853535.6326\n",
      "Epoch 899/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 263901363.8351 - val_loss: 489041527.2929\n",
      "Epoch 900/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 266333970.0000 - val_loss: 422298773.0953\n",
      "Epoch 901/1000\n",
      "2304/2304 [==============================] - 0s 127us/step - loss: 275258910.0729 - val_loss: 346715894.5269\n",
      "Epoch 902/1000\n",
      "2304/2304 [==============================] - 0s 138us/step - loss: 271747127.6337 - val_loss: 306976460.9844\n",
      "Epoch 903/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 260929379.6667 - val_loss: 369722683.9515\n",
      "Epoch 904/1000\n",
      "2304/2304 [==============================] - 0s 137us/step - loss: 265143791.1163 - val_loss: 276019595.0087\n",
      "Epoch 905/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 255306241.6024 - val_loss: 274093744.2288\n",
      "Epoch 906/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 256699789.8438 - val_loss: 277262473.0815\n",
      "Epoch 907/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 252780239.0590 - val_loss: 278316468.3397\n",
      "Epoch 908/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 258722152.9306 - val_loss: 282077333.0884\n",
      "Epoch 909/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 276000927.7778 - val_loss: 264991205.2409\n",
      "Epoch 910/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 258159302.7587 - val_loss: 371187124.1040\n",
      "Epoch 911/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 282072100.7986 - val_loss: 313701164.4367\n",
      "Epoch 912/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 288148500.5104 - val_loss: 285192841.7262\n",
      "Epoch 913/1000\n",
      "2304/2304 [==============================] - 0s 107us/step - loss: 260255583.1771 - val_loss: 280121731.3899\n",
      "Epoch 914/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 264293369.6285 - val_loss: 272761075.5633\n",
      "Epoch 915/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 261811752.6727 - val_loss: 286949288.6170\n",
      "Epoch 916/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 267662499.7465 - val_loss: 315772126.0867\n",
      "Epoch 917/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 257875560.7674 - val_loss: 297400834.2600\n",
      "Epoch 918/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 253702807.7517 - val_loss: 325676149.4835\n",
      "Epoch 919/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 262370902.4896 - val_loss: 306349965.3934\n",
      "Epoch 920/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 268857454.0139 - val_loss: 290743368.4229\n",
      "Epoch 921/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 258344704.8333 - val_loss: 266593406.7660\n",
      "Epoch 922/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 254986973.0087 - val_loss: 267549813.3726\n",
      "Epoch 923/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 297658791.0851 - val_loss: 311632697.8094\n",
      "Epoch 924/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 272390435.0104 - val_loss: 295425586.9185\n",
      "Epoch 925/1000\n",
      "2304/2304 [==============================] - 0s 105us/step - loss: 253937554.5347 - val_loss: 454167195.1196\n",
      "Epoch 926/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 254236400.0625 - val_loss: 264625889.9272\n",
      "Epoch 927/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 270296094.0486 - val_loss: 299760886.6759\n",
      "Epoch 928/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 270984828.3194 - val_loss: 297377628.8042\n",
      "Epoch 929/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 266535381.5000 - val_loss: 301071920.8943\n",
      "Epoch 930/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 274180261.1198 - val_loss: 268472161.8648\n",
      "Epoch 931/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 264815737.2049 - val_loss: 269361023.7574\n",
      "Epoch 932/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 295740731.3750 - val_loss: 286896728.2912\n",
      "Epoch 933/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 263897985.4080 - val_loss: 566977222.5858\n",
      "Epoch 934/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 294887872.6580 - val_loss: 276830703.6395\n",
      "Epoch 935/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 284125436.9722 - val_loss: 291442784.5581\n",
      "Epoch 936/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 266209750.7622 - val_loss: 267747007.0017\n",
      "Epoch 937/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 263840163.1293 - val_loss: 301365524.3189\n",
      "Epoch 938/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 273301693.1927 - val_loss: 257277566.1560\n",
      "Epoch 939/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 274286367.5642 - val_loss: 264948998.4749\n",
      "Epoch 940/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 257722275.0694 - val_loss: 281277181.1300\n",
      "Epoch 941/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 258694464.7326 - val_loss: 263632509.4558\n",
      "Epoch 942/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 260255436.5677 - val_loss: 392710100.7210\n",
      "Epoch 943/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 276653408.9062 - val_loss: 312278404.5199\n",
      "Epoch 944/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 261448983.2622 - val_loss: 290456930.0104\n",
      "Epoch 945/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 252373976.8958 - val_loss: 313043045.0745\n",
      "Epoch 946/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 249518660.4896 - val_loss: 355951954.9255\n",
      "Epoch 947/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 273718342.3576 - val_loss: 294487558.3224\n",
      "Epoch 948/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 261279502.3368 - val_loss: 288221644.7279\n",
      "Epoch 949/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 253193041.1910 - val_loss: 276195486.2392\n",
      "Epoch 950/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 264686499.3542 - val_loss: 350729352.6794\n",
      "Epoch 951/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 282962659.7222 - val_loss: 291191926.6066\n",
      "Epoch 952/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 262257278.7917 - val_loss: 349965546.3709\n",
      "Epoch 953/1000\n",
      "2304/2304 [==============================] - 0s 116us/step - loss: 269012534.8142 - val_loss: 293441547.9445\n",
      "Epoch 954/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 263102652.2413 - val_loss: 288280342.8215\n",
      "Epoch 955/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 283647893.9670 - val_loss: 320184936.4229\n",
      "Epoch 956/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 258535059.5955 - val_loss: 288067818.5511\n",
      "Epoch 957/1000\n",
      "2304/2304 [==============================] - 0s 109us/step - loss: 268399423.5174 - val_loss: 412394857.1369\n",
      "Epoch 958/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 264600353.8663 - val_loss: 268312947.2998\n",
      "Epoch 959/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 261276924.1128 - val_loss: 331464827.7990\n",
      "Epoch 960/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 276097732.5347 - val_loss: 302479556.0763\n",
      "Epoch 961/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 241639622.9792 - val_loss: 287937457.0745\n",
      "Epoch 962/1000\n",
      "2304/2304 [==============================] - 0s 138us/step - loss: 278190824.4132 - val_loss: 293049646.8908\n",
      "Epoch 963/1000\n",
      "2304/2304 [==============================] - 0s 141us/step - loss: 270258413.9167 - val_loss: 330829886.4887\n",
      "Epoch 964/1000\n",
      "2304/2304 [==============================] - 0s 140us/step - loss: 270195736.0365 - val_loss: 336163118.0589\n",
      "Epoch 965/1000\n",
      "2304/2304 [==============================] - 0s 124us/step - loss: 270901896.7066 - val_loss: 299153940.9913\n",
      "Epoch 966/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 251689156.1215 - val_loss: 270231215.5563\n",
      "Epoch 967/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 263368722.4497 - val_loss: 293174276.2773\n",
      "Epoch 968/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 270192964.8333 - val_loss: 345405249.2132\n",
      "Epoch 969/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 252265731.0139 - val_loss: 275341502.7799\n",
      "Epoch 970/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 255280196.6736 - val_loss: 267362046.6135\n",
      "Epoch 971/1000\n",
      "2304/2304 [==============================] - 0s 114us/step - loss: 280665315.8646 - val_loss: 309800877.4905\n",
      "Epoch 972/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 261215800.3837 - val_loss: 294065873.6776\n",
      "Epoch 973/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 269589428.7066 - val_loss: 399261472.8596\n",
      "Epoch 974/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 272026061.3108 - val_loss: 269755751.9307\n",
      "Epoch 975/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 255940312.7222 - val_loss: 384048551.8128\n",
      "Epoch 976/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 258832266.3368 - val_loss: 317028672.8111\n",
      "Epoch 977/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 264979889.7049 - val_loss: 277543477.3345\n",
      "Epoch 978/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 268179472.2448 - val_loss: 397958680.5546\n",
      "Epoch 979/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 259578297.3455 - val_loss: 298086280.1109\n",
      "Epoch 980/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 244126674.8194 - val_loss: 294340798.8769\n",
      "Epoch 981/1000\n",
      "2304/2304 [==============================] - 0s 110us/step - loss: 257597159.1901 - val_loss: 395539144.6378\n",
      "Epoch 982/1000\n",
      "2304/2304 [==============================] - 0s 119us/step - loss: 285965287.9375 - val_loss: 327852232.5685\n",
      "Epoch 983/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 241705340.0590 - val_loss: 296816279.2305\n",
      "Epoch 984/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 243185265.5191 - val_loss: 299297619.5286\n",
      "Epoch 985/1000\n",
      "2304/2304 [==============================] - 0s 112us/step - loss: 251863360.7396 - val_loss: 279188956.9151\n",
      "Epoch 986/1000\n",
      "2304/2304 [==============================] - 0s 118us/step - loss: 274833039.4913 - val_loss: 296036526.7036\n",
      "Epoch 987/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 0s 117us/step - loss: 265010937.0486 - val_loss: 322119395.3206\n",
      "Epoch 988/1000\n",
      "2304/2304 [==============================] - 0s 111us/step - loss: 246448110.7812 - val_loss: 295661436.9289\n",
      "Epoch 989/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 271245026.4253 - val_loss: 327782060.9775\n",
      "Epoch 990/1000\n",
      "2304/2304 [==============================] - 0s 117us/step - loss: 273619415.2708 - val_loss: 327721579.0399\n",
      "Epoch 991/1000\n",
      "2304/2304 [==============================] - 0s 113us/step - loss: 252446726.3411 - val_loss: 288044427.8821\n",
      "Epoch 992/1000\n",
      "2304/2304 [==============================] - 0s 120us/step - loss: 256171693.4271 - val_loss: 286914881.4003\n",
      "Epoch 993/1000\n",
      "2304/2304 [==============================] - 0s 122us/step - loss: 278922355.8455 - val_loss: 490885191.3761\n",
      "Epoch 994/1000\n",
      "2304/2304 [==============================] - 0s 126us/step - loss: 265811147.5243 - val_loss: 290949199.4454\n",
      "Epoch 995/1000\n",
      "2304/2304 [==============================] - 0s 121us/step - loss: 244801909.2222 - val_loss: 296006174.7938\n",
      "Epoch 996/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 246327509.2326 - val_loss: 289998565.0191\n",
      "Epoch 997/1000\n",
      "2304/2304 [==============================] - 0s 130us/step - loss: 254978356.2153 - val_loss: 331892934.1698\n",
      "Epoch 998/1000\n",
      "2304/2304 [==============================] - 0s 125us/step - loss: 257197232.2465 - val_loss: 272762805.2340\n",
      "Epoch 999/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 253948034.8958 - val_loss: 291463954.0173\n",
      "Epoch 1000/1000\n",
      "2304/2304 [==============================] - 0s 115us/step - loss: 258027731.5712 - val_loss: 301793341.2548\n"
     ]
    }
   ],
   "source": [
    "# define the modesl\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the Input layer and the first Hidden Layers\n",
    "classifier.add(Dense(output_dim=50,init ='he_uniform',activation='relu',input_dim=174))\n",
    "\n",
    "# Adding the second Hiddden layers\n",
    "classifier.add(Dense(output_dim=25, init='he_uniform', activation='relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim=50, init='he_uniform',activation='relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim=1, init='he_uniform'))\n",
    "\n",
    "# Compile the Model\n",
    "classifier.compile(loss=losses.mean_squared_error,optimizer='Adamax')\n",
    "\n",
    "#  Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(X_train.values, y_train.values,validation_split=0.20, batch_size = 10, nb_epoch = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred=classifier.predict(df_Test.drop(['SalePrice'],axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
